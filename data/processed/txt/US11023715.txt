( 12 ) United States Patent Qiu et al . ( 54 ) METHOD AND APPARATUS FOR EXPRESSION RECOGNITION ( 71 ) Applicant : ArcSoft Corporation Limited , Hangzhou ( CN ) ( 72 ) Inventors : Han Qiu , Hangzhou ( CN ) ; Fang Deng , Nanjing ( CN ) ; Kangning Song , Hangzhou ( CN ) ( 73 ) Assignee : ArcSoft Corporation Limited , Hangzhou ( CN ) CN CN ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 209 days . ( 21 ) Appl . No .: 16 / 045,325 ( 22 ) Filed : Jul . 25 , 2018 Prior Publication Data US 2019/0034709 A1 US 2019/0294866 A9 Jan. 31 , 2019 Sep. 26 , 2019 ( 65 ) ( 30 ) US011023715B2 ( 10 ) Patent No .: ( 45 ) Date of Patent : US 11,023,715 B2 Jun . 1 , 2021 ( 56 ) References Cited U.S. PATENT DOCUMENTS 2016/0379041 A1 * 12/2016 Rhee 2017/0160813 A1 * 6/2017 Divakaran ( Continued ) G06T 19/20 382/118 GIOL 15/1815 FOREIGN PATENT DOCUMENTS A A * 7/2016 7/2016 ( Continued ) OTHER PUBLICATIONS Facial Expression Recognition Using Kinect Depth Sensor and Convolutional Neural Networks . Ijjina et al . Dec. 2014. * ( Continued ) Primary Examiner â€” Delomia L Gilliard ( 74 ) Attorney , Agent , or Firm Osha Bergman Watanabe & Burton LLP Foreign Application Priority Data ( 57 ) ABSTRACT Jul . 25 , 2017 ( CN ) 201710614130.8 ( 51 ) Int . Ci . GO6K 9/00 G06T 7/593 ( 52 ) U.S. Ci . CPC ( 2006.01 ) ( 2017.01 ) ( Continued ) GOOK 9/00302 ( 2013.01 ) ; G06K 9/00201 ( 2013.01 ) ; GO6K 9/00208 ( 2013.01 ) ; ( Continued ) ( 58 ) Field of Classification Search None See application file for complete search history . The present disclosure provides a method and apparatus for expression recognition , which is applied to the field of image processing . The method includes acquiring a three - dimen sional image of a target face and a two - dimensional image of the target face , where the three - dimensional image includes first depth information of the target face and first color information of the target face , and the two - dimensional image includes second color information of the target face . A first neural network classifies an expression of the target face according to the first depth information , the first color information , the second color information , and a first param eter to the target face . The first parameter includes at least ( Continued ) Acquire a three - dimensional image of a target face and a two - dimensional image of ihe target face , the three - dimensional inage including first depth informalion of the target face and first color information of the larget face , and the two - dimensional image including second color information of the target face nput the first depth information of the target face , the first color information of the target face , and the second color information of the target face to a first neura network Classify an expression of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the larget face , and a first parameter by the firsi neural network , the first parameter including at least one face expression category and first parameter data for recognizing the expression category of the target face US 11,023,715 B2 Page 2 one facial expression category and first parameter data for identifying an expression category of the target facial . The disclosed method and device can accurately recognize facial expressions under different facial positions and different illumination conditions . Claims , 8 Drawing Sheets ( 51 ) Int . Cl . G06T 7/30 GOON 3/08 GO6N 3/04 GO6K 9/46 GO6N 20/10 ( 52 ) U.S. CI . CPC ( 2017.01 ) ( 2006.01 ) ( 2006.01 ) ( 2006.01 ) ( 2019.01 ) ( 2013.01 ) ; G06T 2207/10028 ( 2013.01 ) ; GOOT 2207/20081 ( 2013.01 ) ; G06T 2207/30201 ( 2013.01 ) ( 56 ) References Cited U.S. PATENT DOCUMENTS 2018/0005018 A1 * 2018/0158246 A1 * 2018/0211102 A1 * 2020/0082160 A1 * 1/2018 Young 6/2018 Grau 7/2018 Alsmadi 3/2020 Li G06K 9/00228 GO2B 27/0093 G06K 9/00228 GOON 3/02 FOREIGN PATENT DOCUMENTS CN CN CN CN CN KR WO A A A A 5/2017 5/2017 6/2017 6/2017 A * 11/2017 A 5/2011 WO - 2017177634 A1 * 10/2017 G06K 9/00234 ( 2013.01 ) ; G06K 9/00248 ( 2013.01 ) ; G06K 9/00281 ( 2013.01 ) ; G06K 9/00288 ( 2013.01 ) ; G06K 9/00308 ( 2013.01 ) ; G06K 9/4652 ( 2013.01 ) ; GO6N 3/0454 ( 2013.01 ) ; G06N 3/08 ( 2013.01 ) ; G06N 3/082 ( 2013.01 ) ; G06N 20/10 ( 2019.01 ) ; G06T 7/30 ( 2017.01 ) ; G06T 7/593 ( 2017.01 ) ; GOOT 2207/10012 ( 2013.01 ) ; G06T 2207/10024 G06T 3/00 OTHER PUBLICATIONS Office Action issued in corresponding Chinese Application No. 201710614130.8 , dated May 12 , 2020 ( 43 pages ) . * cited by examiner U.S. Patent Jun . 1 , 2021 Sheet 1 of 8 US 11,023,715 B2 Acquire a three - dimensional image of a target face and a two - dimensional image of the target face , the three - dimensional image including first depth information of the target face and first color information of the target face , and the two - dimensional image including second color information of the target face Input the first depth information of the target face , the first color information of the target face , and the second color information of the target face to a first neural network Classify an expression of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the target face , and a first parameter by the first neural network , the first parameter including at least one face expression category and first parameter data for recognizing the expression category of the target face FIG . 1 U.S. Patent Jun . 1 , 2021 Sheet 2 of 8 US 11,023,715 B2 Acquire a three - dimensional image of a target face , the three - dimensional image including third depth information of the target face and fifth color information of the target face Input the third depth information of the target face to a second neural network and input the fifth color information of the target face to a third neural network Classify an expression of the target face according to the third depth information of the target face and a second parameter and output first classification data by the second neural network , and classify the expression of the target face according to the fifth color information of the target face and a third parameter and output second classification data by the third neural network , the second parameter including at least one face expression category and second parameter data for recognizing the expression categories of the target face , and the third parameter including the at least one face expression category and third parameter data for recognizing the expression calegories of the target face Output classification results on the expression of the target face according to the first classification data and the second classification data FIG . 2 U.S. Patent Jun . 1 , 2021 Sheet 3 of 8 US 11,023,715 B2 Acquiring a three - dimensional image of a target face , the three dimensional image including fifth depth information of the target face and seventh color information of the target face input the fifth depth information of the target face and the seventh color information of the target face to a fourth neural network Classify an expression of the target face according to the fifth depth information of the target face , the seventh color information of the target face , and a fourth parameter by the fourth neural network , the fourth parameter including at least one face expression category and fourth parameter data for recognizing the expression categories of the target face FIG . 3 U.S. Patent Jun . 1 , 2021 Sheet 4 of 8 US 11,023,715 B2 First acquisition module First input module First neural network FIG . 4 U.S. Patent Jun . 1 , 2021 Sheet 5 of 8 US 11,023,715 B2 Second acquisition module Second input module Second neural network Third neural network Second classification module FIG . 5 U.S. Patent Jun . 1 , 2021 Sheet 6 of 8 US 11,023,715 B2 Third acquisition module Third input module Fourth neural network FIG . 6 U.S. Patent Jun . 1 , 2021 Sheet 7 of 8 US 11,023,715 B2 Computer readable storage medium First processor Bus FIG . 7 U.S. Patent Jun . 1 , 2021 Sheet 8 of 8 US 11,023,715 B2 Memory Second processor Bus FIG . 8 US 11,023,715 B2 METHOD AND APPARATUS FOR EXPRESSION RECOGNITION CROSS - REFERENCE TO RELATED APPLICATIONS This application claims priority to Chinese Patent Appli- cation No. 201710614130.8 , filed on Jul . 26 , 2017 , which is hereby incorporated by reference in its entirety . FIELD OF THE INVENTION The present invention relates to an image processing method , and specifically , relates to a method and a device for expression recognition . BACKGROUND OF THE INVENTION the second color information of the target face to neural network , the method further comprises : performing the same first processing on the three - dimen sional image of the target face and the two - dimensional image of the target face , the first processing comprising at first least one of : determining feature points of the three - dimensional image of the target face and the two - dimensional image of the target face , and rotating the three - dimensional image of the target face and the two - dimensional image of the target face based on the feature points ; performing mirroring , linear transformation and affine transformation on the three - dimensional image of the target face and the two - dimensional image of the target face ; aligning the feature points of the three - dimensional image of the target face and the two - dimensional image of the target face with a set position ; With rapid development of artificial intelligence technol- performing contrast stretching on the three - dimensional ogy , deep learning has brought new hope to the technology 20 image of the target face and the two - dimensional image of and also broken a technical bottleneck . Expressions can be globally universal languages , regardless of races and nation- alities . In the human - computer interaction technology , expression recognition is very important , e.g. , when looking after an old man or a child , a robot can judge whether what 25 it did just now satisfies the old man or the child via the face expression of the old man or the child , thus learning the living habit and the character of the old man or the child . In the prior art , a face expression recognition algorithm the target face ; and performing image pixel value normalization processing on the three - dimensional image of the target face and the two - dimensional image of the target face . According to the first executable mode of the first aspect of the present invention , in a second executable mode of the first aspect of the present invention , performing image pixel value normalization processing on the three - dimensional image of the target face and the two - dimensional image of generally adopts two - dimensional image feature extraction 30 the target face comprises : and a classification algorithm to classify expressions so as to obtain expression results . When the face has a certain angle or the light condition is poor , e.g. , when the light is very weak or very strong , the feature information extracted via two - dimensional image features is greatly different or may be erroneous , which would lead to misjudgment of the algorithm on the expressions . SUMMARY OF THE INVENTION A method and a device for expression recognition , pro- vided by the present invention , can effectively solve the problem that the face expression recognition accuracy declines due to different face postures and different light According to a first aspect of the present invention , provided is a method for expression recognition , comprising acquiring a three - dimensional image of a target face and a two - dimensional image of the target face , the three- conditions . normalizing pixel values of channels of the three - dimen sional image of the target face and the two - dimensional image of the target face from [ 0 , 255 ] to [ 0 , 1 ] . According to the first aspect of the present invention and the first executable mode or the second executable mode of the first aspect of the present invention , in a third executable mode of the first aspect of the present invention , the first parameter data for recognizing the expression categories of the target face is obtained by training three - dimensional images of multiple face expression samples and two - dimen sional images of the face expression samples via the first the three - dimensional images of the face expression samples comprise second depth information of the face expression samples and third color information of the face expression samples ; and the two - dimensional images of the face expression samples comprise fourth color information of the face neural network ; dimensional image comprising first depth information of the 50 expression samples . target face and first color information of the target face , and the two - dimensional image comprising second color infor- mation of the target face ; inputting the first depth information of the target face , the According to the third executable mode of the first aspect of the present invention , in a fourth executable mode of the first aspect of the present invention , before the three - dimen sional images of the multiple face expression samples and first color information of the target face and the second color 55 the two - dimensional images of the face expression samples are trained via the first neural network , the method further information of the target face to a first neural network ; and classifying expressions of the target face according to the first depth information of the target face , the first color information of the target face , the second color information comprises : performing the same second processing on the three dimensional images of the face expression samples and the of the target face and a first parameter by the first neural 60 two - dimensional images of the face expression samples , the network , the first parameter comprising at least one face expression category and first parameter data for recognizing the expression categories of the target face . According to the first aspect of the present invention , in second processing comprising at least one of : determining feature points of the three - dimensional images of the face expression samples and the two - dimen sional images of the face expression samples , and rotating a first executable mode of the first aspect of the present 65 the three - dimensional images of the face expression samples invention , before inputting the first depth information of the target face , the first color information of the target face and and the two - dimensional images of the face expression samples based on the feature points ; US 11,023,715 B2 performing mirroring , linear transformation and affine transformation on the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples ; executable mode of the first aspect of the present invention , the third color information and the fourth color information are images of an RGB format or a YUV format . According to a second aspect provided by the present aligning the feature points of the three - dimensional 5 invention , provided is a device for expression recognition , images of the face expression samples and the two - dimen sional images of the face expression samples with a set comprising : sional images of the face expression samples and the two- 20 of the target face according to the first depth information of images of the face expression samples and the two - dimen- 10 prising first depth information of the target face and first position ; performing contrast stretching on the three - dimensional sion samples . sional images of the face expression samples ; and performing image pixel value normalization processing on the three - dimensional images of the face expression samples and the two - dimensional images of the face expres According to the fourth executable mode of the first aspect of the present invention , in a fifth executable mode of the first aspect of the present invention , performing image pixel value normalization processing on the three - dimen dimensional images of the face expression samples com- normalizing pixel values of channels of the three - dimen- sional images of the face expression samples and the two- prises : ] to [ 0 , 1 ] . According to the fourth or fifth executable mode of the first aspect of the present invention , in a sixth executable mode of the first aspect of the present invention , each of the face expression samples satisfies ( belongs to ) at least one of 30 the following face expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt ; each of the face expression sample , the second depth information of the face expression sample , the third color a first acquisition module , configured to acquire a three dimensional image of a target face and a two - dimensional image of the target face , the three - dimensional image com face ; color information of the target face , and the two - dimensional image comprising second color information of the target a first input module , configured to input the first depth information of the target face , the first color information of the target face and the second color information of the target face to a first neural network ; and the first neural network , configured to classify expressions the target face , the first color information of the target face , the second color information of the target face and a first parameter , the first parameter comprising at least one face expression category and first parameter data for recognizing module , According to the second aspect of the present invention , in a first executable mode of the second aspect of the present invention , the device further comprises a first processing the first processing module is configured to perform the same first processing on the three - dimensional image of the target face and the two - dimensional image of the target face , and input the three - dimensional image of the target face and the two - dimensional image of the target face subjected to the dimensional images of the face expression samples from [ 0 , 25 the expression categories of the target face . information of the face expression sample and the fourth 35 first processing to the first input module ; color information of the face expression sample satisfy ( belong to ) the same face expression category . According to the first aspect of the present invention and any of the first to sixth executable modes of the first aspect the first processing module comprises at least one of the following sub - modules : a first rotating sub - module , a first transformation sub - module , a first alignment sub - module , a first contrast stretching sub - module and a first normalization of the present invention , in a seventh executable mode of the 40 processing sub - module ; first aspect of the present invention , the face expression categories included by the first neural network comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . the first rotating sub - module is configured to determine feature points of the three - dimensional image of the target face and the two - dimensional image of the target face , and rotate the three - dimensional image of the target face and the According to any of the first to seventh executable modes 45 two - dimensional image of the target face based on the any of the first to eighth executable modes of the first aspect 50 the two - dimensional image of the target face ; of the first aspect of the present invention , in an eighth executable mode of the first aspect of the present invention , the feature points are eye points . According to the first aspect of the present invention and of the present invention , in a ninth executable mode of the first aspect of the present invention , the first neural network comprises a first convolutional neural network . According to the ninth executable mode of the first aspect of the present invention , in a tenth executable mode of the 55 first aspect of the present invention , the first convolutional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer and two fully- feature points ; the first transformation sub - module is configured to per form mirroring , linear transformation and affine transforma tion on the three - dimensional image of the target face and the first alignment sub - module is configured to align the feature points of the three - dimensional image of the target face and the two - dimensional image of the target face with the first contrast stretching sub - module is configured to perform contrast stretching on the three - dimensional image of the target face and the two - dimensional image of the target face ; and a set position ; connected layers . the first normalization processing sub - module is config According to the first aspect of the present invention and 60 ured to perform image pixel value normalization processing any of the first to tenth executable modes of the first aspect of the present invention , in an eleventh executable mode of the first aspect of the present invention , the first color information and the second color information are images of an RGB format or a YUV format . According to any of the third to eleventh executable modes of the first aspect of the present invention , in a twelfth on the three - dimensional image of the target face and the two - dimensional image of the target face . According to the first executable mode of the second aspect of the present invention , in a second executable mode of the second aspect of the present invention , the first normalization processing sub - module is specifi cally configured to normalize pixel values of channels of the US 11,023,715 B2 three - dimensional image of the target face and the two- dimensional image of the target face from [ 0 , 255 ] to [ 0 , 1 ] . According to the second aspect of the present invention and the first or second executable mode of the second aspect of the present invention , in a third executable mode of the 5 second aspect of the present invention , the first parameter data for recognizing the expression categories of the target face is obtained by training three- dimensional images of multiple face expression samples and tion , According to any of the third to fifth executable modes of the second aspect of the present invention , in a sixth executable mode of the second aspect of the present inven each of the face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt ; each of the face expression samples , the second depth information of the face expression sample , the second color two - dimensional images of the face expression samples via 10 information of the face expression sample and the third color the first neural network ; the three - dimensional images of the face expression samples comprise second depth information of the face expression samples and third color information of the face expression samples ; and the two - dimensional images of the face expression samples comprise fourth color information of the face expression samples . According to the third executable mode of the second aspect of the present invention , in a fourth executable mode 20 of the second aspect of the present invention , the device further comprises a second processing module , the second processing module is configured to perform the same second processing on the three - dimensional images of the face expression samples and the two - dimensional 25 images of the face expression samples , and input the three- dimensional images of the face expression samples and the two - dimensional images of the face expression samples subjected to the second processing to the first input module ; the second processing module comprises a second rotat- 30 ing sub - module , a second transformation sub - module , a second alignment sub - module , a second contrast stretching sub - module and a second normalization processing sub- information of the face expression sample satisfy ( belong to ) the same face expression category . According to the second aspect of the present invention and any of the first to sixth executable mode of the second aspect of the present invention , in a seventh executable mode of the second aspect of the present invention , the face expression categories included by the first neural network comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . According to the second aspect of the present invention and any of the first to seventh executable mode of the second aspect of the present invention , in an eighth executable mode of the second aspect of the present invention , the feature points are eye points . According to the second aspect of the present invention and any of the first to eighth executable mode of the second aspect of the present invention , in a ninth executable mode of the second aspect of the present invention , the first neural network comprises a first convolutional neural network . According to the ninth executable mode of the second aspect of the present invention , in a tenth executable mode of the second aspect of the present invention , the first convolutional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer and module ; the second rotating sub - module is configured to determine 35 two fully - connected layers . feature points of the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples , and rotate the three - dimensional images of the face expression samples and the two - dimen- sional images of the face expression samples based on the 40 feature points ; the second transformation sub - module is configured to perform mirroring , linear transformation and affine transfor- mation on the three - dimensional images of the face expres- sion samples and the two - dimensional images of the face 45 invention , expression samples ; the second alignment sub - module is configured to align the feature points of the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples with a set position ; the second contrast stretching sub - module is configured to perform contrast stretching of images on the three - dimen- sional images of the face expression samples and the two- dimensional images of the face expression samples ; and the second normalization processing sub - module is con- 55 figured to perform image pixel value normalization process- ing on the three - dimensional images of the face expression samples and the two - dimensional images of the face expres- ing : face ; According to the second aspect of the present invention and any of the first to tenth executable mode of the second aspect of the present invention , in an eleventh executable mode of the second aspect of the present invention , the first color information and the second color informa tion are images of an RGB format or a YUV format . According to any of the third to eleventh executable modes of the second aspect of the present invention , in a twelfth executable mode of the second aspect of the present the third color information and the fourth color informa tion are images of an RGB format or a YUV format . According to a third aspect of the present invention , provided is a method for expression recognition , compris acquiring a three - dimensional image of a target face , the three - dimensional image comprising third depth information of the target face and fifth color information of the target inputting the third depth information of the target face to a second neural network and inputting the fifth color infor mation of the target face to a third neural network ; classifying expressions of the target face according to the third depth information of the target face and a second According to the fourth executable mode of the second 60 parameter and outputting first classification data by the aspect of the present invention , in a fifth executable mode of the second aspect of the present invention , the second normalization processing sub - module is spe- cifically configured to normalize pixel values of channels of second neural network , and classifying expressions of the target face according to the fifth color information of the target face and a third parameter and outputting second classification data by the third neural network , the second the three - dimensional images of the face expression samples 65 parameter comprising at least one face expression category and the two - dimensional images of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] . and second parameter data for recognizing the expression categories of the target face , and the third parameter com sion samples . US 11,023,715 B2 prising the at least one face expression category and third parameter data for recognizing the expression categories of performing image pixel value normalization processing on the third depth information of the target face and the fifth the target face ; and color information of the target face . outputting classification results on the expressions of the According to the second executable mode of the third target face according to the first classification data and the 5 aspect of the present invention , in a third executable mode second classification data . According to the third aspect of the present invention , in a first executable mode of the third aspect of the present invention , outputting classification results on the expressions of the target face according to the first classification data and the second classification data comprises : inputting the first classification data and the second clas- sification data and outputting classification results on the expressions of the target face according to the first classi- fication data , the second classification data and support vector machine parameter data by a support vector machine , the support vector machine comprising the at least one face of the third aspect of the present invention , performing image pixel value normalization processing on the third depth information of the target face comprises : normalizing pixel values of the third depth information of the target face from [ 0 , 255 ] to [ 0 , 1 ] ; or performing image pixel value normalization processing on the third depth information of the target face and the fifth color information of the target face comprises : normalizing pixel values of channels of the third depth information of the target face and the fifth color information of the target face from [ 0 , 255 ] to [ 0 , 1 ] . According to the third aspect of the present invention or any of the first to third executable modes of the third aspect expression category and the support vector machine param- 20 of the present invention , in a fourth executable mode of the eter data for recognizing the expression category of the third aspect of the present invention , target face . According to the third aspect of the present invention or the first executable mode of the third aspect of the present invention , in a second executable mode of the third aspect of 25 the present invention , before inputting the third depth information of the target face to a second neural network and inputting the fifth color information of the target face to a third neural network , the method further comprises : performing third processing on the third depth informa tion of the target face , the third processing comprising at determining feature points of the third depth information of the target face , and rotating the third depth information of 35 the target face based on the feature points ; performing mirroring , linear transformation and affine transformation on the third depth information of the target least one of : the second parameter data is obtained by training fourth depth information of multiple face expression samples via the second neural network ; and the third parameter data is obtained by training sixth color information of the multiple face expression samples via the third neural network . According to the fourth executable mode of the third aspect of the present invention , in a fifth executable mode of the third aspect of the present invention , before the fourth depth information of the multiple face expression samples is trained via the second neural network , the method further comprises : performing fourth processing on the fourth depth infor mation of the face expression samples , the fourth processing comprising at least one of : determining feature points of the fourth depth information of the face expression samples , and rotating the fourth depth aligning the feature points of the third depth information 40 information of the face expression samples based on the face ; of the target face with a set position ; performing contrast stretching on the third depth infor- mation of the target face ; and performing image pixel value normalization processing on the third depth information of the target face ; or before inputting the third depth information of the target face to a second neural network and inputting the fifth color information of the target face to a third neural network , the method further comprises : feature points ; performing mirroring , linear transformation and affine transformation on the fourth depth information of the face expression samples ; aligning the feature points of the fourth depth information of the face expression samples with a set position ; performing contrast stretching on the fourth depth infor mation of the face expression samples ; and performing image pixel value normalization processing performing the same third processing on the third depth 50 on the fourth depth information of the face expression information of the target face and the fifth color information of the target face , the third processing comprising at least samples ; determining feature points of the third depth information of the target face and feature points of the fifth color 55 sixth color information of the face expression samples is trained via the third neural network , the method further or , before the fourth depth information of the face expres sion samples is trained via the second neural network and the one of : information of the target face , and rotating the third depth information of the target face and the fifth color information of the target face based on the feature points ; performing mirroring , linear transformation and affine comprises : performing the same fourth processing on the fourth depth information of the face expression samples and the sixth transformation on the third depth information of the target 60 color information of the face expression samples , the fourth face and the fifth color information of the target face ; aligning the feature points of the third depth information of the target face and the fifth color information of the target face with a set position ; processing comprising at least one of : determining feature points of the fourth depth information of the face expression samples and feature points of the sixth color information of the face expression samples , and rotat performing contrast stretching on the third depth infor- 65 ing the fourth depth information of the face expression mation of the target face or the fifth color information of the target face ; and samples and the sixth color information of the face expres sion samples based on the feature points ; US 11,023,715 B2 performing mirroring , linear transformation and affine transformation on the fourth depth information of the face expression samples and the sixth color information of the face expression samples ; executable mode of the third aspect of the present invention , the feature points are eye points . According to the third aspect of the present invention and any of the first to tenth executable modes of the third aspect aligning the feature points of the fourth depth information 5 of the present invention , in an eleventh executable mode of of the face expression samples and the sixth color informa tion of the face expression samples with a set position ; performing contrast stretching on the fourth depth infor mation of the face expression samples and the sixth color information of the face expression samples ; and performing image pixel value normalization processing on the fourth depth information of the face expression samples and the sixth color information of the face expres sion samples . According to the fifth executable mode of the third aspect of the present invention , in a sixth executable mode of the third aspect of the present invention , performing image pixel value normalization processing the third aspect of the present invention , the second neural network comprises a second convolu tional neural network , and the third neural network com prises a third convolutional neural network . According to the eleventh executable mode of the third aspect of the present invention , in a twelfth executable mode of the third aspect of the present invention , the second convolutional neural network comprises three convolutional layers , three down - sampling layers , one drop out layer and two fully - connected layers ; and the third convolutional neural network comprises four convolutional layers , four down - sampling layers , one drop out layer and two fully - connected layers . According to the third aspect of the present invention and on the fourth depth information of the face expression 20 any of the first to twelfth executable modes of the third samples comprises : normalizing pixel values of the fourth depth information of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] ; or performing image pixel value normalization processing on the fourth depth information of the face expression 25 samples and the sixth color information of the face expres- sion samples comprises : normalizing pixel values of channels of the fourth depth information of the face expression samples and the sixth color information of the face expression samples from [ 0 , 30 a YUV format . ] to [ 0 , 1 ] . According to any of the fourth to sixth executable modes of the third aspect of the present in ion , in a seventh executable mode of the third aspect of the present invention , format . aspect of the present invention , in a thirteenth executable mode of the third aspect of the present invention , the fifth color information is an image of an RGB format or a YUV According to the third aspect of the present invention and any of the fourth to thirteenth executable modes of the third aspect of the present invention , in a fourteenth executable mode of the third aspect of the present invention , the sixth color information is images of an RGB format or According to a fourth aspect of the present invention , provided is a device for expression recognition , comprising a second acquisition module , a second input module , a second neural network , a third neural network and a second the support vector machine parameter data for recogniz- 35 classification module , wherein ing the expression category of the target face is obtained by : training the second neural network with the fourth depth information of the facial expression samples , training the third neural network with the sixth color information of the facial expression samples , combining corresponding output 40 data from the second fully - connected layer of the second neural network and the second fully - connected layer of the third neural network as inputs , and training the support vector machine with the inputs and corresponding expres- sion labels of the facial expression samples . According to any of the fourth to seventh executable modes of the third aspect of the present invention , in an eighth executable mode of the third aspect of the present the second acquisition module is configured to acquire a three - dimensional image of a target face , the three - dimen sional image comprising third depth information of the target face and fifth color information of the target face ; the second input module is configured to input the third depth information of the target face to the second neural network and input the fifth color information of the target face to the third neural network ; the second neural network is configured to classify expressions of the target face according to the third depth information of the target face and a second parameter and output first classification data , and the third neural network is configured to classify expressions of the target face according to the fifth color information of the target face and each of the face expression sample satisfies ( belongs to ) 50 a third parameter and output second classification data , the invention , at least one of the following face expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt ; and second parameter comprising at least one face expression category and second parameter data for recognizing the expression categories of the target face , and the third param eter comprising the at least one face expression category and each of the face expression samples , the fourth depth information of the face expression sample and the sixth 55 third parameter data for recognizing the expression catego color information of the face expression sample satisfy ( belong to the same face expression category . According to the third aspect of the present invention and any of the first to eighth executable modes of the third aspect ries of the target face ; and the second classification module is configured to output classification results on the expressions of the target face according to the first classification data and the second of the present invention , in a ninth executable mode of the 60 classification data . third aspect of the present invention , the face expression categories included by the second neural network and the face expression categories included by the third neural network include at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . 65 According to any of the second to ninth executable modes of the third aspect of the present invention , in a tenth According to the fourth aspect of the present invention , in a first executable mode of the fourth aspect of the present invention , the second classification module comprises a support vector machine , and the support vector machine is configured to input the first classification data and the second classification data , and output the classification results on the expressions of the US 11,023,715 B2 target face according to the first classification data , the second classification data and support vector machine parameter data , the support vector machine comprising the at least one face expression category and the support vector According to the second executable mode of the fourth aspect of the present invention , in a third executable mode of the fourth aspect of the present invention , the third normalization processing sub - module is specifi machine parameter data for recognizing the expression 5 cally configured to normalize pixel values of the third depth category of the target face . According to the fourth aspect of the present invention and the first executable mode of the fourth aspect of the present invention , in a second executable mode of the fourth aspect of the present invention , the device further comprises a third processing module , the third processing module is configured to perform third processing on the third depth information of the target face , and input the third depth information of the target face subjected to the third processing to the second input module ; the third processing module comprises at least one of a third rotating sub - module , a third transformation sub - mod- ule , a third alignment sub - module , a third contrast stretching sub - module and a third normalization processing sub - mod- 20 ule ; feature the third rotating sub - module is configured to determine ints of the third depth information of the target face , and rotate the third depth information of the target face based on the feature points ; the third transformation sub - module is configured to perform mirroring , linear transformation and affine transfor- mation on the third depth information of the target face ; the third alignment sub - module is configured to align the information of the target face from [ 0 , 255 ] to [ 0 , 1 ] ; or , the third normalization processing sub - module is specifi cally configured to normalize pixel values of channels of the third depth information of the target face and the fifth color information of the target face from [ 0 , 255 ] to [ 0 , 1 ] . According to the fourth aspect of the present invention and the first to third executable modes of the fourth aspect of the present invention , in a fourth executable mode of the fourth aspect of the present invention , the second parameter data is obtained by training fourth depth information of multiple face expression samples via the second neural network ; and the third parameter data is obtained by training sixth color information of the multiple face expression samples via the third neural network . According to the fourth executable mode of the fourth aspect of the present invention , in a fifth executable mode of the fourth aspect of the present invention , the device com prises a fourth processing module , the fourth processing module is configured to perform fourth processing on the fourth depth information of the face expression samples , and input the fourth depth information feature points of the third depth information of the target 30 of the face expression samples subjected to the fourth processing to the second input module ; face with a set position ; the third contrast stretching sub - module is configured to perform contrast stretching on the third depth information of the target face ; and the third normalization processing sub - module is config ured to perform image pixel value normalization processing on the third depth information of the target face ; or , the fourth processing module comprises at least one of a fourth rotating sub - module , a fourth transformation sub module , a fourth alignment sub - module , a fourth contrast stretching sub - module and a fourth normalization process ing sub - module ; the fourth rotating sub - module is configured to determine feature points of the fourth depth information of the face expression samples , and rotate the fourth depth information the third processing module is further configured to 40 of the face expression samples based on the feature points ; perform the same third processing on the third depth infor- mation of the target face and the fifth color information of the target face , and input the third depth information of the target face and the fifth color information of the target face subjected to the third processing to the second input module ; 45 the third rotating sub - module is further configured to determine feature points of the third depth information of the target face and feature points of the fifth color information of the target face , and rotate the third depth information of sion samples ; the fourth transformation sub - module is configured to perform mirroring , linear transformation and affine transfor mation on the fourth depth information of the face expres the fourth alignment sub - module is configured to align the feature points of the fourth depth information of the face expression samples with a set position ; the fourth contrast stretching sub - module is configured to perform contrast stretching on the fourth depth information the target face and the fifth color information of the target 50 of the face expression samples ; and face based on the feature points ; the third transformation sub - module is further configured to perform mirroring , linear transformation and affine trans- formation on the third depth information of the target face and the fifth color information of the target face ; the third alignment sub - module is further configured to align the feature points of the third depth information of the target face and the fifth color information of the target face with a set position ; samples ; or , the fourth normalization processing sub - module is con figured to perform image pixel value normalization process ing on the fourth depth information of the face expression the fourth processing module is further configured to perform fourth processing on the fourth depth information of the face expression samples and the sixth color information of the face expression samples , and input the fourth depth the third contrast stretching sub - module is further config- 60 information of the face expression samples and the sixth ured to perform contrast stretching on the third depth information of the target face or the fifth color information of the target face ; and the third normalization processing sub - module is further color information of the face expression samples subjected to the fourth processing to the second input module ; the fourth rotating sub - module is further configured to determine feature points of the fourth depth information of configured to perform image pixel value normalization 65 the face expression samples and feature points of the sixth processing on the third depth information of the target face and the fifth color information of the target face . color information of the face expression samples , and rotate the fourth depth information of the face expression samples US 11,023,715 B2 and the sixth color information of the face expression samples based on the feature points ; the fourth transformation sub - module is further config- ured to perform mirroring , linear transformation and affine by the third neural network comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . According to any of the second to ninth executable modes of the fourth aspect of the present invention , in a tenth transformation on the fourth depth information of the face 5 executable mode of the fourth aspect of the present inven expression samples and the sixth color information of the tion , the feature points are eye points . face expression samples ; the fourth alignment sub - module is further configured to align the feature points of the fourth depth information of the face expression samples and the sixth color information of the face expression samples with a set position ; the fourth contrast stretching sub - module is further con- figured to perform contrast stretching on the fourth depth information of the face expression samples or the sixth color information of the face expression samples ; and the fourth normalization processing sub - module is further configured to perform image pixel value normalization processing on the fourth depth information of the face According to the fourth aspect of the present invention and any of the first to tenth executable modes of the fourth aspect of the present invention , in an eleventh executable mode of the fourth aspect of the present invention , the second neural network comprises a second convolu tional neural network , and the third neural network com prises a third convolutional neural network . According to the eleventh executable mode of the fourth aspect of the present invention , in a twelfth executable mode of the fourth aspect of the present invention , the second convolutional neural network comprises three convolutional layers , three down - sampling layers , one drop expression samples and the sixth color information of the 20 out layer and two fully - connected layers ; and the fourth normalization processing sub - module is spe- 25 and the first to twelfth executable modes of the fourth aspect face expression samples . According to the fifth executable mode of the fourth aspect of the present invention , in a sixth executable mode of the fourth aspect of the present invention , cifically configured to normalize pixel values of the fourth depth information of the face expression samples from [ 0 , ] to [ 0 , 1 ] ; or , the fourth normalization processing sub - module is spe- 30 cifically configured to normalize pixel values of channels of the fourth depth information of the face expression samples and the sixth color information of the face expression tion , samples from [ 0 , 255 ] to [ 0 , 1 ] . According to any of the fourth to sixth executable modes 35 a YUV format . of the fourth aspect of the present invention , in a seventh executable mode of the fourth aspect of the present inven- tion , the third convolutional neural network comprises four convolutional layers , four down - sampling layers , one drop out layer and two fully - connected layers . According to the fourth aspect of the present invention of the present invention , in a thirteenth executable mode of the fourth aspect of the present invention , the fifth color information is an image of an RGB format or a YUV format . According to the fourth to thirteenth executable modes of the fourth aspect of the present invention , in a fourteenth executable mode of the fourth aspect of the present inven the sixth color information is images of an RGB format or According to a fifth aspect of the present invention , provided is a method for expression recognition , comprising acquiring a three - dimensional image of a target face , the three - dimensional image comprising fifth depth information the support vector machine parameter data for recogniz- ing the expression category of the target face is obtained by : 40 of the target face and seventh color information of the target training the second neural network with the fourth depth information of the facial expression samples , training the third neural network with the sixth color information of the facial expression samples , combining corresponding output data from the second fully - connected layer of the second 45 neural network and the second fully - connected layer of the third neural network as inputs , and training the support vector machine with the inputs and corresponding expres- sion labels of the facial expression samples . face ; inputting the fifth depth information of the target face and the seventh color information of the target face to a fourth neural network ; and classifying expressions of the target face according to the fifth depth information of the target face , the seventh color information of the target face and a fourth parameter by the fourth neural network , the fourth parameter comprising at least one face expression category and fourth parameter data According to any of the fourth to seventh executable 50 for recognizing the expression categories of the target face . modes of the fourth aspect of the present invention , in an eighth executable mode of the fourth aspect of the present According to the fifth aspect of the present invention , in a first executable mode of the fifth aspect of the present each of the face expression samples satisfies ( belongs to ) before inputting the fifth depth information of the target at least one of the following face expression categories : fear , 55 face and the seventh color information of the target face to sadness , joy , anger , disgust , surprise , nature and contempt ; invention , invention , and each of the face expression samples , the fourth depth information of the face expression sample and the sixth color information of the face expression sample satisfy 60 ( belong to the same face expression category . According to the fourth aspect of the present invention and any of the first to eighth executable modes of the fourth aspect of the present invention , in a ninth executable mode of the fourth aspect of the present invention , the face expression categories included by the second neural network and the face expression categories included least one of : a fourth neural network , the method further comprises : performing fifth processing on the three - dimensional image of the target face , the fifth processing comprising at determining feature points of the three - dimensional image of the target face , and rotating the three - dimensional image of the target face based on the feature points ; performing mirroring , linear transformation and affine transformation on the three - dimensional image of the target aligning the feature points of the three - dimensional image of the target face with a set position ; face ; US 11,023,715 B2 performing contrast stretching on the three - dimensional image of the target face ; and performing image pixel value normalization processing on the three - dimensional image of the target face . of the present invention , in a seventh executable mode of the fifth aspect of the present invention , the face expression categories included by the fourth neural network comprise at least one of : fear , sadness , joy , According to the first executable mode of the fifth aspect 5 anger , disgust , surprise , nature and contempt . of the present invention , in a second executable mode of the fifth aspect of the present invention , the image pixel value normalization processing on the three - dimensional image of the target face comprises : normalizing pixel values of channels of the three - dimen- 10 sional image of the target face from [ 0 , 255 ] to [ 0 , 1 ] . According to the fifth aspect of the present invention and the first or second executable mode of the fifth aspect of the present invention , in a third executable mode of the fifth aspect of the present invention , the fourth parameter data is obtained by training three dimensional images of multiple face expression samples via the fourth neural network ; and the three - dimensional images of the face expression samples comprise sixth depth information of the face 20 expression samples and eighth color information of the face expression samples . According to the third executable mode of the fifth aspect of the present invention , in a fourth executable mode of the fifth aspect of the present invention , before the three - dimensional images of the multiple face expression samples are trained via the fourth neural network , the method further comprises : performing sixth processing on the three - dimensional images of the face expression samples , the sixth processing 30 comprising at least one of : determining feature points of the three - dimensional images of the face expression samples , and rotating the three - dimensional images of the face expression samples based on the feature points ; performing mirroring , linear transformation and affine transformation on the three - dimensional images of the face expression samples ; aligning the feature points of the three - dimensional According to any of the first to seventh executable modes of the fifth aspect of the present invention , in an eighth executable mode of the fifth aspect of the present invention , the feature points are eye points . According to the fifth aspect of the present invention and any of the first to eighth executable modes of the fifth aspect of the present invention , in a ninth executable mode of the fifth aspect of the present invention , the fourth neural network comprises a fourth convolu tional neural network . According to the ninth executable mode of the fifth aspect of the present invention , in a tenth executable mode of the fifth aspect of the present invention , the fourth convolutional neural network comprises one segmentation layer , eight convolutional layers , eight down sampling layers , two dropout layers and five fully - connected According to the fifth aspect of the present invention and the first to tenth executable modes of the fifth aspect of the present invention , in an eleventh executable mode of the fifth aspect of the present invention , the seventh color information is an image of an RGB format or a YUV format . According to the third to eleventh executable modes of the fifth aspect of the present invention , in a twelfth execut able mode of the fifth aspect of the present invention , the eighth color information is images of an RGB format or a layers . YUV format . According to a sixth aspect of the present invention , provided is a device for expression recognition , comprising : a third acquisition module , configured to acquire a three dimensional image of a target face , the three - dimensional image comprising fifth depth information of the target face images of the face expression samples with a set position ; 40 and seventh color information of the target face ; samples . performing contrast stretching on the three - dimensional images of the face expression samples ; and performing image pixel value normalization processing on the three - dimensional images of the face expression According to the fourth executable mode of the fifth aspect of the present invention , in a fifth executable mode of the fifth aspect of the present invention , the image pixel value normalization processing on the a third input module , configured to input the fifth depth information of the target face and the seventh color infor mation of the target face to a fourth neural network ; and the fourth neural network , configured to classify expres sions of the target face according to the fifth depth infor mation of the target face , the seventh color information of the target face and a fourth parameter , the fourth parameter comprising at least one face expression category and fourth parameter data for recognizing the expression categories of three - dimensional images of the face expression samples 50 the target face . comprises : to [ 0 , 1 ] . normalizing pixel values of channels of the three - dimen- sional images of the face expression samples from [ 0 , 255 ] According to the sixth aspect of the present invention , in a first executable mode of the sixth aspect of the present invention , the device further comprises a fifth processing module , According to any of the third to fifth executable modes of 55 the fifth aspect of the present invention , in a sixth executable mode of the fifth aspect of the present invention , each of the face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , the fifth processing module is configured to perform fifth processing on the three - dimensional image of the target face , and input the three - dimensional image of the target face subjected to the fifth processing to the third input module ; the fifth processing module comprises at least one of the sadness , joy , anger , disgust , surprise , nature and contempt ; 60 following sub - modules : a fifth rotating sub - module , a fifth and each of the face expression samples , the sixth depth information of the face expression sample and the eighth color information of the face expression sample satisfy ( belong to ) the same face expression category . According to the fifth aspect of the present invention and any of the first to sixth executable modes of the fifth aspect transformation sub - module , a fifth alignment sub - module , a fifth contrast stretching sub - module and a fifth normalization processing sub - module ; the fifth rotating sub - module is configured to determine feature points of the three - dimensional image of the target face , and rotate the three - dimensional image of the target face based on the feature points ; US 11,023,715 B2 the fifth transformation sub - module is configured to per- form mirroring , linear transformation and affine transforma- tion on the three - dimensional image of the target face ; the fifth alignment sub - module is configured to align the feature points of the three - dimensional image of the target face with a set position ; the fifth contrast stretching sub - module is configured to perform contrast stretching on the three - dimensional image of the target face ; and the sixth normalization processing sub - module is specifi cally configured to normalize pixel values of channels of the three - dimensional images of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] . According to any of the third to fifth executable modes of the sixth aspect of the present invention , in a sixth execut able mode of the sixth aspect of the present invention , each of the face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , the fifth normalization processing sub - module is config 10 sadness , joy , anger , disgust , surprise , nature and contempt ; and of the present invention , in a second executable mode of the 15 ( belong to ) the same face expression category . ured to perform image pixel value normalization processing on the three - dimensional image of the target face . According to the first executable mode of the sixth aspect sixth aspect of the present invention , the fifth normalization processing sub - module is specifi cally configured to normalize pixel values of channels of the three - dimensional image of the target face from [ 0 , 255 ] to According to the sixth aspect of the present invention and the first or second executable mode of the sixth aspect of the present invention , in a third executable mode of the sixth aspect of the present invention , [ 0 , 1 ] . categories of the target face is obtained by training three dimensional images of multiple face expression samples via the fourth neural network ; and comprises a sixth processing module , the sixth processing module is configured to perform fifth processing on the three - dimensional images of the face expression samples , and input the three - dimensional images 40 of the face expression samples subjected to the fifth pro- cessing to the third input module ; the sixth processing module comprises a sixth rotating sub - module , a sixth transformation sub - module , a sixth alignment sub - module , a sixth contrast stretching sub - mod- 45 ule and a sixth normalization processing sub - module ; the sixth rotating sub - module is configured to determine feature points of the three - dimensional images of the face expression samples , and rotate the three - dimensional images each of the face expression samples , the sixth depth information of the face expression sample and the eighth color information of the face expression sample satisfy According to the sixth aspect of the present invention and any of the first to sixth executable modes of the sixth aspect of the present invention , in a seventh executable mode of the sixth aspect of the present invention , the face expression categories included by the fourth neural network comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . According to any of the first to seventh executable modes of the sixth aspect of the present invention , in an eighth the feature points are eye points . According to the sixth aspect of the present invention and any of the first to eighth executable modes of the sixth aspect of the present invention , in a ninth executable mode of the the fourth neural network comprises a fourth convolu tional neural network . According to the ninth executable mode of the sixth aspect of the present invention , in a tenth executable mode layers . the fourth convolutional neural network comprises one segmentation layer , eight convolutional layers , eight down sampling layers , two dropout layers and five fully - connected According to the sixth aspect of the present invention and any of the first to tenth executable modes of the sixth aspect of the present invention , in an eleventh executable mode of the sixth aspect of the present invention , the seventh color information is an image of an RGB format or a YUV format . According to any of the third to eleventh executable modes of the sixth aspect of the present invention , in a twelfth executable mode of the sixth aspect of the present the eighth color information is images of an RGB format invention , the three - dimensional images of the face expression samples comprise sixth depth information of the face 30 sixth aspect of the present invention , expression samples and eighth color information of the face expression samples . According to the third executable mode of the sixth aspect of the present invention , in a fourth executable mode of the 35 of the sixth aspect of the present invention , sixth aspect of the present invention , the device further the fourth parameter data for recognizing the expression 25 executable mode of the sixth aspect of the present invention , of the face expression samples based on the feature points ; 50 or a YUV format . the sixth transformation sub - module is configured to perform mirroring , linear transformation and affine transfor- mation on the three - dimensional images of the face expres- sion samples ; According to a seventh aspect of the present invention , provided is a computer readable storage medium , which stores a computer program , wherein the computer program , when executed by a first processor , implements the steps in the sixth alignment sub - module is configured to align the 55 any executable mode of the first aspect of the present feature points of the three - dimensional images of the face expression samples with a set position ; the sixth contrast stretching sub - module is configured to perform contrast stretching on the three - dimensional images of the face expression samples ; and the sixth normalization processing sub - module is config- ured to perform image pixel value normalization processing on the three - dimensional images of the face expression samples . invention and the first to twelfth executable modes of the first aspect of the present invention , the third aspect of the present invention and the first to fourteenth executable modes of the third aspect of the present invention , and the fifth aspect of the present invention and the first to twelfth executable modes of the fifth aspect of the present invention . According to an eighth aspect of the present invention , provided is a device for expression recognition , comprising a memory , a second processor and a computer program According to the fourth executable mode of the sixth 65 which is stored in the memory and can be run on the second aspect of the present invention , in a fifth executable mode of the sixth aspect of the present invention , processor , wherein the computer program , when executed by the second processor , implements the steps in any execut US 11,023,715 B2 able mode of the first aspect of the present invention and the first to twelfth executable modes of the first aspect of the present invention , the third aspect of the present invention and the first to fourteenth executable modes of the third It should be noted that , for the sake of compactness and clearness of the drawings , the components shown in the drawings do not need to be drawn to scale . For example , for the sake of clearness , the sizes of some components can be aspect of the present invention , and the fifth aspect of the 5 increased relative to other components . In addition , refer present invention and the first to twelfth executable modes of the fifth aspect of the present invention . The method and device for expression recognition , pro vided by the present invention , can effectively solve the problem that the face expression recognition accuracy declines due to different face postures and different light conditions , and improve the accuracy of face expression recognition of the target face at different face postures and in different light conditions . BRIEF DESCRIPTION OF THE DRAWINGS FIG . 1 is a flow diagram of a method for expression recognition provided by embodiment 1 of the present inven- FIG . 2 is a flow diagram of another method for expression recognition provided by embodiment 2 of the present inven- tion ; tion ; ence signs can be repeated , where appropriate , in the draw ings to indicate corresponding or similar components . It should be noted that , since videos and the like are composed of a plurality of pictures , the processing methods for pictures , imaging , images and the like described in the embodiments of the present invention can be applied to the videos and the like . Those skilled in the art could modify the methods disclosed in the present invention to processing methods applied to videos and the like without any creative effort , and these modified methods fall into the protection scope of the present invention . Each embodiment of the present invention is elaborated by using a human face as an example , and the technical solutions of the present invention are also applicable to recognition of face expressions of different objects , e.g. , different animals , or target objects having characteristics similar to those of a face . A method for expression recognition provided by embodi ment 1 of the present invention will be specifically elabo rated below in combination with FIG . 1. As shown in FIG . Step 101 : acquiring a three - dimensional image of a target face and a two - dimensional image of the target face , the three - dimensional image comprising first depth information of the target face and first color information of the target color information of the target face . Optionally , this acquisition step may be acquiring a three dimensional image of a target face and a two - dimensional image of the target face , which are photographed by a photographic device , from a memory . FIG . 3 is a flow diagram of a further method for expres sion recognition provided by embodiment 3 of the present 25 1 , the method comprises : invention ; FIG . 4 is a structural schematic diagram of a device for expression recognition provided by embodiment 4 of the FIG . 5 is a structural schematic diagram of another device 30 face , and the two - dimensional image comprising second for expression recognition provided by embodiment 5 of the present invention ; present invention ; FIG . 6 is a structural schematic diagram of a further device for expression recognition provided by embodiment of the present invention ; FIG . 7 is a structural schematic diagram of yet another device for expression recognition provided by embodiment Optionally , the three - dimensional image of the target face and the two - dimensional image of the target face described FIG . 8 is a structural schematic diagram of still another Optionally , the foregoing first color information and the device for expression recognition provided by embodiment 40 second color information may be images of an RGB format above may be color images . of the present invention ; of the present invention . DETAILED DESCRIPTION OF THE format . EMBODIMENTS The technical solutions in the embodiments of the present invention will be described in detail below in combination with the accompanying drawings in the embodiments of the present invention . or a YUV format , or images of another formats that can be converted to and from the foregoing RGB format or YUV Step 102 : inputting the first depth information of the target face , the first color information of the target face and the second color information of the target face to a first neural network . Optionally , input to the first neural network may be a depth image of the target face , an RGB image of the three - dimensional image of the target face and an RGB The terms â€œ first â€ , â€œ second â€ and the like in the specifica- 50 image of the two - dimensional image of the target face ; and tion , claims and drawings of the present invention are used for distinguishing different objects , rather than limiting specific sequences . The term â€œ and / or â€ in the embodiments of the present invention is merely a correlation for describing correlated 55 target face . objects , and indicates three possible relations , e.g. , A and / or B may indicate three situations : A exists separately , A and B exit simultaneously , and B exists separately . In the embodiments of the present invention , the words input to the first neural network may also be a depth image of the target face , three channels of an RGB image of the three - dimensional image of the target face and three chan nels of an RGB image of the two - dimensional image of the Optionally , the foregoing first neural network comprises a first convolutional neural network , and the first convolu tional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer and two such as â€œ exemplary â€ or â€œ for example â€ are used for indicat- 60 fully - connected layers . ing an example or an illustrative example or illustration . Any embodiment or design scheme described as â€œ exemplary " or " for example â€ in the embodiments of the present invention should not be interpreted as being more preferable or more Step 103 : classifying an expression of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the target face , and a first parameter by the advantageous than other embodiments or design schemes . 65 first neural network , the first parameter comprising at least Exactly , the words such as â€œ exemplary â€ or â€œ for example â€ are used for presenting relevant concepts in specific manners . one face expression category and first parameter data for recognizing the expression category of the target face . US 11,023,715 B2 Because most expressions are compound expressions and may belong to at least one face expression category , the foregoing first neural network comprises the foregoing first parameter , and the face expression categories included by image of the target face , as well as performing the same linear transformation , affine transformation and contrast stretching on the two - dimensional image of the target face ; or , an another example , performing mirroring , linear trans the first parameter comprise at least one of : fear , sadness , 5 formation and image pixel value normalization processing face described above belongs to the foregoing different 15 three - dimensional image of the target face and three chan and contempt , and first parameter data for recognizing the 10 first processing on the three - dimensional image of the target joy , anger , disgust , surprise , nature and contempt . Option ally , in one embodiment , the foregoing first parameter may include face expression categories of eight expression cat egories of fear , sadness , joy , anger , disgust , surprise , nature face expression categories of the foregoing eight expression categories . Specifically , the classification results output by the first neural network may be probabilities that the target expression categories respectively , and the sum of the prob abilities of belonging to the foregoing different expression categories respectively is 1. The first neural network can sequence the output classification results according to mag- of the neural network . Optionally , under the situation that the foregoing first parameter includes one face expression category , the first on the three - dimensional image of the target face , as well as performing mirroring , linear transformation and image pixel value normalization processing on the two - dimensional image of the target face . Optionally , performing the same face and the two - dimensional image of the target face , as described above , may be : respectively performing the same first processing on depth information ( e.g. , a depth image ) of the target face , three channels of an RGB image of the nels of an RGB image of the two - dimensional image of the target face ; or performing the same first processing on the overall image of the three - dimensional image of the target face and the overall image of the two - dimensional image of first depth information of the target face , first color infor mation of the target face and second color information of the target face and inputting them to the first neural network . Optionally , the foregoing feature points may be eye nitudes of the foregoing probabilities . The foregoing first 20 the target face , then decomposing the overall images into parameter data may comprise the weight of at least one node neural network can be configured to judge whether the 25 points , or other face features such as a nose tip point and the expressions of the target face described above belong to the face expression category included by the first parameter . Optionally , in order to cope with the circumstance that the acquired target face posture is not ideal or the light condition like . The foregoing set position aligned with the feature points of the three - dimensional image of the target face and the two - dimensional image of the target face may be one or more feature points of a standard face image , e.g. , eye is not ideal , the same first processing can be performed on 30 points , or a preset position , or feature points in face express the three - dimensional image of the target face and the two - dimensional image of the target face to approximately meet the requirement of a standard face or the using require- ment , specifically , for example , before the first depth infor- samples that are uniformly aligned when the face expression samples are inputted to the foregoing first neural network during training , e.g. , eye points . Optionally , performing contrast stretching on the three mation of the target face , the first color information of the 35 dimensional image of the target face and the two - dimen target face and the second color information of the target face are input to the first neural network , the method further comprises : performing the same first processing on the three - dimensional image of the target face and the two- sional image of the target face , as described above , may comprise performing section - by - section contrast stretching on the three - dimensional image of the target face and the two - dimensional image of the target face according to the dimensional image of the target face , the first processing 40 characteristics of the three - dimensional image of the target comprising at least one of : determining feature points of the three - dimensional image of the target face and the two- dimensional image of the target face , and rotating the three - dimensional image of the target face and the two- face and / or the two - dimensional image of the target face , or comprise performing section - by - section contrast stretching on pixel values of the three - dimensional image of the target face and the two - dimensional image of the target face dimensional image of the target face based on the feature 45 according to the magnitudes of the pixel values . points ; performing mirroring , linear transformation and affine transformation on the three - dimensional image of the target face and the two - dimensional image of the target face ; aligning the feature points of the three - dimensional image of Optionally , performing image pixel value normalization processing on the three - dimensional image of the target face and the two - dimensional image of the target face comprises : normalizing pixel values of channels of the three - dimen the target face and the two - dimensional image of the target 50 sional image of the target face and the two - dimensional face with a set position ; performing contrast stretching on the three - dimensional image of the target face and the two - dimensional image of the target face ; and performing image pixel value normalization processing on the three- image of the target face from [ 0 , 255 ] to [ 0 , foregoing channels may comprise depth information of the three - dimensional image of the target face , three channels of an RGB image of the three - dimensional image of the target ] . The dimensional image of the target face and the two - dimen- 55 face and three channels of an RGB image of the two sional image of the target face . Performing the same first processing on the three - dimen- sional image of the target face and the two - dimensional image of the target face , as described above , may comprise : dimensional image of the target face . Generally , using a human face as an example , the three dimensional image of the target face and the two - dimen sional image of the target face , which are acquired by the performing the first processing on the three - dimensional 60 photographic device , comprise redundant parts such as the image of the target face and performing the identical first processing on the two - dimensional image of the target face . Exemplarily , performing the same first processing on the three - dimensional image of the target face and the two- neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are positioned , and then the dimensional image of the target face , as described above , 65 foregoing first processing is performed . may be : performing linear transformation , affine transfor- mation and contrast stretching on the three - dimensional Optionally , the foregoing first parameter data for recog nizing the expression categories of the target face is obtained US 11,023,715 B2 by training three - dimensional images of multiple face expression samples and two - dimensional images of the face expression samples via the first neural network . The three- dimensional images of the face expression samples comprise feature points , performing mirroring , linear transformation and affine transformation on the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples ; aligning the feature second depth information of the face expression samples and 5 points of the three - dimensional images of the face expres third color information of the face expression samples , and the two - dimensional images of the face expression samples comprise fourth color information of the face expression samples . Specifically , the second depth information , the sion samples and the two - dimensional images of the face expression samples with a set position ; performing contrast stretching on the three - dimensional images of the face expression samples and the two - dimensional images of the third color information and the fourth color information of 10 face expression samples ; and performing image pixel value the foregoing multiple face expression samples can be input to the first neural network and iterated , the multiple face expression samples carry face expression categories repre- senting face expression categories , a parameter combination normalization processing on the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples . The foregoing sec ond processing may be same as or different from the first having high expression accuracy for recognizing the face 15 processing . expression samples is determined as the first parameter for recognizing the expression categories of the target face , and the specific content of the first parameter can be known by referring to the above description . Optionally , the first Performing the same second processing on the three dimensional images of the face expression samples and the two - dimensional images of the face expression samples may comprise : performing the second processing on the three parameter can be obtained by training the foregoing face 20 dimensional images of the face expression samples and expression samples off line , and the product for expression recognition , provided for practical use , may not comprise the foregoing face expression samples . Because most expressions are compound expressions and performing the identical second processing on the two dimensional images of the face expression samples . Exem plarily , performing the same second processing on the three - dimensional images of the face expression samples may belong to at least one expression category , each of the 25 and the two - dimensional images of the face expression foregoing face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Each of the face expression samples , the second depth samples may be : performing linear transformation , affine transformation and contrast stretching on the three - dimen sional images of the face expression samples , as well as performing the foregoing linear transformation , affine trans information of the face expression sample , the third color 30 formation and contrast stretching on the two - dimensional information of the face expression sample and the fourth color information of the face expression sample satisfy ( belong to ) the same face expression category . The third color information and the fourth color information are images of the face expression samples ; or , as another example , performing mirroring , linear transformation and image pixel value normalization processing on the three dimensional images of the face expression samples , as well images of an RGB format or a YUV format . Through the 35 as performing mirroring , linear transformation and image face expression categories carried by the foregoing face expression samples , the face expression categories of com- ponents ( the second depth information of the face expression samples and the third color information of the face expres- pixel value normalization processing on the two - dimen sional images of the face expression samples . Exemplarily , performing the same second processing on the three - dimen sional images of the face expression samples and the two sion samples are components of the three - dimensional 40 dimensional images of the face expression samples , as images , and the fourth color information of the face expres- sion samples is components of the two - dimensional images ) of the foregoing face expression samples input to the first neural network can be determined , and the first neural described above , may be : respectively performing the same second processing on second depth information ( e.g. , depth images ) of the face expression samples , three channels of RGB images of the three - dimensional images of the face network can train them to obtain first parameter data corre- 45 expression samples and three channels of RGB images of ries . sponding to the foregoing different face expression catego- Optionally , in order to cope with the circumstance that the acquired face expression sample postures are not ideal or the the two - dimensional images of the face expression samples ; or performing the same second processing on the overall images of the three - dimensional images of the face expres sion samples and the overall images of the two - dimensional light condition is not ideal , the same second processing can 50 images of the face expression samples , then decomposing be performed on the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples to approximately meet the require- ment of a standard face or the using requirement , specifi- the overall images into second depth information , third color information and fourth color information and inputting them to the first neural network . Optionally , the foregoing feature points may be eye cally , for example , before the three - dimensional images of 55 points , or other face features such as a nose tip point and the the multiple face expression samples and the two - dimen- sional images of the face expression samples are trained via the first neural network , the method further comprises : performing the same second processing on the three - dimen- like . The foregoing set position aligned with the feature points of the three - dimensional images of the multiple face expression samples and the two - dimensional images of the multiple face expression samples may be one or more sional images of the face expression samples and the two- 60 feature points of a standard face image , e.g. , eye points , or dimensional images of the face expression samples , the second processing comprising at least one of : determining feature points of the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples , and rotating the three - dimensional 65 images of the face expression samples and the two - dimen- sional images of the face expression samples based on the a preset position , or feature points in the face expression samples that are uniformly aligned when the face expression samples are inputted to the foregoing first neural network during training , e.g. , eye points . Optionally , performing contrast stretching on the three dimensional images of the face expression samples and the two - dimensional images of the face expression samples , as US 11,023,715 B2 described above , may comprise performing section - by - sec- tion contrast stretching on the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples according to the character- layers . network comprises four convolutional layers , four down sampling layers , one dropout layer and two fully - connected Step 203 : classifying an expression of the target face istics of the three - dimensional images of the face expression 5 according to the third depth information of the target face samples and / or the two - dimensional images of the face expression samples , or comprise performing section - by- section contrast stretching on pixel values of the three- dimensional images of the face expression samples and the and a second parameter and outputting first classification data by the second neural network , and classifying the expression of the target face according to the fifth color information of the target face and a third parameter and two - dimensional images of the face expression samples 10 outputting second classification data by the third neural according to the magnitudes of the pixel values . Optionally , performing image pixel value normalization processing on the three - dimensional images of the face expression samples and the two - dimensional images of the network , the second parameter including at least one face expression category and second parameter data for recog nizing the expression categories of the target face , and the third parameter including the at least one face expression face expression samples comprises : normalizing pixel val- 15 category and third parameter data for recognizing the ues of channels of the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] . The fore- going channels may comprise first depth information of the expression categories of the target face . Because most expressions are compound expressions and may belong to at least one face expression category , the foregoing second neural network comprises the foregoing three - dimensional images of the face expression samples , 20 first classification data , and the face expression categories three channels of RGB images of the three - dimensional images of the face expression samples and three channels of RGB images of the two - dimensional images of the face expression samples . included by the first classification data comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . The foregoing third neural network comprises the foregoing second classification data , and the face expression Generally , using a human face as an example , the three- 25 categories included by the second classification data com dimensional images of the face expression samples and the two - dimensional images of the face expression samples , which are acquired by the photographic device , comprise redundant parts such as the neck , shoulders and the like in prise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Optionally , the face expres sion categories included by the first classification data and the second classification data are same . Exemplarily , both addition to the face , so it needs to be positioned to the face 30 the foregoing first classification data and the foregoing frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are positioned , and then the foregoing second processing is performed . second classification data include eight face expression categories of fear , sadness , joy , anger , disgust , surprise , nature and contempt and eight groups of parameter data corresponding to the foregoing eight face expression cat The method and device for expression recognition , pro- 35 egories , and the eight groups of parameter data may include vided by the present invention , can effectively solve the problem that the face expression recognition accuracy declines due to different face postures and different light conditions , and improve the accuracy of face expression probabilities of belonging to the foregoing eight face expres sion categories respectively . The foregoing second param eter data and third parameter data include second parameter data for recognizing whether the target face belongs to the recognition of the target face at different face postures and 40 foregoing eight face expression categories , e.g. , the weight in different light conditions . A method for expression recognition provided by embodi- ment 2 of the present invention will be specifically elabo- rated below in combination with FIG . 2. As shown in FIG . , the method comprises : Step 201 : acquiring a three - dimensional image of a target face , the three - dimensional image including third depth information of the target face and fifth color information of the target face . of at least one node of the neural network . The second neural network comprises a second convolu tional neural network , and the third neural network com prises a third convolutional neural network . Step 204 : outputting classification results on the expres sion of the target face according to the first classification data and the second classification data . Optionally , outputting classification results on the expres sions of the target face according to the first classification Optionally , this acquisition step may be acquiring a three- 50 data and the second classification data comprises : inputting dimensional image of a target face , which is photographed by a photographic device , from a memory . Optionally , the three - dimensional image of the foregoing target face may be a color image . the first classification data and the second classification data and outputting classification results on the expressions of the target face according to the first classification data , the second classification data and support vector machine Optionally , the fifth color information may be an image of 55 parameter data by a support vector machine , the support an RGB format or a YUV format , or an image of another format that can be converted to and from the foregoing RGB format or YUV format . Step 202 : inputting the third depth information of the vector machine comprising the at least one face expression category and the support vector machine parameter data for recognizing the expression category of the target face . Exemplarily , the first classification data may be a group of target face to a second neural network and inputting the fifth 60 eight - dimensional data , i.e. , data for indicating eight expres color information of the target face to a third neural network . Optionally , input to the third neural network may be an RGB image of the target face , or three channels of the RGB image of the target face . sion categories . The eight expression categories may be fear , sadness , joy , anger , disgust , surprise , nature and contempt . Optionally , the foregoing data for indicating eight expres sion categories may be eight probability values that the Optionally , the second neural network comprises three 65 expressions of the target face respectively belong to the convolutional layers , three down - sampling layers , one drop- out layer and two fully - connected layers . The third neural foregoing eight expression categories , and the sum of the eight probability values is 1. Similarly , the second classifi US 11,023,715 B2 cation data is also of eight expression categories , the input of the support vector machine is two groups of eight- dimensional data , and the support vector machine judges which expression categories the expressions of the target face described above belong to according to the foregoing 5 two groups of eight - dimensional data and the support vector machine parameter data for recognizing the expression category of the target face . The foregoing support vector machine may be a linear support vector machine . The of the target face . color information of the target face ; and performing image pixel value normalization processing on the third depth information of the target face and the fifth color information Performing the same third processing on the third depth information of the target face and the fifth color information of the target face , as described above , may comprise : per forming the third processing on the third depth information of the target face and performing the identical third process classification results output by the support vector machine 10 ing on the fifth color information of the target face . Exem may be probabilities that the target face described above belongs to the foregoing different expression categories respectively , and the sum of the probabilities of belonging to the foregoing different expression categories respectively is plarily , linear transformation , affine transformation and con trast stretching may be performed on the third depth information of the target face , and the same linear transfor mation , affine transformation and contrast stretching are also 1. The support vector machine can sequence the output 15 performed on the fifth color information of the target face . classification results according to the magnitudes of the probabilities . Optionally , under the condition that the foregoing first classification data and second classification data includes For another example , mirroring , linear transformation and image pixel value normalization processing are performed on the third depth information of the target face , and the same mirroring , linear transformation and image pixel value one face expression category , the support vector machine 20 normalization processing are also performed on the fifth also includes the one face expression category , and the support vector machine can be configured to judge whether the expressions of the target face described above belong to the face expression category included by the support vector Optionally , in order to cope with the circumstance that the acquired target face posture is not ideal or the light condition is not ideal , third processing may be performed only on the third depth information of the target face , or third processing machine . color information of the target face . Optionally , performing the same third processing on the third depth information of the target face and the fifth color information of the target face , as described above , may be performing the same third processing on the third depth information ( e.g. , a depth image ) of the target face and an RGB image of the three dimensional image of the target face , or performing the same third processing on the third depth information of the target face and three channels of the RGB image of the three is performed on the third depth information of the target face 30 dimensional image of the target face . and the same third processing is performed on the fifth color information of the target face . Thus , before inputting the third depth information of the target face to a second neural network and inputting the fifth color information of the Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The set position aligned with the feature points of the third depth information of the target face and the fifth color target face to a third neural network , the method further 35 information of the target face may be one or more feature comprises : performing third processing on the third depth informa- tion of the target face , the third processing comprising at least one of : determining feature points of the third depth points of a standard face image , e.g. , eye points , or a preset position , or feature points in face expression samples that are uniformly aligned when the face expression samples are inputted to the foregoing second neural network during information of the target face , and rotating the third depth 40 training and feature points in face expression samples that information of the target face based on the feature points ; performing mirroring , linear transformation and affine trans- formation on the third depth information of the target face ; aligning the feature points of the third depth information of are uniformly aligned when the face expression samples are inputted to the foregoing third neural network during train ing , e.g. , eye points . Optionally , the foregoing set position aligned with the feature points of the third depth information the target face with a set position ; performing contrast 45 of the target face may be one or more feature points of a stretching on the third depth information of the target face ; and performing image pixel value normalization processing on the third depth information of the target face ; or , before inputting the third depth information of the target 50 face to a second neural network and inputting the fifth color information of the target face to a third neural network , the method further comprises : performing the same third pro- cessing on the third depth information of the target face and standard face image , e.g. , eye points , or a preset position , or feature points in face expression samples that are uniformly aligned when the face expression samples are inputted to the foregoing second neural network during training . Optionally , performing contrast stretching on the third depth information of the target face and the fifth color information of the target face , as described above , may comprise performing section - by - section contrast stretching on the third depth information of the target face and the fifth the fifth color information of the target face , the third 55 color information of the target face according to the char processing comprising at least one of : determining feature points of the third depth information of the target face and feature points of the fifth color information of the target face , and rotating the third depth information of the target face acteristics of the three - dimensional image of the target face , or comprise section - by - section contrast stretching on pixel values of the third depth information of the target face and the fifth color information of the target face according to the and the fifth color information of the target face based on the 60 magnitudes of the pixel values . feature points ; performing mirroring , linear transformation and affine transformation on the third depth information of the target face and the fifth color information of the target face ; aligning the feature points of the third depth informa- Optionally , performing image pixel value normalization processing on the third depth information of the target face and the fifth color information of the target face comprises : normalizing pixel values of channels of the third depth tion of the target face and the fifth color information of the 65 information of the target face and the fifth color information target face with a set position ; performing contrast stretching on the third depth information of the target face or the fifth of the target face from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels may comprise third depth information of the target US 11,023,715 B2 values of the third depth information of the target face from 5 acquired face expression sample postures are not ideal or the face and three channels of an RGB image of the three- dimensional image of the target face . Performing image pixel value normalization processing on the third depth information of the target face comprises : normalizing pixel [ 0 , 255 ] to [ 0 , 1 ] . Generally , using a human face as an example , the three dimensional image of the target face , which is acquired by the photographic device , comprises redundant parts such as categories . work can train them to obtain third parameter data corre sponding to the foregoing different face expression Optionally , in order to cope with the circumstance that the light condition is not ideal , fourth processing may be per formed on the fourth depth information of the face expres sion samples , or the same fourth processing is performed on the fourth depth information of the face expression samples the neck , shoulders and the like in addition to the face , so it 10 and the sixth color information of the face expression needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are positioned , and then the foregoing third processing is performed . Optionally , the second parameter data is obtained by training fourth depth information of multiple face expression samples via the second neural network , and the third param- eter data is obtained by training sixth color information of samples , to approximately meet the requirement of a stan dard face or the using requirement , specifically , for example , before the fourth depth information of the multiple face expression samples is trained via the second neural network , the method further comprises : performing fourth processing on the fourth depth infor mation of the face expression samples , the fourth processing comprising at least one of : determining feature points of the fourth depth information of the face expression samples , and the multiple face expression samples via the third neural 20 rotating the fourth depth information of the face expression network . Three - dimensional images of the face expression samples comprise fourth depth information of the face expression samples and sixth color information of the face expression samples . It may be parallel that the second neural samples based on the feature points ; performing mirroring , linear transformation and affine transformation on the fourth depth information of the face expression samples ; aligning the feature points of the fourth depth information of the face network trains the fourth depth information to obtain the 25 expression samples with a set position ; performing contrast second parameter data and the third neural network trains the sixth color information to obtain the third parameter data . Specifically , the fourth depth information and the sixth color information of the foregoing multiple face expression samples can be input to the foregoing second neural network 30 and third neural network and iterated , the multiple face expression samples carry face expression categories repre- senting face expression categories , a parameter combination having high expression accuracy for recognizing the face stretching on the fourth depth information of the face expression samples ; and performing image pixel value nor malization processing on the fourth depth information of the face expression samples ; or , before the fourth depth information of the face expres sion samples is trained via the second neural network and the sixth color information of the face expression samples is trained via the third neural net rk , the method further comprises : performing the same fourth processing on the expression samples , e.g. , the weight of at least one node of 35 fourth depth information of the face expression samples and the neural network , is determined as the second parameter data and the third parameter data for recognizing the expres- sion categories of the target face , and the specific content of the second parameter data and the third parameter data can the sixth color information of the face expression samples , the fourth processing comprising at least one of : determining feature points of the fourth depth information of the face expression samples and feature points of the sixth color be known by referring to the above description . Optionally , 40 information of the face expression samples , and rotating the the second parameter data and the third parameter data can be obtained by training the foregoing face expression samples off line , and the product for expression recognition , provided for practical use , may not comprise the foregoing face expression samples . Because most expressions are compound expressions and may belong to at least one expression category , the face expression categories included by the second neural network and the face expression categories included by the third fourth depth information of the face expression samples and the sixth color information of the face expression samples based on the feature points ; performing mirroring , linear transformation and affine transformation on the fourth depth information of the face expression samples and the sixth color information of the face expression samples ; aligning the feature points of the fourth depth information of the face expression samples and the sixth color information of the face expression samples with a set position ; performing neural network include at least one of : fear , sadness , joy , 50 contrast stretching on the fourth depth information of the anger , disgust , surprise , nature and contempt . Each of the face expression samples , the fourth depth information of the face expression sample and the sixth color information of the face expression sample satisfy ( belong to ) the same face face expression samples and the sixth color information of the face expression samples ; and performing image pixel value normalization processing on the fourth depth infor mation of the face expression samples and the sixth color expression category . The foregoing sixth color information 55 information of the face expression samples . The foregoing is images of an RGB format or a YUV format . Through the face expression categories carried by the foregoing face expression samples , the face expression categories of com- ponents ( the fourth depth information of the three - dimen- processing fourth processing may be same as or different from the third Performing the same fourth processing on the fourth depth information of the face expression samples and the sional images of the face expression samples and the sixth 60 sixth color information of the face expression samples may color information of the three - dimensional images of the face expression samples ) of the three - dimensional images of the foregoing face expression samples input to the second neural network and the third neural network can be deter- comprise : performing the fourth processing on the fourth depth information of the face expression samples and per forming the identical fourth processing on the sixth color information of the face expression samples . Exemplarily , mined , the second neural network can train them to obtain 65 performing the same fourth processing on the fourth depth second parameter data corresponding to the foregoing dif- ferent face expression categories , and the third neural net- information of the face expression samples and the sixth color information of the face expression samples may be : US 11,023,715 B2 performing linear transformation , affine transformation and contrast stretching on the fourth depth information of the face expression samples , as well as performing linear trans- formation , affine transformation and contrast stretching on and three channels of RGB images of the sixth color information of the face expression samples . Generally , using a human face as an example , the three dimensional images of the face expression samples , which the sixth color information of the face expression samples ; 5 are acquired by the photographic device , comprise redun mation and image pixel value normalization processing on 10 tioned , and then the foregoing fourth processing is per or , as another example , performing mirroring , linear trans formation and image pixel value normalization processing on the fourth depth information of the face expression samples , as well as performing mirroring , linear transfor the sixth color information of the face expression samples . Exemplarily , performing the same fourth processing on the fourth depth information of the face expression samples and the sixth color information of the face expression samples , as described above , may be : respectively performing the 15 same fourth processing on the fourth depth information ( e.g. , depth images ) of the face expression samples and three channels of RGB images of the three - dimensional images of the face expression samples ; or performing the fourth pro dant parts such as the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are posi formed . The fifth color information is an image of an RGB format or a YUV format . The sixth color information is images of an RGB format or a YUV format . The support vector machine parameter data for recogniz ing the expression category of the target face is obtained by training the second neural network with the fourth depth information of the facial expression samples , training the cessing on the overall images of the three - dimensional 20 third neural network with the sixth color information of the images of the face expression samples , then decomposing the overall images into the fourth depth information of the face expression samples and the sixth color information of the face expression samples and inputting them to the second neural network and the third neural network . Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The set position aligned with the feature points of the fourth depth information of the face expression samples and facial expression samples , combining corresponding output data from the second fully - connected layer of the second neural network and the second fully - connected layer of the third neural network as inputs , and training the support vector machine with the inputs and corresponding expres sion labels of the facial expression samples . Exemplarily , the output data when the second neural network trains the fourth depth information of the multiple face expression samples may be a group of eight - dimensional data , i.e. , data for the sixth color information of the face expression samples , 30 indicating eight expression categories , and the eight expres or the set position aligned with the feature points of the fourth depth information of the face expression samples , as described above , may be one or more feature points of a standard face image , e.g. , eye points , or a preset position , or sion categories may be fear , sadness , joy , anger , disgust , surprise , nature and contempt . Similarly , the output data when the third neural network trains the sixth color infor mation of the multiple face expression samples is also of feature points in the face expression samples that are uni- 35 eight expression categories , the input of the support vector formly aligned when the face expression samples are input- ted to the foregoing second neural network and third neural network during training , e.g. , eye points . Optionally , performing contrast stretching on the fourth machine is two groups of eight - dimensional data described above , and because the two groups of eight - dimensional data described above carry face expression categories rep resenting expression categories , the support vector machine depth information of the face expression samples , or per- 40 data carrying the face expression categories of the expres forming contrast stretching on the fourth depth information of the face expression samples and the sixth color informa- tion of the face expression samples , as described above , may comprise : performing section - by - section contrast stretching sion categories can be trained via the two groups of eight dimensional data described above . The two groups of eight dimensional data described above may be probabilities that the face expression samples respectively belong to different on the fourth depth information of the face expression 45 face expression categories . samples and the sixth color information of the face expres sion samples according to the characteristics of the fourth depth information of the face expression samples and / or the sixth color information of the face expression samples , or The method and device for expression recognition , pro vided by the present invention , can effectively solve the problem that the face expression recognition accuracy declines due to different face postures and different light performing section - by - section contrast stretching on pixel 50 conditions , and improve the accuracy of face expression values of the fourth depth information of the face expression samples and the sixth color information of the face expres- sion samples according to the magnitudes of the pixel values . recognition of the target face at different face postures and in different light conditions . A method for expression recognition provided by embodi ment 3 of the present invention will be specifically elabo Optionally , performing image pixel value normalization 55 rated below in combination with FIG . 3. As shown in FIG . processing on the fourth depth information of the face expression samples comprises : normalizing pixel values of the fourth depth information of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] ; or , performing image pixel value , the method comprises : Step 301 : acquiring a three - dimensional image of a target face , the three - dimensional image including fifth depth information of the target face and seventh color information normalization processing on the fourth depth information of 60 of the target face . the face expression samples and the sixth color information of the face expression samples comprises : normalizing pixel values of channels of the fourth depth information of the face expression samples and the sixth color information of Optionally , this acquisition step may be acquiring a three dimensional image of a target face , which is photographed by a photographic device , from a memory . Optionally , the three - dimensional image of the target face the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] . The 65 described above may be a color image . foregoing channels may comprise fourth depth information of three - dimensional images of the face expression samples , Optionally , the seventh color information may be an image of an RGB format or a YUV format , or an image of US 11,023,715 B2 another format that can be converted to and from the foregoing RGB format or YUV format . Step 302 : inputting the fifth depth information of the target face and the seventh color information of the target sional image of the target face ; and performing image pixel value normalization processing on the three - dimensional image of the target face . Performing the fifth processing on the three - dimensional face to a fourth neural network . Optionally , input to the 5 image of the target face , as described above , may be face . fourth neural network may be a depth image of the target face and an RGB image of the three - dimensional image of the target face ; input to the fourth neural network may also be a depth image of the target face and three channels of an RGB image of the three - dimensional image of the target Optionally , the fourth neural network comprises a fourth convolutional neural network . The fourth convolutional neu- ral network comprises one segmentation layer , eight convo- lutional layers , eight down - sampling layers , two dropout layers and five fully - connected layers . Step 303 : classifying an expression of the target face according to the fifth depth information of the target face , the parameter by the fourth neural network , the fourth parameter including at least one face expression category and fourth parameter data for recognizing the expression categories of the target face . performing the same fifth processing on the fifth depth information of the target face and the seventh color infor mation of the target face , i.e. , performing the fifth processing on the fifth depth information of the target face and per forming the identical fifth processing on the seventh color information of the target face . Exemplarily , performing the same fifth processing on the fifth depth information of the target face and the seventh color information of the target face may be : performing linear transformation , affine trans formation and contrast stretching on the fifth depth infor mation of the target face , as well as performing linear transformation , affine transformation and contrast stretching on the seventh color information of the target face ; or , as tion and image pixel value normalization processing on the fifth depth information of the target face , as well as per forming mirroring , linear transformation and image pixel value normalization processing on the seventh color infor seventh color information of the target face , and a fourth 20 another example , performing mirroring , linear transforma Optionally , because most expressions are compound 25 mation of the target face . Optionally , performing the fifth expressions and may belong to at least one expression category , the fourth neural network may include the fourth parameter , and the face expression categories included by the fourth parameter include at least one of : fear , sadness , processing on the three - dimensional image of the target face , as described above , may be : respectively performing the same fifth processing on the fifth depth information ( e.g. , a depth image ) of the target face and three channels of an RGB joy , anger , disgust , surprise , nature and contempt . Exem- 30 image of the seventh color information of the target face ; or plarily , the foregoing fourth parameter may include the face expression categories of eight expression categories of fear , sadness , joy , anger , disgust , surprise , nature and contempt , and fourth parameter data for recognizing the foregoing performing the fifth processing on the overall image of the three - dimensional image of the target face , then decompos ing the overall image into the fifth depth information and the seventh color information and inputting them to the fourth eight face expression categories , e.g. , the weight of at least 35 neural network . one node of the fourth neural network . Specifically , the classification results output by the fourth neural network may be probabilities that the target face described above belongs to the foregoing different expression categories Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The set position aligned with the feature points of the three - dimensional image of the target face may be one or respectively , and the sum of the probabilities of belonging to 40 more feature points of a standard face image , e.g. , eye the foregoing different expression categories respectively is 1. The fourth neural network can sequence the output classification results according to the magnitudes of the foregoing probabilities . Optionally , under the condition that the foregoing fourth 45 parameter includes one face expression category , the fourth neural network can be configured to judge whether the expressions of the target face described above belong to the face expression category included by the fourth parameter . points , or a preset position , or feature points in face expres sion samples that are uniformly aligned when the face expression samples are inputted to the foregoing fourth neural network during training , e.g. , eye points . Performing contrast stretching on the three - dimensional image of the target face , as described above , may comprise performing section - by - section contrast stretching on the three - dimensional image of the target face according to the characteristics of the three - dimensional image of the target Optionally , in order to cope with the circumstance that the 50 face , or comprise performing section - by - section contrast acquired target face posture is not ideal or the light condition is not ideal , fifth processing may be performed on the three - dimensional image of the target face to approximately meet the requirement of a standard face or the using require- stretching on pixel values of the three - dimensional image of the target face according to the magnitudes of the pixel Optionally , performing image pixel value normalization values . ment , specifically , for example , before inputting the fifth 55 processing on the three - dimensional image of the target face depth information of the target face and the seventh color information of the target face to a fourth neural network , the method further comprises : performing fifth processing on the three - dimensional image of the target face , the fifth comprises : normalizing pixel values of channels of the three - dimensional image of the target face from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels may comprise depth infor mation of the three - dimensional image of the target face and processing comprising at least one of : determining feature 60 three channels of an RGB image of the three - dimensional points of the three - dimensional image of the target face , and rotating the three - dimensional image of the target face based on the feature points ; performing mirroring , linear transfor- mation and affine transformation on the three - dimensional image of the target face . Generally , using a human face as an example , the three dimensional image of the target face , which is acquired by the photographic device , comprises redundant parts such as image of the target face ; aligning the feature points of the 65 the neck , shoulders and the like in addition to the face , so it three - dimensional image of the target face with a set posi- tion ; performing contrast stretching on the three - dimen- needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned US 11,023,715 B2 face features , e.g. , eye points , are positioned , and then the foregoing fifth processing is performed . Optionally , the fourth parameter data is obtained by training three - dimensional images of multiple face expres samples . The foregoing sixth processing may be same as or different from the fifth processing . Optionally , performing the sixth processing on the three dimensional images of the face expression samples may sion samples via the fourth neural network . The three- 5 comprise : performing the same sixth processing on the sixth information of the foregoing multiple face expression 10 the eighth color information of the face expression samples . dimensional images of the face expression samples comprise sixth depth information of the face expression samples and eighth color information of the face expression samples . Specifically , the sixth depth information and the eighth color samples can be input to the fourth neural network and iterated , the multiple face expression samples carry face expression categories representing face expression catego racy for recognizing the face expression samples , e.g. , the weight of at least one node of the neural network , is determined as the fourth parameter for recognizing the expression categories of the target face , and the specific depth information and the eighth color information of the face expression samples , i.e. , performing the sixth process ing on the sixth depth information of the face expression samples , and performing the identical sixth processing on Exemplarily , linear transformation , affine transformation and contrast stretching may be performed on the sixth depth information of the face expression samples , and the forego ing linear transformation , affine transformation and contrast tion of the face expression samples ; or , as another example , mirroring , linear transformation and image pixel value nor malization processing are performed on the sixth depth information of the face expression samples , and mirroring , ries , a parameter combination having high expression accu 15 stretching are also performed on the eighth color informa content of the fourth parameter can be known by referring to 20 linear transformation and image pixel value normalization the above description . Optionally , the fourth parameter can be obtained by training the foregoing face expression samples off line , and the product for expression recognition , provided for practical use , may not comprise the foregoing face expression samples . Because most expressions are compound expressions and may belong to at least one expression category , each of the face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , sadness , joy , processing are also performed on the eighth color informa tion of the face expression samples . Exemplarily , perform ing the same sixth processing on the sixth depth information of the face expression samples and the eighth color infor mation of the face expression samples , as described above , may be : respectively performing the same sixth processing on the sixth depth information ( e.g. , depth images ) of the face expression samples , and three channels of the eighth color information , e.g. , RGB images , of the three - dimen anger , disgust , surprise , nature and contempt . Each of the 30 sional images of the face expression samples ; or performing face expression samples , the sixth depth information of the face expression sample and the eighth color information of the face expression sample satisfy ( belong to ) the same face expression category . The eighth color information is images the same sixth processing on the overall images of the three - dimensional images of the face expression samples , then decomposing the overall images into the sixth depth information and the eighth color information and inputting of an RGB format or a YUV format . Through the face 35 them to the fourth neural network . expression categories carried by the foregoing face expres- sion samples , the face expression categories of components ( the sixth depth information of the face expression samples and the eighth color information of the face expression Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The foregoing set position aligned with the feature points of the three - dimensional images of the multiple face samples are components of the three - dimensional image ) of 40 expression samples may be one or more feature points of a the foregoing face expression samples input to the fourth neural network can be determined , and the fourth neural network can train them to obtain the fourth parameter corresponding to the foregoing different face expression standard face image , e.g. , eye points , or a preset position , or feature points in the face expression samples that are uni formly aligned when the face expression samples are input ted to the foregoing fourth neural network during training , categories . e.g. , eye points . Optionally , in order to cope with the circumstance that the acquired face expression sample postures are not ideal or the light condition is not ideal , six processing can be performed on the three - dimensional images of the face expression Optionally , performing contrast stretching on the three dimensional images of the face expression samples , as described above , may comprise performing section - by - sec tion contrast stretching on the three - dimensional images of samples to approximately meet the requirement of a stan- 50 the face expression samples according to the characteristics dard face or the using requirement , specifically , for example , before the three - dimensional images of the multiple face expression samples are trained via the fourth neural network , sixth processing is performed on the three - dimensional of the three - dimensional images of the face expression samples , or comprise performing section - by - section contrast stretching on pixel values of the three - dimensional images of the face expression samples according to the magnitudes images of the face expression samples , and the sixth pro- 55 of the pixel values . cessing comprises at least one of : determining feature points of the three - dimensional images of the face expression samples , and rotating the three - dimensional images of the face expression samples based on the feature points ; per- Optionally , performing image pixel value normalization processing on the three - dimensional images of the face expression samples comprises : normalizing pixel values of channels of the three - dimensional images of the face expres forming mirroring , linear transformation and affine transfor- 60 sion samples from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels mation on the three - dimensional images of the face expres- sion samples ; aligning the feature points of the three- dimensional images of the face expression samples with a set position ; performing contrast stretching on the three- may comprise the sixth depth information of the three dimensional images of the face expression samples , and three channels of the eight color information , e.g. , RGB images , of the three - dimensional images of the face expres dimensional images of the face expression samples ; and 65 sion samples . performing image pixel value normalization processing on the three - dimensional images of the face expression Generally , using a human face as an example , the three dimensional images of the face expression samples , which US 11,023,715 B2 are acquired by the photographic device , comprise redun- dant parts such as the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face detection , then the face is extracted , the foregoing eight face expression categories , e.g. , the weight of at least one node of the first neural network . Specifically , the classification results output by the first neural network may be probabilities that the target face described above above - mentioned face features , e.g. , eye points , are posi- 5 belongs to the foregoing different expression categories tioned , and then the foregoing sixth processing is performed . The method and device for expression recognition , pro- vided by the present invention , can effectively solve the problem that the face expression recognition accuracy respectively , and the sum of the probabilities of belonging to the foregoing different expression categories respectively is 1. The first neural network 403 can sequence the output classification results according to the magnitudes of the declines due to different face postures and different light 10 foregoing probabilities . Optionally , under the situation that conditions , and improve the accuracy of face expression recognition of the target face at different face postures and in different light conditions . A device for expression recognition provided by embodi- ment 4 of the present invention will be specifically elabo- 15 parameter . rated below in combination with FIG . 4. The device 400 may comprise the following modules : A first acquisition module 401 is configured to acquire a three - dimensional image of a target face and a two - dimen- comprising first depth information of the target face and first color information of the target face , and the two - dimensional image comprising second color information of the target face . the foregoing first parameter includes one face expression category , the first neural network can be configured to judge whether the expressions of the target face described above belong to the face expression category included by the first Optionally , in order to cope with the circumstance that the acquired target face posture is not ideal or the light condition is not ideal , the same first processing can be performed on the three - dimensional image of the target face and the meet the requirement of a standard face or the using require ment , specifically , the device further comprises a first pro cessing module , and the first processing module is config ured to perform the same first processing on the three sional image of the target face , the three - dimensional image 20 two - dimensional image of the target face to approximately Optionally , the acquisition module 401 may acquire a 25 dimensional image of the target face and the two three - dimensional image of a target face and a two - dimen- sional image of the target face , which are photographed by a photographic device , from a memory . Optionally , the foregoing first color information and the dimensional image of the target face , and input the three dimensional image of the target face and the two dimensional image of the target face subjected to the first processing to the first input module . The first processing second color information may be images of an RGB format 30 module comprises at least one of the following sub - modules : or a YUV format , or images of other formats that can be a first rotating sub - module , a first transformation sub - mod converted to and from the foregoing RGB format or YUV ule , a first alignment sub - module , a first contrast stretching format . A first input module 402 is configured to input the first sub - module and a first normalization processing sub - mod ule . The first rotating sub - module is configured to determine depth information of the target face , the first color informa- 35 feature points of the three - dimensional image of the target tion of the target face and the second color information of the target face to a first neural network . Optionally , input to the first neural network may be a depth image of the target face , an RGB image of the three - dimensional image of the target face and the two - dimensional image of the target face , and rotate the three - dimensional image of the target face and the two - dimensional image of the target face based on the feature points . The first transformation sub - module is con face and an RGB image of the two - dimensional image of the 40 figured to perform mirroring , linear transformation and target face ; and input to the first neural network may also be a depth image of the target face , three channels of an RGB image of the three - dimensional image of the target face and three channels of an RGB image of the two - dimensional image of the target face . Optionally , the foregoing first neural network comprises a first convolutional neural network , and the first convolu- tional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer and two fully - connected layers . The first neural network 403 is configured to classify expressions of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the target affine transformation on the three - dimensional image of the target face and the two - dimensional image of the target face . The first alignment sub - module is configured to align the feature points of the three - dimensional image of the target face and the two - dimensional image of the target face with a set position . The first contrast stretching sub - module is configured to perform contrast stretching on the three dimensional image of the target face and the two - dimen sional image of the target face . The first normalization processing sub - module is configured to perform image pixel value normalization processing on the three - dimensional image of the target face and the two - dimensional image of the target face . Performing the same first processing on the three - dimen face and a first parameter , the first parameter comprising at 55 sional image of the target face and the two - dimensional least one face expression category and first parameter data for recognizing the expression categories of the target face . Because most expressions are compound expressions and may belong to at least one face expression category , the image of the target face , as described above , may comprise : performing the first processing on the three - dimensional image of the target face and performing the identical first processing on the two - dimensional image of the target face . foregoing first neural network comprises the foregoing first 60 Exemplarily , performing the same first processing of the first parameter , and the face expression categories included by the first parameter comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Option- ally , in one embodiment , the foregoing first parameter may processing module on the three - dimensional image of the target face and the two - dimensional image of the target face , as described above , may be : performing linear transforma tion and affine transformation of the first transformation include face expression categories of eight expression cat- 65 sub - module on the three - dimensional image of the target egories of fear , sadness , joy , anger , disgust , surprise , nature and contempt , and first parameter data for recognizing the face and contrast stretching of the first contrast stretching sub - module on the three - dimensional image of the target US 11,023,715 B2 face , as well as performing the same linear transformation and affine transformation of the first transformation sub- module on the two - dimensional image of the target face and contrast stretching of the first contrast stretching sub - module Optionally , the foregoing first parameter data for recog nizing the expression categories of the target face is obtained by training three - dimensional images of multiple face expression samples and two - dimensional images of the face on the two - dimensional image of the target face ; or , as 5 expression samples via the first neural network . The three another example , performing mirroring and linear transfor mation by the first transformation sub - module and perform ing image pixel value normalization processing by the first normalization processing sub - module on the three - dimen sional image of the target face , as well as performing mirroring and linear transformation by the first transforma tion sub - module and performing image pixel value normal ization processing by the first normalization processing Optionally , the first processing module specifically can be configured to : respectively perform the same first processing on depth information ( e.g. , a depth image ) of the target face , three channels of an RGB image of the three - dimensional dimensional images of the face expression samples comprise second depth information of the face expression samples and third color information of the face expression samples , and the two - dimensional images of the face expression samples comprise fourth color information of the face expression samples . Specifically , the first input module 402 can input the second depth information , the third color information and the fourth color information of the multiple face expres sion samples to the first neural network 403 and iterate them , categories representing face expression categories , the first neural network 403 determines a parameter combination having high expression accuracy for recognizing the face expression samples , e.g. , the weight of at least one node sub - module on the two - dimensional image of the target face . 15 the multiple face expression samples carry face expression image of the target face and three channels of an RGB image 20 thereof , as the first parameter for recognizing the expression of the two - dimensional image of the target face ; or perform the same first processing on the overall image of the three- dimensional image of the target face and the overall image of the two - dimensional image of the target face , then decom- categories of the target face , and the specific content of the first parameter can be known by referring to the above description . Optionally , the first parameter can be obtained by training the foregoing face expression samples off line , pose the overall images into first depth information of the 25 and the product for expression recognition , provided for target face , first color information of the target face and second color information of the target face and input them to the first neural network . Optionally , the foregoing feature points may be eye sion samples . practical use , may not comprise the foregoing face expres Because most expressions are compound expressions and may belong to at least one expression category , each of the points , or other face features such as a nose tip point and the 30 foregoing face expression samples satisfies ( belongs to ) at like . The foregoing set position aligned with the feature points of the three - dimensional image of the target face and the two - dimensional image of the target face may be one or more feature points of a standard face image , e.g. , eye least one of the following face expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Each of the face expression samples , the second depth information of the face expression sample , the third color points , or a preset position , or feature points in face expres- 35 information of the face expression sample and the fourth sion samples that are uniformly aligned when the face expression samples are inputted to the foregoing first neural network during training , e.g. , eye points . Optionally , the foregoing first contrast stretching sub- color information of the face expression sample satisfy ( belong to ) the same face expression category . The third color information and the fourth color information are images of an RGB format or a YUV format . Through the module specifically can be configured to perform section- 40 face expression categories carried by the foregoing face by - section contrast stretching on the three - dimensional image of the target face and the two - dimensional image of the target face according to the characteristics of the three- dimensional image of the target face and / or the two - dimen- expression samples , the first neural network 403 can deter mine the face expression categories of components ( the second depth information of the face expression samples and the third color information of the face expression samples sional image of the target face , or perform section - by- 45 are components of the three - dimensional images , and the section contrast stretching on pixel values of the three- dimensional image of the target face and the two- dimensional image of the target face according to the magnitudes of the pixel values . fourth color information of the face expression samples is components of the two - dimensional images ) of the forego ing face expression samples input to the first neural network , and the first neural network 403 can train them to obtain first Optionally , the first normalization processing sub - module 50 parameter data corresponding to the foregoing different face specifically can be configured to normalize pixel values of channels of the three - dimensional image of the target face and the two - dimensional image of the target face from [ 0 , ] to [ 0 , 1 ] . The foregoing channels may comprise depth expression categories . Optionally , in order to cope with the circumstance that the acquired face expression sample postures are not ideal or the light condition is not ideal , the same second processing can information of the three - dimensional image of the target 55 be performed on the three - dimensional images of the face face , three channels of an RGB image of the three - dimen- sional image of the target face and three channels of an RGB image of the two - dimensional image of the target face . Generally , using a human face as an example , the three- expression samples and the two - dimensional images of the face expression samples to approximately meet the require ment of a standard face or the using requirement , specifi cally , the device further comprises a second processing dimensional image of the target face and the two - dimen- 60 module , and the second processing module is configured to sional image of the target face , which are acquired by the photographic device , comprise redundant parts such as the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face perform the same second processing on the three - dimen sional images of the face expression samples and the two dimensional images of the face expression samples , and input the three - dimensional images of the face expression detection , then the face is extracted , the above - mentioned 65 samples and the two - dimensional images of the face expres face features , e.g. , eye points , are positioned , and then the foregoing first processing is performed . sion samples subjected to the second processing to the first input module . The second processing module comprises a US 11,023,715 B2 second rotating sub - module , a second transformation sub- module , a second alignment sub - module , a second contrast stretching sub - module and a second normalization process- ing sub - module . The second rotating sub - module is config- ured to determine feature points of the three - dimensional 5 images of the face expression samples and the two - dimen- sional images of the face expression samples , and rotate the three - dimensional images of the face expression samples and the two - dimensional images of the face expression images of the face expression samples , then decompose of the overall images into second depth information , third color information and fourth color information and input them to the first neural network . Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The foregoing set position aligned with the feature points of the three - dimensional images of the face expres sion samples and the two - dimensional images of the face samples based on the feature points . The second transfor- 10 expression samples may be one or more feature points of a mation sub - module is configured to perform mirroring , linear transformation and affine transformation on the three- dimensional images of the face expression samples and the two - dimensional images of the face expression samples . The standard face image , e.g. , eye points , or a preset position , or feature points in the face expression samples that are uni formly aligned when the face expression samples are input ted to the foregoing first neural network during training , e.g. , second alignment sub - module is configured to align the 15 eye points . feature points of the three - dimensional images of the face expression samples and the two - dimensional images of the face expression samples with a set position . The second contrast stretching sub - module is configured to perform Optionally , the foregoing second contrast stretching sub module specifically can be configured to perform section by - section contrast stretching on the three - dimensional images of the face expression samples and the two - dimen contrast stretching on the three - dimensional images of the 20 sional images of the face expression samples according to face expression samples and the two - dimensional images of the face expression samples . The second normalization processing sub - module is configured to perform image pixel value normalization processing on the three - dimensional the characteristics of the three - dimensional images of the face expression samples and / or the two - dimensional images of the face expression samples , or perform section - by section contrast stretching on pixel values of the three images of the face expression samples and the two - dimen- 25 dimensional images of the face expression samples and the sional images of the face expression samples . The foregoing second processing module may be same as or different from the first processing module . The second processing module specifically can be con- two - dimensional images of the face expression samples according to the magnitudes of the pixel values . Optionally , the second normalization processing sub module specifically can be configured to normalize pixel figured to perform the second processing on the three- 30 values of channels of the three - dimensional images of the dimensional images of the face expression samples and perform the identical second processing on the two - dimen- sional images of the face expression samples . Exemplarily , the second processing module specifically can be configured face expression samples and the two - dimensional images of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels may comprise first depth information of the three - dimensional images of the face expression to : perform linear transformation and affine transformation 35 samples , three channels of RGB images of the three - dimen on the three - dimensional images of the face expression samples via the second transformation sub - module and perform contrast stretching on the three - dimensional images of the face expression samples via the second contrast sional images of the face expression samples and three channels of RGB images of the two - dimensional images of the face expression samples . Generally , using a human face as an example , the three stretching sub - module , as well as perform the foregoing 40 dimensional images of the face expression samples and the linear transformation and affine transformation on the two- dimensional images of the face expression samples via the second transformation sub - module and perform contrast stretching on the two - dimensional images of the face expres- two - dimensional images of the face expression samples , which are acquired by the photographic device , comprise redundant parts such as the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face sion samples via the second contrast stretching sub - module ; 45 frame position by face detection , then the face is extracted , or , as another example , perform mirroring and linear trans- formation on the three - dimensional images of the face expression samples via the second transformation sub - mod- ule and perform image pixel value normalization processing the above - mentioned face features , e.g. , eye points , are positioned , and then the foregoing second processing is The method and device for expression recognition , pro performed . on the three - dimensional images of the face expression 50 vided by the present invention , can effectively solve the samples via the second normalization processing sub - mod- ule , as well as perform mirroring and linear transformation on the two - dimensional images of the face expression samples via the second transformation sub - module and problem that the face expression recognition accuracy declines due to different face postures and different light conditions , and improve the accuracy of face expression recognition of the target face at different face postures and perform image pixel value normalization processing on the 55 in different light conditions . two - dimensional images of the face expression samples via the second normalization processing sub - module . Exemplar- ily , the foregoing second processing module specifically can be configured to respectively perform the same second A device for expression recognition provided by embodi ment 5 of the present invention will be specifically elabo rated below in combination with FIG . 5. As shown in FIG . , the device 500 comprises a second acquisition module processing on second depth information ( e.g. , depth images ) 60 501 , a second input module 502 , a second neural network , a third neural network 504 and a second classification of the face expression samples , three channels of RGB images of the three - dimensional images of the face expres- sion samples and three channels of RGB images of the two - dimensional images of the face expression samples ; or module 505 . The second acquisition module 501 is configured to acquire a three - dimensional image of a target face , the perform the same second processing on the overall images 65 three - dimensional image comprising third depth information of the three - dimensional images of the face expression samples and the overall images of the two - dimensional of the target face and fifth color information of the target face . Optionally , the three - dimensional image of the target US 11,023,715 B2 face described above may be a color image . Optionally , the foregoing fifth color information may be an image of an RGB format or a YUV format , or an image of other format that can be converted to and from the foregoing RGB format can be configured to : input the first classification data and the second classification data and output classification results on the expressions of the target face according to the first classification data , the second classification data and or YUV format . Optionally , the second acquisition module 5 support vector machine parameter data , and the support third depth information of the target face to the second 10 eight - dimensional data , i.e. , data for indicating eight expres neural network 503 and input the fifth color information of may acquire a three - dimensional image of a target face , which is photographed by a photographic device , from a The second input module 502 is configured to input the memory . the target face to the third neural network 504 . Optionally , the second neural network 503 comprises three convolutional layers , three down - sampling layers , one neural network 504 comprises four convolutional layers , four down - sampling layers , one dropout layer and two fully - connected layers . vector machine comprises the at least one face expression category and the support vector machine parameter data for recognizing the expression category of the target face . Exemplarily , the first classification data may be a group of sion categories , and the eight expression categories may be fear , sadness , joy , anger , disgust , surprise , nature and con tempt . Optionally , the foregoing data for indicating eight the expressions of the target face respectively belong to the foregoing eight expression categories , and the sum of the eight probability values is 1. Similarly , the second classifi cation data is also of eight expression categories , the input dropout layer and two fully - connected layers . The third 15 expression categories may be eight probability values that The second neural network 503 is configured to classify expressions of the target face according to the third depth 20 of the support vector machine is two groups of eight information of the target face and a second parameter and output first classification data , and the third neural network is configured to classify expressions of the target face according to the fifth color information of the target face and dimensional data , and the support vector machine judges which expression categories the expressions of the target face described above belong to according to the foregoing two groups of eight - dimensional data and the support vector a third parameter and output second classification data , the 25 machine parameter data for recognizing the expression third parameter data for recognizing the expression catego- 30 belongs to the foregoing different expression categories second parameter comprising at least one face expression category and second parameter data for recognizing the expression categories of the target face , and the third param eter comprising the at least one face expression category and ries of the target face . Because most expressions are compound expressions and may belong to at least one face expression category , the foregoing second neural network comprises the foregoing included by the first classification data comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . The foregoing third neural network comprises the foregoing second classification data , and the face expression category of the target face . The foregoing support vector machine may be a linear support vector machine . The classification results output by the support vector machine may be probabilities that the target face described above respectively , and the sum of the probabilities of belonging to the foregoing different expression categories respectively is 1. The support vector machine can sequence the output Optionally , under the condition that the foregoing first classification data and second classification data includes one face expression category , the support vector machine first classification data , and the face expression categories 35 classification results according to the magnitudes of the foregoing probabilities . categories included by the second classification data com- 40 also includes the one face expression category , and the prise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Optionally , the face expres- sion categories included by the first classification data and the second classification data are same . Both the foregoing first classification data and the foregoing second classifica- 45 tion data include eight face expression categories of fear , sadness , joy , anger , disgust , surprise , nature and contempt , and eight groups of parameter data corresponding to the face expression categories of the foregoing eight expression machine . support vector machine can be configured to judge whether the expressions of the target face described above belong to the face expression category included by the support vector Optionally , in order to cope with the circumstance that the acquired target face posture is not ideal or the light condition is not ideal , the device further comprises a third processing module , and the third processing module is configured to perform third processing on the third depth information of categories , e.g. , probabilities that the expressions of the 50 the target face , and input the third depth information of the target face described above belong to the foregoing eight face expression categories respectively . The foregoing sec- ond parameter data and the third parameter data are used for recognizing which of the foregoing eight face expression target face subjected to the third processing to the second input module . The third processing module comprises at least one of a third rotating sub - module , a third transforma tion sub - module , a third alignment sub - module , a third categories the expressions of the target face belong to , e.g. , 55 contrast stretching sub - module and a third normalization the weight of at least one node of the foregoing second neural network , and the weight of at least one node of the third neural network . The second neural network comprises a second convolu- processing sub - module . The third rotating sub - module is configured to determine feature points of the third depth information of the target face , and rotate the third depth information of the target face based on the feature points . tional neural network , and the third neural network com- 60 The third transformation sub - module is configured to per prises a third convolutional neural network . The second classification module 505 is configured to output classification results on the expressions of the target face according to the first classification data and the second classification data . Optionally , the second classification module 505 com- prises a support vector machine , the support vector machine form mirroring , linear transformation and affine transforma tion on the third depth information of the target face . The third alignment sub - module is configured to align the feature points of the third depth information of the target face with a set position . The third contrast stretching sub - module is configured to perform contrast stretching on the third depth information of the target face . The third normalization US 11,023,715 B2 processing sub - module is configured to perform image pixel value normalization processing on the third depth informa- tion of the target face . The third processing module is further configured to information of the target face may be one or more feature points of a standard face image , e.g. , eye points , or a preset position , or feature points in face expression samples that are uniformly aligned when the face expression samples are perform the same third processing on the third depth infor- 5 inputted to the foregoing second neural network during mation of the target face and the fifth color information of the target face , and input the third depth information of the target face and the fifth color information of the target face subjected to the third processing to the second input module . training and feature points in face expression samples that are uniformly aligned when the face expression samples are inputted to the foregoing third neural network during train ing , e.g. , eye points . Optionally , the foregoing set position The third rotating sub - module is further configured to deter- 10 aligned with the feature points of the third depth information mine feature points of the third depth information of the target face and feature points of the fifth color information of the target face , and rotate the third depth information of the target face and the fifth color information of the target of the target face may be one or more feature points of a standard face image , e.g. , eye points , or a preset position , or feature points in face expression samples that are uniformly aligned when the face expression samples are inputted to the face based on the feature points . The third transformation 15 foregoing second neural network during training . sub - module is further configured to perform mirroring , linear transformation and affine transformation on the third depth information of the target face and the fifth color information of the target face . The third alignment sub- Optionally , the foregoing third contrast stretching sub module specifically can be configured to perform section by - section contrast stretching on the third depth information of the target face and the fifth color information of the target module is further configured to align the feature points of the 20 face according to the characteristics of the three - dimensional third depth information of the target face and the fifth color information of the target face with a set position . The third contrast stretching sub - module is further configured to per- form contrast stretching on the third depth information of the image of the target face , or perform section - by - section contrast stretching on pixel values of the third depth infor mation of the target face and the fifth color information of the target face according to the magnitudes of the pixel target face or the fifth color information of the target face . 25 values . The third normalization processing sub - module is further configured to perform image pixel value normalization processing on the third depth information of the target face and the fifth color information of the target face . Optionally , the third normalization processing sub - mod ule specifically can be configured to : normalize pixel values of channels of the third depth information of the target face and the fifth color information of the target face from [ 0 , The foregoing third processing module specifically can be 30 255 ] to [ 0 , 1 ] . The foregoing channels may comprise third configured to : perform the third processing on the third depth information of the target face and perform the iden- tical third processing on the fifth color information of the target face . Exemplarily , the third processing module can depth information of the target face and three channels of an RGB image of the three - dimensional image of the target face . The third normalization processing sub - module is specifically configured to : normalize pixel values of the third perform linear transformation and affine transformation on 35 depth information of the target face from [ 0 , 255 ] to [ 0 , 1 ] . the third depth information of the target face via the third transformation sub - module and perform contrast stretching on the third depth information of the target face via the third contrast stretching sub - module , as well as perform the same Generally , using a human face as an example , the three dimensional image of the target face , which is acquired by the photographic device , comprises redundant parts such as the neck , shoulders and the like in addition to the face , so it linear transformation and affine transformation on the fifth 40 needs to be positioned to the face frame position by face color information of the target face via the third transfor- mation sub - module and perform the same contrast stretching on the fifth color information of the target face via the third contrast stretching sub - module . For another example , the detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are positioned , and then the foregoing third processing is performed . Optionally , the second parameter data is obtained by third processing module can perform mirroring and linear 45 training fourth depth information of multiple face expression transformation on the third depth information of the target face via the third transformation sub - module and perform image pixel value normalization processing on the third depth information of the target face via the third normal- samples via the second neural network , and the third param eter data is obtained by training sixth color information of the multiple face expression samples via the third neural network . Three - dimensional images of the face expression ization processing sub - module , as well as perform the same 50 samples comprise fourth depth information of the face mirroring and linear transformation on the fifth color infor- mation of the target face via the third transformation sub- module and perform the image pixel value normalization processing on the fifth color information of the target face expression samples and sixth color information of the face expression samples . It may be parallel that the second neural network trains the fourth depth information to obtain the second parameter data and the third neural network trains via the third normalization processing sub - module . Option- 55 the sixth color information to obtain the third parameter ally , the foregoing third processing module can respectively perform the same third processing on the third depth infor- mation ( e.g. , a depth image ) of the target face and an RGB image of the three - dimensional image of the target face , or data . Specifically , the second input module 502 can respec tively input the fourth depth information and the sixth color information of the multiple face expression samples to the foregoing second neural network and third neural network respectively perform the same third processing on the third 60 and iterate them , the multiple face expression samples carry depth information of the target face and three channels of the RGB image of the three - dimensional image of the target Optionally , the foregoing feature points may be eye face . face expression categories representing face expression cat egories , a parameter combination having high expression accuracy for recognizing the face expression samples , e.g. , the weight of at least one node of the neural network , is points , or other face features such as a nose tip point and the 65 determined as the second parameter data and the third like . The set position aligned with the feature points of the third depth information of the target face and the fifth color parameter data for recognizing the expression categories of the target face , and the specific content of the second US 11,023,715 B2 parameter data and the third parameter data can be known by referring to the above description . Optionally , the second parameter data and the third parameter data can be obtained by training the foregoing face expression samples off line , to the fourth processing to the second input module . The fourth rotating sub - module is further configured to deter mine feature points of the fourth depth information of the face expression samples and feature points of the sixth color and the product for expression recognition , provided for 5 information of the face expression samples , and rotate the practical use , may not comprise the foregoing face expres- sion samples . Because most expressions are compound expressions and may belong to at least one expression category , the face fourth depth information of the face expression samples and the sixth color information of the face expression samples based on the feature points . The fourth transformation sub - module is further configured to perform mirroring , expression categories included by the second neural network 10 linear transformation and affine transformation on the fourth and the face expression categories included by the third neural network include at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . Each of the face expression samples , the fourth depth information of the depth information of the face expression samples and the sixth color information of the face expression samples . The fourth alignment sub - module is further configured to align the feature points of the fourth depth information of the face face expression sample and the sixth color information of 15 expression samples and the sixth color information of the the face expression sample satisfy ( belong to ) the same face expression category . The foregoing sixth color information is images of an RGB format or a YUV format . Through the face expression categories carried by the foregoing face face expression samples with a set position . The fourth contrast stretching sub - module is further configured to per form contrast stretching on the fourth depth information of the face expression samples or the sixth color information of expression samples , the second neural network and the third 20 the face expression samples . The fourth normalization pro neural network can determine the face expression categories of components ( the fourth depth information of the three- dimensional images of the face expression samples and the sixth color information of the three - dimensional images of cessing sub - module is further configured to perform image pixel value normalization processing on the fourth depth information of the face expression samples and the sixth color information of the face expression samples . The fore the face expression samples ) of the three - dimensional 25 going fourth processing module may be same as or different images of the foregoing face expression samples input to the second neural network and the third neural network , the second neural network can train them to obtain second parameter data corresponding to the foregoing different face from the third processing module . The fourth processing module specifically can be config ured to : perform the fourth processing on the fourth depth information of the face expression samples and perform the expression categories , and the third neural network can train 30 identical fourth processing on the sixth color information of them to obtain third parameter data corresponding to the foregoing different face expression categories . Optionally , in order to cope with the circumstance that the acquired face expression sample postures are not ideal or the the face expression samples . Exemplarily , the fourth pro cessing module specifically can perform linear transforma tion and affine transformation on the fourth depth informa tion of the face expression samples via the fourth light condition is not ideal , the device comprises a fourth 35 transformation sub - module and perform contrast stretching processing module , and the fourth processing module is configured to perform fourth processing on the fourth depth information of the face expression samples , and input the fourth depth information of the face expression samples on the fourth depth information of the face expression samples via the fourth contrast stretching sub - module , as well as perform linear transformation and affine transforma tion on the sixth color information of the face expression subjected to the fourth processing to the second input 40 samples via the fourth transformation sub - module and per module . The fourth processing module comprises at least one of a fourth rotating sub - module , a fourth transformation sub - module , a fourth alignment sub - module , a fourth con- trast stretching sub - module and a fourth normalization pro- form contrast stretching on the sixth color information of the face expression samples via the fourth contrast stretching sub - module ; or , as another example , perform mirroring and linear transformation on the fourth depth information of the cessing sub - module . The fourth rotating sub - module is con- 45 face expression samples via the fourth transformation sub figured to determine feature points of the fourth depth information of the face expression samples , and rotate the fourth depth information of the face expression samples based on the feature points . The fourth transformation module and perform image pixel value normalization pro cessing on the fourth depth information of the face expres sion samples via the fourth normalization processing sub as well as perform mirroring and linear module , sub - module is configured to perform mirroring , linear trans- 50 transformation on the sixth color information of the face formation and affine transformation on the fourth depth information of the face expression samples . The fourth alignment sub - module is configured to align the feature points of the fourth depth information of the face expression expression samples via the fourth transformation sub - mod ule and perform image pixel value normalization processing on the sixth color information of the face expression samples via the fourth normalization processing sub - module . Exem samples with a set position . The fourth contrast stretching 55 plarily , the foregoing fourth processing module specifically sub - module is configured to perform contrast stretching on the fourth depth information of the face expression samples . The fourth normalization processing sub - module is config- ured to perform image pixel value normalization processing can be configured to : respectively perform the same fourth processing on the fourth depth information ( e.g. , depth images ) of the face expression samples and three channels of RGB images of the three - dimensional images of the face on the fourth depth information of the face expression 60 expression samples ; or perform the fourth processing on the samples . The fourth processing module is further configured to perform fourth processing on the fourth depth information of the face expression samples and the sixth color information overall images of the three - dimensional images of the face expression samples , then decompose the overall images into the fourth depth information of the face expression samples and the sixth color information of the face expression of the face expression samples , and input the fourth depth 65 samples and respectively input them to the second neural information of the face expression samples and the sixth color information of the face expression samples subjected module 502 . network and the third neural network via the second input US 11,023,715 B2 Optionally , the foregoing feature points may be eye points , or other face features such as a nose tip point and the like . The set position aligned with the feature points of the fourth depth information of the face expression samples and mation of the multiple face expression samples is also of eight expression categories , the input of the support vector machine is two groups of eight - dimensional data described above , and because the two groups of eight - dimensional the sixth color information of the face expression samples , 5 data described above carry face expression categories rep or the set position aligned with the feature points of the fourth depth information of the face expression samples , as described above , may be one or more feature points of a standard face image , e.g. , eye points , or a preset position , or feature points in the face expression samples that are uni- 10 formly aligned when the face expression samples are input- ted to the foregoing second neural network and the third neural network during training , e.g. , eye points . Optionally , the fourth contrast stretching sub - module spe- resenting expression categories , the support vector machine data carrying the face expression categories of the expres sion categories can be trained via the two groups of eight dimensional data described above . The method and device for expression recognition , pro vided by the present invention , can effectively solve the problem that the face expression recognition accuracy declines due to different face postures and different light conditions , and improve the accuracy of face expression cifically can be configured to : perform section - by - section 15 recognition of the target face at different face postures and contrast stretching on the fourth depth information of the face expression samples and the sixth color information of the face expression samples according to the characteristics of the fourth depth information of the face expression in different light conditions . A device for expression recognition provided by embodi ment 6 of the present invention will be specifically elabo rated below in combination with FIG . 6. The device com samples and / or the sixth color information of the face 20 prises a third acquisition module 601 , a third input module expression samples , or perform section - by - section contrast stretching on pixel values of the fourth depth information of the face expression samples and the sixth color information of the face expression samples according to the magnitudes of the pixel values . Optionally , the fourth normalization processing sub - mod- ule specifically can be configured to : normalize pixel values of the fourth depth information of the face expression samples from [ 0 , 255 ] to [ 0 , 1 ] ; or , the fourth normalization and a fourth neural network 603 . The third acquisition module 601 is configured to acquire a three - dimensional image of a target face , the three - dimen sional image comprising fifth depth information of the target face and seventh color information of the target face . Optionally , the third acquisition module 601 can acquire a three - dimensional image of a target face , which is photo graphed by a photographic device , from a memory . Option ally , the three - dimensional image of the target face described processing sub - module specifically can be configured to : 30 above may be a color image . Optionally , the seventh color normalize pixel values of channels of the fourth depth information of the face expression samples and the sixth color information of the face expression samples from [ 0 , ] to [ 0 , 1 ] . The foregoing channels may comprise fourth information may be an image of an RGB format or a YUV format , or an image of other format that can be converted to and from the foregoing RGB format or YUV format . The third input module 602 is configured to input the fifth depth information of three - dimensional images of the face 35 depth information of the target face and the seventh color expression samples , and three channels of RGB images of the sixth color information of the face expression samples . Generally , using a human face as an example , the three- dimensional images of the face expression samples , which information of the target face to the fourth neural network . Optionally , input to the fourth neural network may be a depth image of the target face and an RGB image of the three - dimensional image of the target face ; input to the are acquired by the photographic device , comprise redun- 40 fourth neural network may also be a depth image of the dant parts such as the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are posi- target face and three channels of an RGB image of the three - dimensional image of the target face . Optionally , the fourth neural network comprises a fourth convolutional neural network . The fourth convolutional neural network tioned , and then the foregoing fourth processing is per- 45 comprises one segmentation layer , eight convolutional lay formed . The fifth color information is an image of an RGB format or a YUV format . The sixth color information is images of an RGB format or a YUV format . ers , eight down - sampling layers , two dropout layers and five fully - connected layers . The fourth neural network 603 is configured to classify expressions of the target face according to the fifth depth The support vector machine parameter data for recogniz- 50 information of the target face , the seventh color information ing the expression category of the target face is obtained by : training the second neural network with the fourth depth information of the facial expression samples , training the third neural network with the sixth color information of the facial expression samples , combining corresponding output 55 data from the second fully - connected layer of the second neural network and the second fully - connected layer of the third neural network as inputs , and training the support vector machine with the inputs and corresponding expres- of the target face and a fourth parameter , the fourth param eter comprising at least one face expression category and fourth parameter data for recognizing the expression cat egories of the target face . Optionally , because most expressions are compound expressions and may belong to at least one expression category , the fourth neural network may include the fourth parameter , and the face expression categories included by the fourth parameter include at least one of : fear , sadness , sion labels of the facial expression samples . Exemplarily , the 60 joy , anger , disgust , surprise , nature and contempt . Exem output data when the second neural network trains the fourth depth information of the multiple face expression samples may be a group of eight - dimensional data , i.e. , data for indicating eight expression categories , and the eight expres- plarily , the foregoing fourth parameter may include the face expression categories of eight expression categories of fear , sadness , joy , anger , disgust , surprise , nature and contempt , and fourth parameter data for recognizing the foregoing sion categories may be fear , sadness , joy , anger , disgust , 65 eight face expression categories , e.g. , the weight of at least surprise , nature and contempt . Similarly , the output data when the third neural network trains the sixth color infor- one node of the fourth neural network . Specifically , the classification results output by the fourth neural network 603 US 11,023,715 B2 may be probabilities that the target face described above belongs to the foregoing different expression categories respectively , and the sum of the probabilities of belonging to the foregoing different expression categories respectively is normalization processing on the seventh color information of the target face via the fifth normalization processing sub - module . Optionally , the foregoing fifth processing mod ule specifically can be configured to : respectively perform 1. The fourth neural network 603 can sequence the output 5 the same fifth processing on the fifth depth information ( e.g. , neural network can be configured to judge whether the 10 pose the overall image into the fifth depth information and classification results according to the magnitudes of the foregoing probabilities . Optionally , under the condition that the foregoing fourth parameter includes one face expression category , the fourth expressions of the target face described above belong to the face expression category included by the fourth parameter . Optionally , in order to cope with the circumstance that the acquired target face posture is not ideal or the light condition can be processed to approximately meet the requirement of a standard face or the using requirement , specifically , the device further comprises a fifth processing module , and the fifth processing module is configured to perform fifth pro- a depth image ) of the target face and three channels of an RGB image of the seventh color information of the target face , or perform the fifth processing on the overall image of the three - dimensional image of the target face , then decom the seventh color information and input them to the fourth neural network via the second input module 502 . Optionally , the foregoing feature points may be eye like . The foregoing set position aligned with the feature points of the three - dimensional image of the target face may be one or more feature points of a standard face image , e.g. , eye points , or a preset position , or feature points in face is not ideal , the three - dimensional image of the target face is points , or other face features such as a nose tip point and the cessing on the three - dimensional image of the target face , 20 expression samples that are uniformly aligned when the face and input the three - dimensional image of the target face subjected to the fifth processing to the third input module . The fifth processing module comprises at least one of the following sub - modules : a fifth rotating sub - module , a fifth expression samples are inputted to the foregoing fourth neural network during training , e.g. , eye points . Optionally , the foregoing fifth contrast stretching sub module specifically can be configured to perform section transformation sub - module , a fifth alignment sub - module , a 25 by - section contrast stretching on the three - dimensional fifth contrast stretching sub - module and a fifth normalization processing sub - module . The fifth rotating sub - module is configured to determine feature points of the three - dimen- sional image of the target face , and rotate the three - dimen- image of the target face according to the characteristics of the three - dimensional image of the target face , or perform section - by - section contrast stretching on pixel values of the three - dimensional image of the target face according to the sional image of the target face based on the feature points . 30 magnitudes of the pixel values . The fifth transformation sub - module is configured to per- form mirroring , linear transformation and affine transforma- tion on the three - dimensional image of the target face . The fifth alignment sub - module is configured to align the feature Optionally , the fifth normalization processing sub - module specifically can be configured to normalize pixel values of channels of the three - dimensional image of the target face from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels may com points of the three - dimensional image of the target face with 35 prise depth information of the three - dimensional image of a set position . The fifth contrast stretching sub - module is configured to perform contrast stretching on the three- dimensional image of the target face . The fifth normalization processing sub - module is configured to perform image pixel the target face and three channels of an RGB image of the three - dimensional image of the target face . Generally , using a human face as an example , the three dimensional image of the target face , which is acquired by value normalization processing on the three - dimensional 40 the photographic device , comprises redundant parts such as image of the target face . The foregoing fifth processing module specifically can be configured to perform the same fifth processing on the fifth depth information of the target face and the seventh color the neck , shoulders and the like in addition to the face , so it needs to be positioned to the face frame position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are positioned , and then the information of the target face , i.e. , perform the fifth pro- 45 foregoing fifth processing is performed . cessing on the fifth depth information of the target face and perform the identical fifth processing on the seventh color information of the target face . Exemplarily , the foregoing fifth processing module specifically can be configured to : Optionally , the fourth parameter data is obtained by training three - dimensional images of multiple face expres sion samples via the fourth neural network . The three dimensional images of the face expression samples comprise perform linear transformation and affine transformation on 50 sixth depth information of the face expression samples and the fifth depth information of the target face via the fifth transformation sub - module and perform contrast stretching on the fifth depth information of the target face via the fifth contrast stretching sub - module , as well as perform linear eighth color information of the face expression samples . Specifically , the sixth depth information and the eighth color information of the multiple face expression samples can be input to the fourth neural network and iterated , the multiple transformation and affine transformation on the seventh 55 face expression samples carry face expression categories color information of the target face via the fifth transforma- tion sub - module and perform contrast stretching on the seventh color information of the target face via the fifth contrast stretching sub - module ; or , as another example , representing face expression categories , the fourth neutral network can determine a parameter combination having high expression accuracy for recognizing the face expression samples , e.g. , the weight of at least one node of the neural perform mirroring and linear transformation on the fifth 60 network , as the fourth parameter for recognizing the expres depth information of the target face via the fifth transfor- mation sub - module and perform image pixel value normal- ization processing on the fifth depth information of the target face via the fifth normalization processing sub - module , as sion categories of the target face , and the specific content of the fourth parameter can be known by referring to the above description . Optionally , the fourth parameter can be obtained by training the foregoing face expression samples off line , well as perform mirroring and linear transformation on the 65 and the product for expression recognition , provided for seventh color information of the target face via the fifth transformation sub - module and perform image pixel value sion samples . practical use , may not comprise the foregoing face expres US 11,023,715 B2 Because most expressions are compound expressions and may belong to at least one expression category , each of the face expression samples satisfies ( belongs to ) at least one of the following face expression categories : fear , sadness , joy , transformation on the eighth color information of the face expression samples via the sixth transformation sub - module and perform contrast stretching on the eighth color infor mation of the face expression samples via the sixth contrast anger , disgust , surprise , nature and contempt . Each of the 5 stretching sub - module ; or , as another example , perform of an RGB format or a YUV format . Through the face 10 the face expression samples via the sixth normalization the eighth color information of the face expression samples 15 module and perform image pixel value normalization pro face expression samples , the sixth depth information of the face expression sample and the eighth color information of the face expression sample satisfy ( belong to ) the same face expression category . The eighth color information is images expression categories carried by the foregoing face expres sion samples , the fourth neural network can determine the face expression categories of the input components ( the sixth depth information of the face expression samples and are components of the three - dimensional image ) of the face expression samples described above , and the fourth neural network can train them to obtain the fourth parameter corresponding to the foregoing different face expression Optionally , in order to cope with the circumstance that the acquired face expression sample postures are not ideal or the light condition is not ideal , the three - dimensional images of the face expression samples can be processed to approxi- categories . mirroring and linear transformation on the sixth depth information of the face expression samples via the sixth transformation sub - module and perform image pixel value normalization processing on the sixth depth information of processing sub - module , as well as perform mirroring and linear transformation on the eighth color information of the face expression samples via the sixth transformation sub cessing on the eighth color information of the face expres sion samples via the sixth normalization processing sub module . Exemplarily , the foregoing sixth processing module specifically can be configured to : respectively perform the same sixth processing on the sixth depth information ( e.g. , depth images ) of the face expression samples , and three channels of the eighth color information , e.g. , RGB images , of the three - dimensional images of the face expression samples ; or perform the same sixth processing on the overall mately meet the requirement of a standard face or the using 25 images of the three - dimensional images of the face expres requirement , specifically , for example , the device further comprises a sixth processing module , and the sixth process- ing module is configured to perform fifth processing on the three - dimensional images of the face expression samples , sion samples , then decompose the overall images into the sixth depth information and the eighth color information and input them to the fourth neural network . Optionally , the foregoing feature points may be eye and input the three - dimensional images of the face expres- 30 points , or other face features such as a nose tip point and the sion samples subjected to the fifth processing to the third input module . The sixth processing module comprises a sixth rotating sub - module , a sixth transformation sub - mod- ule , a sixth alignment sub - module , a sixth contrast stretching like . The foregoing set position aligned with the feature points of the three - dimensional images of the multiple face expression samples may be feature points of a standard face image , e.g. , eye points , or a preset position , or feature points sub - module and a sixth normalization processing sub - mod- 35 in the face expression samples that are uniformly aligned ule . The sixth rotating sub - module is configured to deter- mine feature points of the three - dimensional images of the face expression samples , and rotate the three - dimensional images of the face expression samples based on the feature when the face expression samples are inputted to the fore going fourth neural network during training , e.g. , eye points . Optionally , the foregoing sixth contrast stretching sub module specifically can be configured to perform section points . The sixth transformation sub - module is configured to 40 by - section contrast stretching on the three - dimensional perform mirroring , linear transformation and affine transfor- mation on the three - dimensional images of the face expres- sion samples . The sixth alignment sub - module is configured to align the feature points of the three - dimensional images of images of the face expression samples according to the characteristics of the three - dimensional images of the face expression samples , or perform section - by - section contrast stretching on pixel values of the three - dimensional images the face expression samples with a set position . The sixth 45 of the face expression samples according to the magnitudes contrast stretching sub - module is configured to perform contrast stretching of images on the three - dimensional images of the face expression samples . The sixth normal- ization processing sub - module is configured to perform of the pixel values . Optionally , the sixth normalization processing sub - mod ule is specifically configured to : normalize pixel values of channels of the three - dimensional images of the face expres image pixel value normalization processing on the three- 50 sion samples from [ 0 , 255 ] to [ 0 , 1 ] . The foregoing channels dimensional images of the face expression samples . The foregoing sixth processing module may be same as or different from the fifth processing module . Optionally , the sixth processing module specifically can be configured to : perform the same sixth processing on the 55 sion samples . may comprise the sixth depth information of the three dimensional images of the face expression samples , and three channels of the eight color information , e.g. , RGB images , of the three - dimensional images of the face expres sixth depth information and the eighth color information of the face expression samples , i.e. , perform the sixth process- ing on the sixth depth information of the face expression samples and perform the identical sixth processing on the Generally , using a human face as an example , the three dimensional images of the face expression samples , which are acquired by the photographic device , comprise redun dant parts such as the neck , shoulders and the like in addition eighth color information of the face expression samples . 60 to the face , so it needs to be positioned to the face frame Exemplarily , the sixth processing module can perform linear transformation and affine transformation on the sixth depth information of the face expression samples via the sixth transformation sub - module and perform contrast stretching position by face detection , then the face is extracted , the above - mentioned face features , e.g. , eye points , are posi tioned , and then the foregoing sixth processing is performed . The method and device for expression recognition , pro on the sixth depth information of the face expression 65 vided by the present invention , can effectively solve the samples via the sixth contrast stretching sub - module , as well as perform the foregoing linear transformation and affine problem that the face expression recognition accuracy declines due to different face postures and different light US 11,023,715 B2 conditions , and improve the accuracy of face expression recognition of the target face at different face postures and in different light conditions . A computer readable storage medium 700 provided by an The memory can be configured to store the computer program and / or modules , and the processor achieves various functions of the device / terminal equipment by running or executing the computer program and / or modules stored in embodiment of the present invention will be specifically 5 the memory and calling data stored in the memory . The elaborated below in combination with FIG . 7. The computer readable storage medium 700 stores a computer program , and is wherein the computer program , when executed by a first processor 701 , implements the steps of the method of any of the foregoing embodiments 1-3 . The computer readable storage medium 700 provided by the present invention can effectively solve the problem that the face expression recognition accuracy declines due to memory may include a program storage area and a data storage area , wherein the program storage area can store an operating system , an application required by at least one function ( e.g. , image play function , etc. ) , etc .; and the data storage area can store data ( e.g. , video data , images , etc. ) created according to the use of a mobile phone . Moreover , the memory may include a high - speed random access memory , and may also include a non - volatile memory such as a hard disk , a memory or a plug - in hard disk , a smart different face postures and different light conditions , and 15 media card ( SMC ) , a secure digital ( SD ) card , a flash card , improve the accuracy of face expression recognition of the target face at different face postures and in different light conditions . A device 800 for expression recognition , provided by an at least one hard disk storage device , a flash device , or other non - volatile solid - state storage device . When the modules / units integrated in the device / terminal equipment are implemented in the form of software func embodiment of the present invention , will be specifically 20 tional units and sold or used as independent products , they elaborated below in combination with FIG . 8. The device comprises a memory 801 , a second processor 802 and a computer program which is stored in the memory 801 and can be run on the second processor 802 , and is wherein the may be stored in a computer readable storage medium . Based on such an understanding , all of or part of processes in the methods of the above - mentioned embodiments of the present invention may also be implemented with a computer computer program , when executed by the second processor 25 program instructing corresponding hardware . The computer ments 1-3 . , implements the steps of the method of any of embodi- The device 800 for expression recognition , provided by the present invention , can effectively solve the problem that program may be stored in a computer readable storage medium . The computer program , when executed by the processor , can implement the steps of the method embodi ments described above . The computer program includes the face expression recognition accuracy declines due to 30 computer program codes , which may be in the form of different face postures and different light conditions , and improve the accuracy of face expression recognition of the different face postures and in different light target face conditions . source codes , object codes or executable files , or in some intermediate form , etc. The computer readable medium may include any entity or device which can carry the computer program codes , a recording medium , a U disk , a mobile hard Exemplarily , the computer program can be segmented 35 disk , a magnetic disk , an optical disk , a computer memory , into one or more modules / units , and the one or more modules / units are stored in the memory and executed by the processor to accomplish the present invention . The one or more modules / units may be a series of computer program a read - only memory ( ROM ) , a random access memory ( RAM ) , an electric carrier signal , an electrical signal , a software distribution medium , etc. Imaging of the object target object in the embodiments instruction segments which can achieve specific functions , 40 described above may be partial imaging or integral imaging and the instruction segments are used for describing the execution process of the computer program in the device / terminal equipment . The device / terminal equipment may be computing equip- of the target object . Whichever of the partial imaging or the integral imaging , or a corresponding adjustment made to the partial imaging or the integral imaging is adopted is appli cable to the method or device provided by the present ment such as a mobile phone , a tablet computer , a desktop 45 invention . The foregoing adjustment made by those of computer , a notebook computer , a palm computer , a cloud server or the like . The device / terminal equipment may include , but not limited to , a processor or a memory . It could be understood by those skilled in the art that the schematic diagrams of the present invention are merely examples of 50 the device / terminal equipment , instead of limiting the device / terminal equipment , which may include more or less components than in the diagrams , or combine some com ponents or different components , e.g. , the device / terminal equipment may further include input / output equipment , net- 55 work access equipment , a bus , etc. The foregoing processor may be a central processing unit ( CPU ) , and may also be other general processor , a digital signal processor ( DSP ) , an application specific integrated circuit ( ASIC ) , a field - programmable gate array ( FPGA ) or 60 other programmable logic device , a discrete gate or transis- tor logic device , a discrete hardware component , etc. The general processor may be a microprocessor or any conven tional processor or the like , and the processor is a control center of the device / terminal equipment and connects all 65 parts of the whole device / terminal equipment by using various interfaces and lines . ordinary skill in the art without any creative effort shall fall into the protection scope of the present invention . What is claimed is : 1. A method for expression recognition , comprising acquiring a three - dimensional image and a two - dimen sional image of a target face , the three - dimensional image comprising first depth information of the target face and first color information of the target face , and the two - dimensional image comprising second color information of the target face ; inputting the first depth information of the target face and the first color information and the second color infor mation of the target face into one or more neural classifying an expression of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the target face , and a first parameter by using the one or more neural networks , the first param eter comprising at least one facial expression category and first parameter data for recognizing an expression category of the target face . networks ; and US 11,023,715 B2 2. The method according to claim 1 , wherein before inputting the first depth information of the target face , the first color information of the target face , and the second color information of the target face into the one or more neural networks , the method further comprises performing a same processing on the three - dimensional image of the target face and the two - dimensional image of the target face , the processing comprising at least one of : determining feature points of the three - dimensional image of the target face and the two - dimensional image of the target face , and rotating the three - dimensional image of the target face and the two - dimensional image of the target face based on the feature points ; performing mirroring , linear transformation , and affine transformation on the three - dimensional image of the target face and the two - dimensional image of the target face ; aligning the feature points of the three - dimensional image aligning the feature points of the three - dimensional images of the facial expression samples and the two dimensional images of the facial expression samples with a set position ; performing contrast stretching on the three - dimensional images of the facial expression samples and the two dimensional images of the facial expression samples ; performing pixel value normalization on the three - dimen sional images of the facial expression samples and the two - dimensional images of the facial expression and samples . 6. The method according to claim 5 , wherein the perform ing pixel value normalization on the three - dimensional images of the facial expression samples and the two - dimen sional images of the facial expression samples comprises normalizing pixel values of each channel of the three dimensional images of the facial expression samples and the of the target face and the two - dimensional image of the 20 two - dimensional images of the facial expression samples ] to [ 0 , 1 ] . target face with a set position ; performing contrast stretching on the three - dimensional image of the target face and the two - dimensional image of the target face ; and performing pixel value normalization on the three - dimen- 25 sional image of the target face and the two - dimensional image of the target face . 3. The method according to claim 2 , wherein the perform ing pixel value normalization on the three - dimensional image of the target face and the two - dimensional image of 30 the target face comprises normalizing pixel values of each channel of the three - dimensional image of the target face and the two - dimensional image of the target face from [ 0 , 4. The method according to claim 1 , wherein : the first parameter data for recognizing the expression category of the target face is obtained by training the one or more neural networks with three - dimensional images of facial expression samples and two - dimen- sional images of the facial expression samples ; the three - dimensional images of the facial expression samples comprise second depth information of the facial expression samples and second color information of the facial expression samples ; and the two - dimensional images of the facial expression 45 samples comprise third color information of the facial expression samples . 5. The method according to claim 4 , wherein before training the one or more neural networks with the three- two - dimensional images of the facial expression samples , the method further comprises performing a same processing on the three - dimensional images of the facial expression samples and the two - dimensional images of the facial expression samples , the processing comprising at least one 55 of : determining feature points of the three - dimensional images of the facial expression samples and the two dimensional images of the facial expression samples , and rotating the three - dimensional images of the facial 60 expression samples and the two - dimensional images of the facial expression samples based on the feature performing mirroring , linear transformation , and affine transformation on the three - dimensional images of the 65 facial expression samples and the two - dimensional images of the facial expression samples ; points ; from [ 0 , 255 ] to [ 0 , 1 ] . 7. The method according to claim 5 , wherein : each facial expression sample has at least one of the following facial expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt ; each facial expression sample , the second depth informa tion of the facial expression sample , the second color information of the facial expression sample , and the third color information of the facial expression sample have the same facial expression category . 8. The method according to claim 2 , wherein the facial expression categories included in the one or more neural networks comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . 9. The method according to claim 2 , wherein the feature points are eye points . 10. The method according to claim 1 , wherein the one or more neural networks comprise a convolutional neural net 11. The method according to claim 10 , wherein the convolutional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer , and two fully - connected layers . 12. The method according to claim 2 , wherein the first color information and the second color information are images of an RGB format or a YUV format . 13. The method according to claim 4 , wherein the second color information and the third color information are images 14. The method according to claim 1 , wherein : the inputting comprises inputting the first depth informa tion of the target face to a first neural network and inputting the first color information of the target face to a second neural network ; the classifying comprises : classifying the expression of the target face according to the first depth information of the target face and a first parameter , and outputting first classification data by the first neural network , and classifying the expression of the target face according to the first color information of the target face and a second parameter , and outputting second classifica tion data by the second neural network , the second parameter comprising the at least one facial expres sion category and second parameter data for recog nizing the expression category of the target face ; and dimensional images of the facial expression samples and the 50 of an RGB format or a YUV format . work . US 11,023,715 B2 the outputting comprises outputting a classification result on the expression of the target face according to the first classification data and the second classification data . 15. The method according to claim 14 , wherein the outputting a classification result on the expression of the 5 target face according to the first classification data and the second classification data comprises : inputting the first classification data and the second clas- sification data into a support vector machine ; and outputting the classification result on the expression of the target face according to the first classification data , the second classification data , and support vector machine parameter data by the support vector machine , the support vector machine comprising the at least one facial expression category and support vector machine parameter data for recognizing the expression category of the target face . performing mirroring , linear transformation , and affine transformation on the second depth information or the second color information of the facial expression samples ; aligning the feature points of the second depth informa tion or the second color information of the facial expression samples with a set position ; performing contrast stretching on the second depth infor mation or the second color information of the facial expression samples ; and performing pixel value normalization on the second depth information or the second color information of the facial expression samples . 20. The method according to claim 19 , wherein the performing pixel value normalization on the second depth information of the facial expression samples comprises normalizing pixel values of the second depth information or the second color information of the facial expression 16. The method according to claim 15 , wherein before inputting the first depth information of the target face to the 20 samples from [ 0 , 255 ] to [ 0 , 1 ] . first neural network and inputting the first color information of the target face to the second neural network , the method further comprises performing a first processing on the first depth information or the first color information of the target face , the first processing comprising at least one of : determining feature points of the first depth information or the first color information of the target face , and rotating the first depth information or the first color information of the target face based on the feature points ; 21. The method according to claim 18 , wherein the support vector machine parameter data for recognizing the expression category of the target face is obtained by : training the first neural network with the second depth information of the facial expression samples ; training the second neural network with the second color information of the facial expression samples ; combining corresponding data output from a second fully connected layer of the first neural network and a second fully - connected layer of the second neural network as performing mirroring , linear transformation , and affine transformation on the first depth information or the first color information of the target face ; aligning the feature points of the first depth information or the first color information of the target face with a set performing contrast stretching on the first depth informa- tion or the first color information of the target face ; and performing pixel value normalization on the first depth 40 information or the first color information of the target position ; face . 17. The method according to claim 16 , wherein perform ing pixel value normalization on the first depth information inputs ; and sion samples . training the support vector machine with the inputs and corresponding expression labels of the facial expres 22. The method according to claim 18 , wherein each facial expression sample has at least one of the following facial expression categories : fear , sadness , joy , anger , disgust , surprise , nature , and contempt ; and each facial expression sample , the second depth informa tion of the facial expression sample , and the second color information of the facial expression sample have the same facial expression category . 23. The method according to claim 14 , wherein the facial of the target face comprises normalizing pixel values of each 45 expression categories included in the first neural network channel of the first depth information or the first color information of the target face from [ 0 , 255 ] to [ 0 , 1 ] . 18. The method according to claim 15 , wherein : the first parameter data is obtained by training the first neural network with second depth information of facial 50 expression samples , and the second parameter data is obtained by training the second neural network with second color information of the facial expression samples . 19. The method according to claim 18 , wherein before 55 training the first neural network with the second depth information of the facial expression samples or training the second neural network with the second color information , the method further comprises performing a second process ing on the second depth information or the second color 60 information of the facial expression samples , the second processing comprising at least one of : determining feature points of the second depth informa- tion or the second color information of the facial expression samples , and rotating the second depth 65 information or the second color information of the facial expression samples based on the feature points ; and the second neural network include at least one of : fear , sadness , joy , anger , disgust , surprise , nature , and contempt . 24. The method according to claim 16 , wherein the feature points are eye points . 25. The method according to claim 14 , wherein the first neural network comprises a first convolutional neural net work , and the second neural network comprises a second convolutional neural network . 26. The method according to claim 25 , wherein : the first convolutional neural network comprises three convolutional layers , three down - sampling layers , one dropout layer , and two fully - connected layers ; and the second convolutional neural network comprises four convolutional layers , four down - sampling layers , one dropout layer , and two fully - connected layers . 27. The method according to claim 14 , wherein the first color information is an image of an RGB format or a YUV 28. The method according to claim 18 , wherein the second color information is images of an RGB format or a YUV format . format . US 11,023,715 B2 29. The method according to claim 1 , wherein : the inputting comprises inputting the first depth informa- tion of the target face and the first color information of the target face to the one or more neural networks ; and the classifying comprises classifying the expression of the 5 target face according to the first depth information of the target face , the first color information of the target face , and a first parameter by the neural network . 30. The method according to claim 29 , wherein before inputting the first depth information of the target face and the 10 first color information of the target face to the one or more neural networks , the method further comprises performing a first processing on the three - dimensional image of the target face , the first processing comprising at least one of : determining feature points of the three - dimensional image 15 of the target face , and rotating the three - dimensional image of the target face based on the feature points ; performing mirroring , linear transformation , and affine transformation on the three - dimensional image of the target face ; aligning the feature points of the three - dimensional image of the target face with a set position ; performing contrast stretching on the three - dimensional image of the target face ; and ues of each channel of the three - dimensional images of the facial expression samples from [ 0 , 255 ] to [ 0 , 1 ] . 35. The method according to claim 32 , wherein : each facial expression sample has at least one of the following facial expression categories : fear , sadness , joy , anger , disgust , surprise , nature and contempt ; and each facial expression sample , the second depth informa tion of the facial expression samples , and the second color information of the facial expression samples have the same facial expression category . 36. The method according to claim 29 , wherein the facial expression categories included in the one or more neural networks comprise at least one of : fear , sadness , joy , anger , disgust , surprise , nature and contempt . 37. The method according to claim 30 , wherein the feature points are eye points . 38. The method according to claim 29 , wherein the fourth neural network comprises a fourth convolutional neural 39. The method according to claim 38 , wherein the convolutional neural network comprises one segmentation layer , eight convolutional layers , eight down - sampling lay ers , two dropout layers , and five fully - connected layers . 40. The method according to claim 29 , wherein the second performing pixel value normalization on the three - dimen- 25 color information is an image of an RGB format or a YUV network . sional image of the target face . 31. The method according to claim 30 , wherein the pixel value normalization on the three - dimensional image of the target face comprises normalizing pixel values of each channel of the three - dimensional image of the target face 30 from [ 0 , 255 ] to [ 0 , 1 ] . 32. The method according to claim 29 , wherein : the first parameter data is obtained by training three dimensional images of facial expression samples via the one or more neural networks ; and the three - dimensional images of the facial expression samples comprise second depth information of the facial expression samples and second color information of the facial expression samples . 33. The method according to claim 32 , wherein before the 40 three - dimensional images of the facial expression samples are trained via the one or more neural networks , the method further comprises performing a second processing on the three - dimensional images of the facial expression samples , the second processing comprising at least one of : determining feature points of the three - dimensional images of the facial expression samples , and rotating the three - dimensional images of the facial expression samples based on the feature points ; performing mirroring , linear transformation , and affine 50 transformation on the three - dimensional images of the facial expression samples ; aligning the feature points of the three - dimensional images of the facial expression samples with a set performing contrast stretching on the three - dimensional images of the facial expression samples ; and performing pixel value normalization on the three - dimen sional images of the facial expression samples . 34. The method according to claim 33 , wherein the pixel 60 value normalization on the three - dimensional images of the facial expression samples comprises normalizing pixel val position ; format . format . 41. The method according to claim 32 , wherein the second color information is images of an RGB format or a YUV 42. A non - transitory computer readable storage medium , which stores a computer program , wherein the computer program , when executed by a first processor , implements the steps of the method of claim 1 . 43. A device for expression recognition , comprising a memory , a second processor and a computer program which is stored in the memory and can be run on the second processor , wherein the computer program , when executed by the second processor , implements the steps of the method of claim 1 . comprising : 44. A device for expression recognition , characterized by target face ; one or more neural networks ; an acquisition module configured to acquire a three dimensional image and a two - dimensional image of a target face , the three - dimensional image comprising first depth information of the target face and first color information of the target face , and the two - dimensional image comprising second color information of the an input module configured to input the first depth infor mation of the target face and the first color information and the second color information of the target face to the one or more neural networks ; and wherein the one or more neural networks are configured to classify an expression of the target face according to the first depth information of the target face , the first color information of the target face , the second color information of the target face , and a first parameter , the first parameter comprising at least one face expression category and first parameter data for recognizing an expression category of the target face .