US008332207 B2 (12) United States Patent Brants et al. (10) Patent No.: (45) Date of Patent: US 8,332,207 B2 Dec. 11, 2012 (54) LARGE LANGUAGE MODELS IN MACHINE TRANSLATION (75) Inventors: Thorsten Brants, Palo Alto, CA (US); Ashok C. Popat, Menlo Park, CA (US); Peng Xu, San Jose, CA (US); Franz J. Och, Mountain View, CA (US); Jeffrey Dean, Palo Alto, CA (US) (73) Assignee: Google Inc., Mountain View, CA (US) (*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 U.S.C. 154(b) by 995 days. (21) Appl. No.: 11/767,436 (22) Filed: Jun. 22, 2007 (65) Prior Publication Data US 2008/0243481 A1 Oct. 2, 2008 6,314,399 B1 * 1 1/2001 Deligne et al. ................ 704/257 7,197.457 B2 * 3/2007 Weng et al. ... 7.406,416 B2 * 7/2008 Chelba et al. .. 7.565,372 B2 * 7/2009 Zhang et al. .......................... 1.1 7.593,843 B2 * 9/2009 Aue et al. ... 7,711,545 B2* 5/2010 Koehn ........ 2002fOO77823 A1* 6, 2002 Fox et al. .... 2003/019 1625 A1 * 10, 2003 Gorin et al. ....................... TO4f1 2004/0220809 A1 * 1 1/2004 Wang et al. ................... 704/257 TO4/255 ... 704/240 ... 704/2 TO4/5 ... 704,260 (Continued) FOREIGN PATENT DOCUMENTS CN A 9, 2005 OTHER PUBLICATIONS International Search Report and Written Opinion from correspond ing International Application No. PCT/US2008/0581 16 dated Sep. 12, 2008. (Continued) Primary Examiner — Michael Colucci (74) Attorney, Agent, or Firm — Harness, Dickey & Pierce, Related U.S. Application Data (60) Provisional application No. 60/920,283, filed on Mar. 26, 2007. P.L.C. (57) (51) Int. Cl. G06F 17/27 (2006.01) (52) U.S. Cl. ............ 7949; 7025; 704,25570425 704,240, 704/2; 704/10; 704/1:370/270 (58) Field of Classification Search ................ 704/1, 10, 704/240, 257, 255, 251, 2: 707/999.102; See application file for complete search history. 370/270 (56) References Cited U.S. PATENT DOCUMENTS ABSTRACT Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an ordern corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score deter mined as a function of a backoff factor and a relative fre quency of a corresponding backoff n-gram in the corpus. 6,167,369 A * 12/2000 Schulze ....... 6,167,398 A * 12/2000 Wyard et al. .......................... 1f1 704/9 Claims, 4 Drawing Sheets N. Language Model n-grams so n-gram probabilities Backoff scores Y 206 US 8,332,207 B2 Page 2 U.S. PATENT DOCUMENTS 2005, 0096907 A1* 5/2005 Bacchiani et al. ............ 704/257 2005/0276235 A1* 12, 2005 Lee et al. .... 2006/01 16997 A1* 6/2006 Yu et al. ............................ 707/4 2007/0078653 A1* 4, 2007 Olsen ....... 2008/0040114 A1 2/2008 Zhou et al. .. 2008/00591.87 A1* 3, 2008 Rotblat et al. 2008/0082485 A1 4/2008 Church et al. .................... 707/3 2008. O154600 A1* 6, 2008 Tian et al. ...... 2008/01621 17 A1* 7/2008 Bangalore et al. . 2009/0171662 A1* 7/2009 Huang et al. ... 2010/0004930 A1* 1/2010 Strope et al. .................. TO4/240 370,270 704/240 704/257 704/257 704,251 ... 704/10 704,251 OTHER PUBLICATIONS Katz, S.M.. “Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer", IEEE Trans actions on Acoustics, Speech and Signal Processing, IEEE Inc. New York, USA, vol. ASSP-35, No. 3, Mar. 1, 1987, pp. 400-401. Kneseret al., “Improved Backing-Off for M-GRAM Language Mod eling”. Acoustics, Speech, and Signal Processing, 1995. ICASSP 95., 1995 Inter National Conference on Detroit, MI, USA May 9-12, 1995, New York, NY, USA, IEEE, US, vol. 1, May 9, 1995, pp. 181-184, XPO10625 199, ISBN: 978-0-7803-2431-2 sections 1-4, abstract. Brants et al., "Large Language Models in Machine Translation' Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Online), Jun. 28-30, 2007, pp. 858-867, XP002503558 Prague, CZ. Retrieved from the Internet: URL:http://acl.ldc.upenn. edu/D/D07/DO7-109 O.pdf> retrieved on Nov. 12, 2008), the whole document. in Natural Language Processing. Online, Jul. 22-23, 2006, page 216-223, XP002503559 Sydney, Australia Retrieved from the Internet: URL:http://www.aclweb.org/anthology-new W. W06/ W06-1626.pdf> retrieved on Nov. 12, 2008 sections 1-4, abstract. Dean et al., “MapReduce: Simplified Data Processing on Large Clus ters'. OSDI'04: Sixth Symposium on Operating System Design and Implementation, Online Dec. 6-8, 2004, XP002503560, San Fran cisco, CA, USA Retrieved from the Internet: URL:http://labs.google. com/papers/mapreduc e-osd104.pdf> retrieved on Nov. 11, 2008 the whole document. Placeway, et al., “The Estimation of Powerful Language Models From Small and Large Corpora'. Plenary, Special, Audio, Underwa ter Acoustics, VLSI, Neural Networks. Minneapolis, Apr. 27-30, 1993; Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)), NewYork, IEEE, US, vol. 2, Apr. 27, 1993, pp. 33-36, XPO 10110386, ISBN: 978-0-7803 0946-3 pp. 33-36. Goodman, "A bit of progress in language modeling'. Computer Speech and Language, Elsevier, London, GB, vol. 15, No. 4, Oct. 1, 2001, pp. 403-434, XP004418713, ISSN: 0885-2308 sections 1-3 abstract. Xiaoyong, “Estimating Language Models Using Hadoop and Hbase', Master of Science Thesis in Artificial Intelligence. Online 2008, XP002503561 School ofInformatics, University of Edinburgh, Scotland, UK. Retrieved from the Internet: URL:http://www.iccs.inf. ed.ac.uk/imiles/ms c-projects/yu.pdf>, retrieved on Nov. 12, 2008 Sections 1, 2, 3, & 6 abstract. First Office Action issued Aug. 17, 2011 from related Chinese Patent Application No. 200880016830.6, 8 pages. Zhang et al., “Distributed Language Modeling for N-best List Re ranking'. Proceedings of the 2006 Conference on Empirical Methods * cited by examiner U.S. Patent Dec. 11, 2012 Sheet 1 of 4 US 8,332,207 B2 Parallel Corpus Target Corpus Translation Model Language Model Translated 12 U.S. Patent Dec. 11, 2012 Sheet 2 of 4 US 8,332,207 B2 Y Language Model n-grams n-gram probabilities BackOff SCOres Y 204 \-20 U.S. Patent Dec. 11, 2012 Sheet 3 of 4 US 8,332,207 B2 300Y, Receive training data Generate vocabulary 3O4 Generate n-grams Determine relative frequency of the n-grams ldentify backoff O U.S. Patent Dec. 11, 2012 Sheet 4 of 4 US 8,332,207 B2 PROCESSOR(S) N. DATA PROCESSINGAPPARATUS TRANSLATION PROGRAM OPERATING SYSTEM HARDWARE/FIRMWARE PROCESSOR(S) ADDITIONAL DEVICE(S) COMPUTER READABLE MEDIUM 50 COMMUNICATIO N INTERFACE USER INTERFACE DEVICE(S) US 8,332,207 B2 1. LARGE LANGUAGE MODELS IN MACHINE TRANSLATON CROSS-REFERENCE TO RELATED APPLICATIONS This application claims the benefit under 35 U.S.C. S 119 (e) of U.S. Patent Application No. 60/920,283, entitled Large Language Models in Machine Translation, to Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean, which was filed on Mar. 26, 2007. The disclosure of the above application is incorporated herein by reference in its entirety. BACKGROUND This specification relates to statistical machine translation. Manual translation of text by a human operator can be time consuming and costly. One goal of machine translation is to automatically translate text in a source language to corre sponding text in a target language. There are several different approaches to machine translation including example-based machine translation and Statistical machine translation. Sta tistical machine translation attempts to identify a most prob able translation in a target language given a particular input in a source language. For example, when translating a sentence from French to English, statistical machine translation iden tifies the most probable English sentence given the French sentence. This maximum likelihood translation can be written which describes the English sentence, e, out of all possible sentences, that provides the highest value for P(elf). Addi tionally, Bayes Rule provides that: argmax P(ef) P(e) P(fe) aS aS Using Bayes Rule, this most likely sentence can be re-written argmaxP(ef) = argmaxP(e) P(fe). Consequently, the most likely e (i.e., the most likely English translation) is one that maximizes the product of the probability that e occurs and the probability that e would be translated into f (i.e., the probability that a given English sentence would be translated into the French sentence). SUMMARY model. Systems, methods, and computer program products for machine translation are provided. In general, in one aspect, a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an ordern corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score deter mined as a function of a backoff factor and a relative fre quency of a corresponding backoff n-gram in the corpus. Other embodiments of this aspect include corresponding sys tems, apparatus, computer program products, and computer readable media. In general, in one aspect, a method is provided. The method includes generating a language model, including identifying a collection of n-grams from a corpus of training data, each n-gram of the collection having a corresponding relative fre quency of occurring in the corpus and an order n correspond ing to a number of tokens in the n-gram, each n-gram corre sponding to a backoff n-gram having an order of n-1 and identifying one or more backoff factors, where the backoff factors are used to identify a backoff score for one or more n-grams as a function of a relative frequency of a backoff n-gram. Other embodiments of this aspect include methods, apparatus, computer program products, and computer read able media. In general, in another aspect, a method is provided. The method includes receiving an input string having a plurality of tokens, the input string being divided into one or more n-grams, each n-gram having an order, the order identifying a number of tokens in the n-gram and using a language model to identify a probability for each n-gram in the input string. When a first n-gram in the input string is not found in the language model, identifying a backoff n-gram, the backoff n-gram having an order that is one less than the order of the first n-gram, and when the backoff n-gram is found in the language model and identifying a backoff score for the first n-gram, the backoff score being a function of a backoff factor and a relative frequency of the backoffn-gram in the language model. Other embodiments of this aspect include correspond ing systems, apparatus, computer program products, and computer readable media. Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. A backoff for an n-gram can be determined directly as a function of relative frequency with out calculating normalized probabilities for a backoffn-gram. Language models can be generated in a distributed environ ment where n-gram backoff values for particular n-grams can be generated on a same shard. The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the Subject matter will become apparent from the description, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS tion system. Like reference numbers and designations in the various drawings indicate like elements. DETAILED DESCRIPTION tion system 100. Machine translation system includes a target corpus 102, a language model 104, a parallel corpus 106, a US 8,332,207 B2 translation model 108, and a decoder 110. Providing input text 112 to the translation system 100 produces translated text where <start represents the beginning of a sentence Such that P(The |<startd) represents the probability that a sentence begins with “The”. This string probability can be generalized to: 114. The target corpus 102 provides a collection of text in a target language (e.g., English), which is used to train the language model 104. The target corpus 102 can include a number of different Sources of text, including, e.g., web pages and news articles. In some implementations, the target corpus includes text on the order of tens to hundreds of billions of words, or even more. One Such corpus is the Linguistic Data Consortium (“LDC) Web 1T 5-gram Version 1 corpus, LDC Catalog No.: DC2006T13, ISBN: 1-58563-397-6, con tributed by Google Inc. This corpus uses approximately one trillion tokens (including individual words, punctuation, and markers identifying a beginning and end of individual sen tences) of text from publicly accessible Web pages. The language model 104 identifies the probability that a particular string (e.g., a phrase, sentence, or collection of sentences) in the Source language occurs. Thus, for English, the language model 104 identifies the probability that a par ticular string in English occurs. To identify the probability of a particular string occurring, the language model 104 calcu lates the number of times the string occurs in the target corpus divided by the total number of string in the target corpus 102. For example, if the phrase “The red wheelbarrow” occurs 53,000 times in a corpus of 100,000,000 words, the probability equals: P(The red wheelbarrow) = 100,000,000 = 0.00053. 53,000 However, a number of possible strings will have a prob ability of Zero since they are not found within the target corpus 102. Therefore, in some implementations, the prob ability of a particular string is calculated as a function of the probabilities of Sub-string components. One technique for representing Sub-strings is by using n-grams. An n-gram is a sequence of n consecutive tokens. An n-gram has an order, which is the number of tokens in the n-gram. For example, a 1-gram (orunigram) includes one token; a 2-gram (or bigram) includes two tokens. An n-gram language model uses in-gram Sub-strings to calculate the probability o a string. The probability of a given string can be calculated as a product of n-gram conditional probabilities. The conditional probability for a bigram, rep resented P(yx), is the probability that wordy follows wordx. The conditional probabilities are generally determined empirically, according to relative frequencies in the target corpus 102. In the example above, the probability of the word y given X is given by: where f(xy) is a frequency or a count of the occurrences of the string "xy' in the target corpus 102. The probability for the string can be determined as a prod uct of conditional probabilities. For example, to calculate P(The red wheelbarrow) for the sentence beginning with the phrase “The red wheelbarrow” using a bigram language model, the n-gram language model calculates: P(The|<starts) P(red|The) P(wheelbarrowlred), P(e1, ... , et) = Pe. ei-n-1. . . . . e-1) k i=1 where (e., ..., e) represent tokens in the string and n is the order of the largest n-gram allowed in the language model. The parallel corpus 106 includes a collection of text in the Source language (e.g., English) and a corresponding transla tion in one or more target languages (e.g., French). The par allel corpus 106 can include a number of different sources of text, including, e.g., web page and news article pairs where each pair includes text in the Source language and the corre sponding translated text in the target language. In another example, the parallel corpus 106 can include multi-lingual data. For example, United Nations proceedings are available, which provide parallel translations in six languages. The translation model 108 identifies the conditional prob ability of a particular target language string given a particular Source string. Thus, for an English Source language and a French target language, the translation model 108 identifies the probability P(fle) of a French string f given an English string e. Translation models can be generated in a number of different ways. In some implementations, a number of param eters are estimated in order to determine P(fle). For example, a translation model can be defined according to four param eters: t, n, d, and p (e.g., IBM Model 3 described, for example, P. F. Brown, V. J. Della Pietra, S.A. Della Pietra, and R. L. Mercer. The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics 19(2), 1993, which is incorporated by reference). A translation parameter, t, provides a probability of pro ducing a translated word from a source word, e.g., t(bonjour|hello). A fertility parameter, n, provides a probabil ity that source word will produce n target words. For example, n(2hello) represents the probability that the source word “hello” will produce exactly two French words. A distortion parameter, d, provides a probability associated with the posi tion of a target word in a target sentence relative to the posi tion of the corresponding source word in a source sentence. For example, d(35) represents the probability that the English word in position 5 of a sentence (e.g., the fifth word) will provide a French word in position 3 of a translated French sentence (e.g., the third word). Additionally, a parameter p provides a probability of the translation including a spurious word. A spurious word is a word that appears in the target language translation of a source language string that does not correspond to a source word in the source string. The values of the model parameters can be estimated directly if the words in the Source and target sentence pairs are all aligned. The term “alignment” will be used to refer to a data structure that represents a word-for-word connection between source and target words (e.g., that “hello” in one sentence aligns with 'bonjour’) in a pair of sentences. In Some implementations, the alignment is simply a vector iden tifying positions of Source words that various target words connect to. If the alignment is known, the parameter values can be estimated. There can be more than one possible alignment. For example, a sentence pair could have two equally likely align ments. Consequently, a probability can be calculated for a US 8,332,207 B2 particular alignment. The alignment probability defines the likelihood that words are aligned in a particular way. The probability of a particular alignment given a particular sen tence pair can be written P(ale, f), where: steps where the values for the variable calculated in the maxi mization step are used for a next expectation step. The term “EM algorithm' refers to a class of related algorithms: the expectation and maximization steps provide a roadmap for developing specific EM algorithms. In some implementa tions, other techniques are used to find maximum likelihood estimates other than the EM algorithm, for example, gradient descent or conjugate gradient techniques. Using a technique Such as an EM algorithm, the translation model 108 is trained to determine a most likely parameter values and alignment probability for a give source and target language. The decoder 110 applies the language model 104 and the translation model 108 to a given String (e.g., from input text 112) in order to produce a translated String (e.g., as translated text 114). In particular, the decoder 110 translates an observed sentence, f. (e.g., a French sentence) by seeking the sentence, e, (e.g., an English sentence) that maximizes the product of P(e) determined by the language model 104 and P(fle) deter mined by the translation model 108. 200. The language model 200 includes n-grams 202, n-gram probabilities 204, and backoff scores 206. The n-grams 202 are derived from training data, for example, from target cor pus 102. The n-grams 202 include n-grams of each order for the particular language model 200. For example, for a 5-gram language model, the n-grams 202 include 5-grams, 4-grams, 3-grams, 2-grams, and 1-grams derived from the training data. In some implementations, the n-gram represents a string of n consecutive tokens, where a token can include punctua tion and other information, e.g., an identifier of a start of a Sentence. The n-gram probabilities 204 are probability estimates cal culated as a function of n-gram relative frequency in the training data. For example, a string of L tokens can be repre sented as w-(w, w, . . . , w). A probability can be assigned to the string was: tion. where the approximation is based on a Markov assumption that only the most recent (n-1) tokens are relevant when predicting a next word in the string, and the “” notation for P indicates that it is an approximation of the probability func For any substring (e.g., aparticular n-gram), denoted w;', of the sting w,", a frequency, f(w; ), can be calculated. The frequency of the Substring denotes the frequency of occur rence of that Substring in the training data, which can be considered a long fixed sample string. The maximum-likeli hood probability estimates for the n-grams are given by their relative frequencies. The relative frequencies, r, for each n-gram can be determined as: r(w; will) The relative frequency of an n-gram is the frequency of the n-gram divided by the frequency of the (n-1) gram. The probability estimates for each n-gram in the training data can be calculated and stored for each n-gram as n-gram probabili ties 204 in the language model 200. P(a, fe) P(ae, f) = Pfe) where, Pfle) =XP(a, fe), and where the P(a, fle) is Summed over all alignments a and represents the joint probability of an alignment and a source sentence, given a target sentence. Alignment, C., is repre sented by a single letter; however, it represents a matrix whose dimension can vary. Specifically, C. is a matrix random variable, a specific value of which refers to a matrix of asso ciations (e.g., links) between a specific source sentence and a specific paired target sentence. Columns correspond to Source word position and rows to target word positions. An addi tional row and column may be labeled with the null word, in cases where there is no correspondence. The elements in the matrix are Zeroes and ones, indicating the presence or absence of an association between the corresponding source and tar get sentence words. Depending on the alignment model used, constraints may be imposed on where in the matrix links can occur (e.g., whether a source word can map to multiple target words, whether words can map to a null word, etc.) Therefore, P(ale, f) can be described in terms of P(a, fe). However, P(a, fle) can also be described as a function of the parameter values. Thus, if the parameter values are known, the align ment probabilities can be directly calculated. A particular alignment C. a refers to a specific alignment between a specific (f, e) pair, while P(ale, f) is the posterior probability over possible alignments, again for the specific (f. e) pair. P(ale, f) is described by parameters which can be estimated by Some training procedure that iteratively learns the parameters by looping over a large number of (f, e) sen tence pairs, using the current parameter values to achieve a better word alignment between each pair than was achieved in the previous iteration, then using that alignment to update the parameter values, then repeating. Additional details on align ment can be found, for example, in Franz Joseph Och and Hermann Ney, A Systematic Comparison of Various Statisti cal Alignment Models, Computational Linguistics, 29(1): 9-5 1, Mar. 2003, which is incorporated by reference. Consequently, to calculate P(fle), the translation system calculates P(a, fle). However, to calculate P (a, fle), the parameter values are needed. Additionally, to get the param eter values, the system needs P(ale, f), but to do that P(a, fle) is again needed. Thus, a technique is used to solve for both parameter values and alignment probabilities Substantially simultaneously. An expectation-maximization (“EM) algorithm can be used to estimate parameter values and alignment probabilities using an iterative process until local optimum values are determined. An EM algorithm calculates maximum likeli hood estimates of variables in probabilistic models. An EM algorithm is a two-step process. An expectation step calcu lates an expectation of the likelihood by including the vari able values as if they were observed. A maximization step calculates the maximum likelihood estimates by maximizing the expected likelihood calculated in the expectation step. The process iterates between expectation and maximization US 8,332,207 B2 The relative frequency can be zero or undefined for par ticular n-grams if they are not found in the training data. As a result, the corresponding probability estimates can be inac curate or undefined. This is referred to as a sparse data prob lem. Additionally, the use of higher order n-grams, while potentially increasing language model accuracy, exacerbates the sparse data problem. As a result, Smoothing techniques are used to account for missing n-grams. The smoothing can be provided by backoff scores 206. The back-off scores 206 can be used to ensure that there is always a non-Zero probability for a given n-gram. Generally, the backoff considers whether some part of the n-gram occurs. For example, if the trigram "xyz' is not found, Smoothing is performed to identify whether a backoffn-gram “yz’ occurs. Ifyz' does not occur, the smoothing can recursively identify whether “Z” occurs. In some implementations, the backoff scores 206 are cal culated directly as a function of the relative frequencies of the n-grams. Consequently, the probability for a given n-gram, P(w, w, ...') can be represented as the relative frequency of the n-gram when the n-gram exists. Thus, when the n-gram frequency is greater than Zero, f(w")>0, the probability estimate is simply the relative frequency: (e.g., the target corpus 102). In some implementations, an n-gram language model is generated using the identified training data. Additionally, in some implementations, a distributed train ing environment is used for large training data (e.g., terabytes of data). One example technique for distributed training is MapReduce. The term MapReduce describes both a program ming model and an implementation of the model for process ing and generating large data sets. The model and its library implementation will both be referred to as MapReduce. Using MapReduce, programmers specify a map function that pro cesses input (key, value) pairs to generate a set of intermediate (key, value) pairs, and a reduce function that merges all inter mediate values associated with the same intermediate key. Programs written in this functional style can automatically be parallelized and executed on a large cluster of commodity computers. The runtime system or framework can be imple mented to partition the input data, Schedule the programs execution across a set of machines, handle machine failures, and manage the required inter-machine communication. A MapReduce computation takes a set of input (key, value) pairs, and produces a set of output (key, value) pairs. The user expresses the computation as two functions: Map and Reduce. f(wiki) f(w: ) However, when the n-gram frequency is not greater than Zero, a backoff score is calculated as: CS(w, w, .. 2), where C. is a backoff factor and S is used to indicate that the calculations are scores not probabilities, since they are directly calculated using the relative frequencies of the n-grams. Essentially, the backoff factor applies a penalty to the relative frequency of the backoffn-gram (e.g., to compen sate for the n-gram not being present). The backoff factor C. can, in some implementations, depend on k (i.e., the n-gram order). Examples of techniques for determining variable C. values are described below. Alternatively, a single value (e.g., C-0.4) can be specified regardless of n-gram order. The calculations are recursive for backoffn-grams that are not found until the backoff has reached a unigram. At the unigram level, the score becomes: S(w) = ty. where N is the size of the training data. In some alternative implementations, the backoff scores include one or more C. values for particular n-gram orders. The specific scores for a particular n-gram are then calculated at run-time in response to an input in-gram using the C. value for the n-gram order and the relative frequency of an identified backoff n-gram. Thus, the individual backoff scores are not stored in the language model 200, but instead are generated as needed. model. For convenience, the method 300 will be described with respect to a system that performs the method 300. The system receives 302 training data. The training data can be, for example, part or all of the text in a target corpus Map, written, e.g., by a programmer, takes an input (key, value) pair and produces a set of intermediate (key, value) pairs. The MapReduce library groups together all intermedi ate values associated with the same intermediate key I and passes them to the Reduce function. The Reduce function, also written, e.g., by a programmer, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just Zero or one output value is produced for each Reduce invocation. The intermediate values are Sup plied to the user's reduce function through an iterator. In this way lists of values that are too large to fit in memory can be handled. Additional details of MapReduce are described in J. Dean and S. Ghemawat, MapReduce. Simplified Data Pro cessing on Large Clusters, Proceedings of the 6th Sympo sium on Operating Systems Design and Implementation, pp. 150 (Dec. 6, 2004), the contents of which are incorpo rated here by reference. Typically, several map tasks operate independently on dif ferent processors (e.g., on different computing devices) and on different portions of input data. Similarly, several reduce tasks operate independently on a portion of the intermediate data. The portions of the intermediate data are partitioned according to the intermediate keys so that the same reducer operates on all values for a given intermediate key. The system generates 304 a vocabulary from the training data. The vocabulary identifies the individual tokens (e.g., words, punctuation) within the training data. MapReduce can be applied to the training data to generate the Vocabulary along with a frequency count associated with each token in the vocabulary. The Map phase takes an input (key, value) pair and pro duces an intermediate (key, value) pair. In particular, the input (key, value) pairs to the map phase correspond to a key iden tifying the node and a value that is the text in the node. The intermediate (key, value) pair has a key value corresponding to individual words and a corresponding value for the word. The Reduce phase reduces the intermediate (key, value) pairs having the same key into a single output (key, value). Specifi cally, the reduce outputs a vocabulary for the words in the training data and their frequencies, (word, frequency). For example, for three shards S1, S2, and S3, the vocabu lary and word frequencies can be generated. Shard S1 US 8,332,207 B2 includes training data text “a rose'. Shard S2 includes the text “is a rose'. Shard S3 includes the text “a rose is a rose'. During the Map phase, the input for each shard is key=ID and value-sentence. The map output is: key—word and value=1. In particular, the map from S1 produces intermediate (key, The system generates 306 n-grams using the Vocabulary. The vocabulary allows the system to determine every possible combination of Vocabulary token up to a maximum n-gram order. The system can be implemented using MapReduce to generate the n-grams and their frequencies within the training The map from S2 is output as intermediate (key, value) Finally, the map from S3 produces intermediate (key, value) pairs: (a, 1) (rose, 1) (</S>, 1). pairs: (is, 1) (a, 1) (rose, 1) (</S>, 1). value) pairs: (a, 1) (rose, 1) (is, 1) (a, 1) (rose, 1) (</S>, 1). (a, 2) (rose, 2) (is, 1) (</s), 1). (a, 4) (is, 2) (rose, 4) (</s), 3). data. pairs. The Map phase can process the training data shards in view of the Vocabulary. The Reduce phase combines in-grams such that the output (key, value) pairs are (n-gram, frequency) For example, three example shards are used to illustrate the n-gram frequency for 2-grams using MapReduce. The example shards are S4, S5, and S6. Shard S4 includes the text “a rose foo’. Shard S5 includes the text “is a rose bar” and the shard S6 includes the text “a rose is a rose'. During the Map phase, a map function is applied for the input (key, value) for the node as a function of the Vocabulary. The vocabulary in this example is (<sd, </s, <UNK>, a, is, rose), where <UNK> provides a generic placeholder repre senting rare words in the training data e.g., “foo’. The inter mediate (key, value) pairs produced in the map phase have keys equal to 2-grams and a value for the 2-gram. Thus, for S4, the intermediate (key, value) pairs are: (<sca, 1) (arose, 1) (rose <UNK>, 1) (<UNK></s>, 1). (<ssis, 1) (is a, 1) (arose, 1) (rose <UNK>, 1) (<UNK></s>, 1). (<sca, 1) (arose, 1) (rose is, 1) (is a, 1) (arose, 1) (rose </s), 1). Finally, the intermediate (key, value) pairs for S6 are: During the Reduce phase, the intermediate (key, value) pairs are combined to produce output (key, value) pairs where the keys are the distinct 2-grams and the value is the count for each n-gram: key=2-gram, value-count. The resulting output (key, value) pairs in the example are: In some implementations, intermediate (key, value) pairs can be optimized before reduce phase by simplifying redun dant entries. For example, the intermediate (key, value) pairs from S3 can be optimized to: For S5, the intermediate (key, value) pairs are: During the Reduce phase, the intermediate (key, value) pairs are combined to produce output (key, value) pairs where the key-word and value-count. The resulting output (key, value) pairs in the example are: The results of the Reduce phase provide a vocabulary for the text in the training data as well as the word frequencies. In Some implementations, a particular reduce shard is identified for intermediate (key, values) of a particular shard using, for example, a hash function. The map and reduce functions for generating the Vocabu lary can be expressed as follows: (<sca, 2) (<ssis, 1) (a rose, 3) (is a, 2) (rose is, 1) (rose <UNK>, 2) (rose </s), 1) Map (string key, string value) { i?key=docid, ignored; value= document array words = Tokenize(value); for i=1...#words Emit (wordsii), “1”); int Shard ForKey (string key, intinshards) { return Hash(key) % inshards: Thus, the results of the Reduce phase provide 2-grams along with their frequencies in the corpus. The map function for generating the n-grams can be expressed as follows: Reduce (string key, iterator values) { if key=term; values= counts int sum = 0; for each v in values Sum += Parsent (v); Emit (AsString(Sum)); Map (string key, string value) { if key=docid, ignored; value=document array ids = Tolds (Tokenize (value)); for i = 1... iiids for j = 0 ... maxorder-1 Emit (idsi-. i.), “1”); US 8,332,207 B2 The reduce function can be expressed in the same manner as the reduce function for the Vocabulary generation. The system determines 308 relative frequencies for the n-grams. Recall, the relative frequency was defined above as: r(w; I will) = Tit. +1 foil) The relative frequency of n-grams in the corpus can also be identified using MapReduce. For example, n-grams can be divided into a number of shards. The input of n-grams and frequencies can be processed in the Map and Reduce phases to produce relative frequencies for the n-grams in a similar manner as describe above. In particular, a sharding function can be used that places the values need for the numerator and denominator to calculate the relative frequency on the same shard. In particular, a hash function can be applied to the first words of the n-grams. As a result, the required n-grams wi' and w, ... ', will share the same first word, W, and be placed on the same shard (with the exception of all unigrams). However, in some implementations, sharding using the first word only can result in imbalanced shards. For example, Some terms can be found at the beginning of a disproportion ate number of n-grams (e.g., Stopwords, some punctuation marks, or the beginning of sentence marker). The shards can be more evenly balanced by hashing based on the first two words of the n-grams, for example: Int Shard ForKey (string key, intinshards) { String prefix = FirstTwoWords (key); Return Hash(prefix) %nshards; Additionally, the unigram counts are reproduced on each shard in order to calculate relative frequencies within indi vidual shards. However, the data amount is Small in compari son to the total number of n-grams. The system determines 310 backoff scores. The individual shards including all information necessary to calculate the relative frequencies for the n-grams in the shard. Backoff scores are calculated when a full n-gram is not found. Thus, if the relative frequency of the full n-gram is not found, r(w, w"), the system recursively looks for the backoff rela tive frequencies, e.g., r(w, w, ..."), r(w, wis'), etc. until the unigram backoff is reached. Since the n-grams where sharded on their last two words and all unigrams duplicated on each shard, all backoff calculations can be performed within the same shard. In some implementations, the backoff scores are stored on each shard for the respective backoff n-grams of the shard. Alternatively, the system stores values for the backoff factor C., which are used by individual shards to calculate the par ticular backoff scores at run-time in response to an input n-gram (e.g., from an input string to be translated). To Summarize, a language model can be generated from a corpus in the following steps. The system generates a Vocabu lary. Using input text from the corpus, the system outputs a Vocabulary of 1-grams and their frequencies. Next, n-grams and their frequencies are identified. The n-grams and their frequencies are identified using the text and Vocabulary as input. Relative frequencies for the n-grams are calculated using the n-grams and their respective frequencies. Each step in the process can be implemented using MapReduce as described above. The backoff scores are then determined directly as a function of the relative frequencies of the n-grams. As stated above, there can be multiple values for the back off factor C. depending on n-gram order. For example, if for particular sample data 4-grams are often used, then the pen alty (i.e., C. value) for backing off from a 5-gram to a 4-gram should be small. Conversely, if 4-grams are often used within the sample data, then the penalty for backing off from a 4-gram to a 3-gram should be large. For particular sample data, referred to as heldout data, a count can be obtained as to how many of the n-grams at each order also occur in the language model. One technique for determining multiple backoff factors C. in a n-gram language model includes determining the backoff factor as propor tional to the counts at each order. For example, for a particular heldout data set, examine all k-grams up to Some maximum order n, i.e., for k=1,..., n. The coverage of n-grams at order k is calculated as the number of n-gram occurrences of order k in the heldout data that also occur in the language model divided by the total number of n-gram occurrences of orderk in the heldout data, i.e., C. L/N, where L is less than or equal to N. Additionally, C is less than or equal to the coverage at the next lower n-gram order, i.e., C is less than or equal to C. Given the calculated value of C, the backoff factor values at each order, C, are determined as: a k = 1 - C if k = n (highest order) and Ci (1 - C) Ok+1 if k = 1,..., (n - 1). Consequently, the C values are proportional to the counts at each order. For example, for a set of holdout data and a maximum order of 5, the following backoff factors can be calculated: Cs—0.7012, O=0.6090, C.-0.2547, C-0.1052, and C. -0.0399. The notation Cls denotes the backoff factor when backing off from a 5-gram to a 4-gram. In some imple mentations, there is no backoff from 1-grams to 0-grams, therefore that C. value is not used. Another technique slightly adjusts the formulas for calcu lating the backoff factors as: a = 1 - C if k = n (highest order) and (1 - C) (1 - C+1) Ok : - if k = 1,..., (n - 1). ( ) The above adjustment results in a slight increase in C. values for lower order backoffs. For example, using the same holdout data, the backoff factors are calculated as: Cis-0.7012, C-0.6090, C-0.3632, and C--O.1728. In another technique, the formulas for calculating backoff factors provide higher backoff values for higher order back offs: a k = (C-1 - C)f C if k = n (highest order), a = C 1 - C a (C - C+1) (1 - C) C. : - - - - k = C, C, if k = 2, ... , (n - 1), and if k = 1. US 8,332,207 B2 The above formulas adjust the sample C values above as follows: Os–0.9177, C. 0.9916, C-0.4719, C-0.1761, and C=0.1858. As above, C. can be ignored for a given language tions. Zero otherwise), a step function (e.g., 1 when the language model has backed off at least k times), or other parameteriza model. In some implementations, cap values can be included. The 5 cap values can be introduced to prevent the backoff values from being too large. Therefore, the backoff factors can be capped at Some C. Such that the backoff value is equal to the minimum of the calculated C. value or C. For example, in the above sample C. values, when an O-0.95 is used, the backoff factor C. is reduced to the cap value of 0.95 from the calculated value of 0.9916. In one other technique for calculating backoff factors, a term M is introduced. M is the sum of the probabilities at order k that are used in the heldout data set. The backoff factors are then calculated such that the total mass of the backed-off distribution at order k is proportional to the prob ability mass of the heldout set at order k: CFM/M if k=2... in and Again for the same sample set of holdout data, the above formulas provide the following example backoff values: C=0.5396, C-0.5138, C-0.1111, C-0.0248, and C=0.0. As with the previous example, a maximum backoff value, C. can be used to cap the backoff values. In some implementations, discriminative training can be used to determine the values for one or more backoff factors. Machine translation systems typically use discriminative training techniques to optimize free parameters in order to minimize an automated measure of translation quality, e.g., a Bilingual Evaluation Understudy (“BLEU’) score, on some sample data for which a set of reference translations are available. Discriminative training does not attempt to opti mize numeric scores directly. Instead, a set of backoff factor values are identified that produce a better translation score than other backoff factor values. BLEU scores are described, for example, in Papineni, K. Roukos, S. Ward, T., and Zhu, W. J. "BLEU: a method for automatic evaluation of machine translation” in ACL-2002: 40th Annual meeting of the Asso ciation for Computational Linguistics pp. 311-318 (2002), which is incorporated by reference. Specifically, to discriminatively train the backoff factors, a collection of sentences for which translations are available is translated using different values for the backoff factors. The backoff factors that result in a high value of BLEU (or similar) 50 score are used. One example method of discriminatively training features in statistical machine translation is described in Franz Josef Och, Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160-167 (2003), which is incorporated by reference. The above method can be used to train the backoff factors. The general method takes described in the above reference can be applied to determining backoff factors, by taking as addi tional “feature functions”, the degree of actual backoff 60 encountered when applying the language model at each word position during the translations. The resulting feature func tion weights are then converted to backoff factors using a log-linear relationship (e.g., a log-linear formulation of a decoding search objective function). The feature functions can be indicator functions for a specific backoff level (e.g., 1 when the language model has backed off exactly k times, and This specification has been described with reference to machine translations systems. However, the language models and techniques for generating and training language models disclosed can be implemented in other systems using lan guage models including, for example, speech recognition, optical character recognition, character recognition, etc. data processing apparatus 410 can include hardware/firm ware, an operating system and one or more programs, includ ing translation program 420. The translation program 420 operates, in conjunction with the data processing apparatus 410, to effect the operations described in this specification. Thus, the translation program 420, in combination with one or more processors and computer-readable media (e.g., memory), represents one or more structural components in 2O the system 400. The translation program 420 can be a translation process ing application, or a portion. As used here, an application is a computer program that the user perceives as a distinct com puter tool used for a defined purpose. An application can be built entirely into the operating system (OS) of the data pro cessing apparatus 410, or an application can have different components located in different locations (e.g., one portion in the OS or kernel mode, one portion in the user mode, and one portion in a remote server), and an application can be built on a runtime library serving as a Software platform of the appa ratus 410. Moreover, application processing can be distrib uted over a network 480 using one or more processors 490. For example, a language model of the translation program can be distributively trained over the one or more proces Sors 490. The data processing apparatus 410 includes one or more processors 430 and at least one computer-readable medium (e.g., random access memory, storage device, etc.). The data processing apparatus 410 can also include a communi cation interface 450, one or more user interface devices 460, and one or more additional devices 470. The user interface devices 460 can include display Screens, keyboards, mouse, stylus, or any combination thereof. Once programmed, the data processing apparatus 410 is operable to identify backoff factors as a function of relative n-gram frequency. Additionally, the language model can be generated such that the backoff factors can be derived from a single shard. Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer Software, firm ware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combina tions of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a tan gible program carrier for execution by, or to control the opera tion of data processing apparatus. The tangible program car rier can be a propagated signal or a computer-readable medium. The propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or elec tromagnetic signal, that is generated to encode information for transmission to Suitable receiver apparatus for execution by a computer. The computer-readable medium can be a machine-readable storage device, a machine-readable Stor age Substrate, a memory device, a composition of matter US 8,332,207 B2 effecting a machine-readable propagated signal, or a combi nation of one or more of them. The term “data processing apparatus' encompasses all apparatus, devices, and machines for processing data, includ ing by way of example a programmable processor, a com puter, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execu tion environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, oracom bination of one or more of them. A computer program (also known as a program, Software, Software application, Script, or code) can be written in any form of programming language, including compiled or inter preted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, Subroutine, or other unit Suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, Sub-programs, or por tions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network. The processes and logic flows described in this specifica tion can be performed by one or more programmable proces sors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer pro gram include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a per Sonal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few. Computer-readable media Suitable for storing computer program instructions and data include all forms of non-vola tile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be Supplemented by, or incorporated in, special purpose logic circuitry. To provide for interaction with a user, embodiments of the Subject matter described in this specification can be imple mented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., amouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Embodiments of the subject matter described in this speci fication can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client com puter having a graphical user interface or a Web browser through which a user can interact with an implementation of the Subject matter described is this specification, or any com bination of one or more such back-end, middleware, or front end components. The components of the system can be inter connected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN), e.g., the Inter net. The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descrip tions of features that may be specific to particular embodi ments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as Such, one or more features from a claimed combination can in Some cases be excised from the combi nation, and the claimed combination may be directed to a Subcombination or variation of a Subcombination. Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circum stances, multitasking and parallel processing may be advan tageous. Moreover, the separation of various system compo nents in the embodiments described above should not be understood as requiring Such separation in all embodiments, and it should be understood that the described program com ponents and systems can generally be integrated together in a single software product or packaged into multiple Software products. Particular embodiments of the subject matter described in this specification have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not nec essarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementa tions, multitasking and parallel processing may be advanta geous. US 8,332,207 B2 k S. = f' Ci What is claimed is: 1. A language model system comprising: one or more computers; and one or more computer-readable storage devices, coupled to the one or more computers, on which are stored: a collection of n-grams, and for each n-gram, a corre sponding relative frequency, wherein each n-gram is obtained from a corpus and each relative frequency is the relative frequency of the n-gram in the corpus, each n-gram being a sequence of tokens of length k, wherein k is an integer between 1 and a maximum length N, inclusive; and a set of backoff factors C, one backoff factor for each value of k from 2 to N, inclusive, wherein every backoff factor is different from every other backoff factor and every backoff factor is a COnStant, wherein for each value of k from 2 to N, the backoff factors define how to compute a score S for any n-gram of length k that is not present in the lan guage model, wherein the n-gram of length k that is not present in the language model has a longest backoff n-gram that is present in the language model, and the score S is defined in accordance with k S. = f' Ci wherein f is the frequency of the longest backoff n-gram present in the language model and m is the length of the longest backoff n-gram present in the language model. 2. The system of claim 1, where the backoff factor at each order comprises a constant penalty value. 3. The system of claim 1, wherein each backoff factor C is based on: a coverage C for each value ofk, wherein C for each value ofkis based on a count Lof n-grams of length k present in the language model that are also presentina collection of heldout data and a total count N of n-grams of length k in the heldout data. 4. A method comprising: generating, at one or more data processing apparatuses, a language model, including: identifying a collection of n-grams from a corpus of training data, and determining for each n-gram of the collection a corresponding relative frequency of occurrence in the corpus, each n-gram being a sequence of tokens of length k, wherein k is an integer between 1 and a maximum length N; and determining a set of backoff factors C, one backoff factor for each value ofk from 2 to N, wherein every backoff factor is different from every other backoff factor and every backoff factor is a COnStant, wherein for each value of k from 2 to N, the backoff factors define how to compute a score S for any n-gram of length k that is not present in the lan guage model, wherein the n-gram of length k that is not present in the language model has a longest backoff n-gram that is present in the language model, and the score S is in accordance with wherein f is the frequency of the longest backoff n-gram present in the language model and m is the length of the longest backoff n-gram present in the language model. 5. The method of claim 4, wherein determining the set of backoff factors comprises: performing discriminative training on a set of sample data, the discriminative training identifying values for the backoff factors that maximize a measure of translation quality. 6. The method of claim 5, where indicator feature functions are used to identify the backoff factors. 7. The method of claim 4, wherein identifying the backoff factors comprises: calculating a coverage C for each value of k, wherein C. for each value of k is based on a count L of n-grams of length k present in the language model that are also present in a collection of heldout data and a total count N of n-grams of length k in the heldout data. 8. The method of claim 4, further comprising calculating backoff scores for n-grams using the backoff factors and storing the backoff scores. 9. The method of claim 7, wherein the backoff factor C for each value of k is based on: 1-C, if k is equal to a maximum n-gram length N; and if k is less than N. 10. The method of claim 7, wherein the backoff factor at each order k is based on: (1-C), if k is equal to a maximum n-gram length N; and if k is less than N. 11. The method of claim 7, wherein the backoff factor at each order k is based on: (C-C)/C, if k is greater than or equal to a maximum n-gram length N; and (1 - C) Ok+1 (1 - C) (1 - C-1) (C 1 - C) (C. - C-1) if k is greater than or equal to 2 and less than N. 12. The method of claim 9, wherein each backoff factor C. does not exceed a maximum value. 13. The method of claim 4, further comprising: for a collection of heldout data, determining a respective Sum M of probabilities of n-grams present in the lan guage model that also occur in heldout data for each value ofk; and calculating each backoff factor C as: US 8,332,207 B2 19. A method comprising: receiving an input string, the input string being divided into one or more n-grams, each n-gram having a length k, whereink is an integer between 1 and a maximum length N, inclusive; calculating a score for each n-gram present in a language model, the score being based on a relative frequency of the n-gram in the language model; obtaining for each n-gram not present in the language model a respective longest backoff n-gram of length m present in the language model and a corresponding rela tive frequency f for each backoff n-gram present in the language model; obtaining a set of backoff factors C, one backoff factor for each value of k from 2 to N, inclusive, wherein every backoff factor is different from every other backoff fac tor and every backoff factor is a constant; and calculating, using one or more computing devices, a score S for each n-gram not present in the language model by: 20. The method of claim 19, where identifying a backoff score includes looking up a calculated backoff score for the 21. The method of claim 19, where identifying a backoff score includes calculating the backoff score using the backoff factor for the n-gram and the relative frequency of the backoff 22. A computer program product, encoded on a computer readable storage device, operable to cause data processing apparatus to perform operations comprising: receiving an input string, the input string being divided into one or more n-grams, each n-gram having a length k, whereink is an integer between 1 and a maximum length N, inclusive; and calculating a score for each n-gram present in a language model, the score being based on a relative frequency of the n-gram in the language model; obtaining for each n-gram not present in the language model a respective longest backoff n-gram of length m present in the language model and a corresponding rela tive frequency f for each backoff n-gram present in the language model; obtaining a set of backoff factors C, one backoff factor for each value of k from 2 to N, inclusive, wherein every backoff factor is different from every other backoff fac tor and every backoff factor is a constant; and calculating, using one or more computing devices, a score S for each n-gram not present in the language model by: 23. The computer program product of claim 22, wherein calculating a score includes looking up a calculated backoff score for the n-gram. if k is greater than or equal to 2. 14. A computer program product, encoded on a computer readable storage device, operable to cause data processing apparatus to perform operations comprising: generating a language model, including: identifying a collection of n-grams from a corpus of training data, and determining for each n-gram of the collection a corresponding relative frequency of occurrence in the corpus, each n-gram being a sequence of tokens of length k, wherein k is an integer between 1 and a maximum length N, inclusive; and determining a set of backoff factors C, one backoff factor for each value ofk from 2 to N, wherein every backoff factor is different from every other backoff factor and every backoff factor is a COnStant, wherein for each value of k from 2 to N, the backoff factors define how to compute a score S for any n-gram of length k that is not present in the lan guage model, wherein the n-gram of length k that is not present in the language model has a longest backoff n-gram that is present in the language model, and the score S is in accordance with n-gram. n-gram. k S. = f' Ci wherein f is the frequency of the longest backoff n-gram present in the language model and m is the length of the longest backoff n-gram present in the language model. 15. The computer program product of claim 14, wherein determining the backoff factors comprises: performing discriminative training on a set of sample data, the discriminative training identifying values for the backoff factors that maximize a measure of translation quality. factors. 16. The computer program product of claim 15, where indicator feature functions are used to identify the backoff 17. The computer program product of claim 14, wherein identifying the backoff factors comprises: calculating a coverage C for each value of k, wherein C. for each value of k is based on a count L of n-grams of length k present in the language model that are also present in a collection of heldout data and a total count N of n-grams of length k in the heldout data. 18. The computer program product of claim 14, wherein the operations further comprise calculating backoff scores for n-grams using the backoff factors and storing the backoff SCOS. UNITED STATES PATENT AND TRADEMARK OFFICE CERTIFICATE OF CORRECTION Page 1 of 1 It is certified that error appears in the above-identified patent and that said Letters Patent is hereby corrected as shown below: PATENT NO. APPLICATIONNO. : 8,332,207 B2 : 1 1/767436 DATED : December 11, 2012 INVENTOR(S) : Brants et al. On the Title Page: The first or Sole Notice should read -- Subject to any disclaimer, the term of this patent is extended or adjusted under 35 U.S.C. 154(b) by 1110 days. Signed and Sealed this Nineteenth Day of May, 2015 74-4-04- 2% 4 Michelle K. Lee Director of the United States Patent and Trademark Office