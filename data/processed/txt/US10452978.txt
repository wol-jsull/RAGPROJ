US010452978B2 ( 12 ) United States Patent Shazeer et al . ( 10 ) Patent No . : US 10 , 452 , 978 B2 ( 45 ) Date of Patent : Oct . 22 , 2019 ( 54 ) ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS ) U . S . Ci . ( 71 ) Applicant : Google LLC , Mountain View , CA ( US ) ( 58 ) Field of Classification Search CPC . . . . . . . . . . . . . . . . . GOON 3 / 08 ( 2013 . 01 ) ; G06N 3 / 04 ( 2013 . 01 ) ; G06N 3 / 0454 ( 2013 . 01 ) CPC USPC . . . . . . . . . . . . . . . . . . . . . . . . . GOOF 3 / 015 . . . . . . 706 / 15 , 45 See application file for complete search history . ( 72 ) Inventors : Noam M . Shazeer , Palo Alto , CA ( US ) ; Aidan Nicholas Gomez , Toronto ( CA ) ; Lukasz Mieczyslaw Kaiser , Mountain View , CA ( US ) ; Jakob D . Uszkoreit , Portola Valley , CA ( US ) ; Llion Owen Jones , San Francisco , CA ( US ) ; Niki J . Parmar , Sunnyvale , CA ( US ) ; Illia Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U . S . C . 154 ( b ) by 0 days . ( 21 ) Appl . No . : 16 / 021 , 971 ( 22 ) Filed : Jun . 28 , 2018 ( 65 ) Prior Publication Data US 2018 / 0341860 A1 Nov . 29 , 2018 Related U . S . Application Data ( 63 ) Continuation of application PCT / US2018 / 034224 , filed on May 23 , 2018 . No . ( 60 ) Provisional application No . 62 / 541 , 594 , filed on Aug . , 2017 , provisional application No . 62 / 510 , 256 , filed on May 23 , 2017 . ( 51 ) Int . Cl . G06F 15 / 18 GO6N 3 / 08 GOON 3 / 04 ( 2006 . 01 ) ( 2006 . 01 ) ( 2006 . 01 ) ( 56 ) References Cited PUBLICATIONS Luong et al . , Effective Approaches to Attention - based Neural Machine Translation , ( 2015 ) Conf . on Empirical Methods in Natural Lan guage Processing at p . 1412 - 1421 ( Year : 2015 ) . * ( Continued ) Primary Examiner - David R Vincent ( 74 ) Attorney , Agent , or Firm — Fish & Richardson P . C . ( 57 ) ABSTRACT Methods , systems , and apparatus , including computer pro grams encoded on a computer storage medium , for gener ating an output sequence from an input sequence . In one aspect , one of the systems includes an encoder neural network configured to receive the input sequence and gen erate encoded representations of the network inputs , the encoder neural network comprising a sequence of one or more encoder subnetworks , each encoder subnetwork con figured to receive a respective encoder subnetwork input for each of the input positions and to generate a respective subnetwork output for each of the input positions , and each encoder subnetwork comprising : an encoder self - attention sub - layer that is configured to receive the subnetwork input for each of the input positions and , for each particular input position in the input order : apply an attention mechanism over the encoder subnetwork inputs using one or more queries derived from the encoder subnetwork input at the particular input position . Claims , 3 Drawing Sheets Neural Network System Attention - Based Neural Network Output Sequence 1 er 176 S160 Input Sequence US 10 , 452 , 978 B2 Page 2 ( 56 ) References Cited PUBLICATIONS Cheng et al . , Long Short - Term Memory - Networks for Machine Reading ( 2016 ) , Conf . on Empirical Methods in Natural Language Processing , available from Internet < https : / / arxiv . org / abs / 1601 . > at p . 551 - 561 ( Year : 2016 ) . * Lin et al . , A Structured Self - Attentive Sentence Embedding ( Mar . ) available from Internet < https : / / arxiv . org / abs / 1703 . 03130 > , ICLR 2017 at p . 1 - 15 ( Year : 2017 ) . * Sukhbaatar et al . , End - to - End Memory Networks , ( 2015 ) available from Internet < https : / / arxiv . org / pdf / 1503 . 08895 . pdf > at p . 1 - 11 ( Year : 2015 ) . * Internet Ba et al . , Layer Normalization , ( 2016 ) available from https : / / arxiv . org / abs / 1607 . 06450 at p . 1 - 14 ( Year : 2016 ) . * Daniluk et al . , Frustratingly Short Attention Spans in Neural Lan guage Modeling , ( Feb . 2017 ) ICLR 2017 , available from Internet < https : / / arxiv . org / abs / 1702 . 04521 > at p . 1 - 10 ( Year : 2017 ) . * PCT International Search Report and Written Opinion issued in International Application No . PCT / US2018 / 034224 , dated Sep . 24 , , 14 pages . , 15 pages . , 14 pages . pages . Vaswan et al . “ Attention is All You Need , ” 31st Conference on Neural Information Processing Systems , Jun . 12 , 2017 , arXiv1706 . Ba et al . “ Layer Normalization , ” arXiv 1607 . 06450v1 , Jul . 21 , Bahdanau et al . “ Neural Machine Translation by Jointly Learning to Align and Translate , " arXiv 1409 . 0473v7 , mailed on May 19 , 2016 , Britz et al “ Massive exploration of neural machine translation architectures , " arXiv 1703 . 03906v2 , Mar . 21 , 2017 , 9 pages . Cheng et al . “ Long short - term memory - networks for machine reading , " arXiv 1601 . 06733v7 , Sep . 20 , 2016 , 11 pages . Cho et al . “ Learning phrase representations using rnn encoder decoder for statistical machine translation , ” arXiv 1406 . 1078v3 , Sep . 3 , 2014 , 15 pages . Chollet . “ Xception : Deep Learning with depthwise separable con volution , ” arXiv 1610 . 02357v3 , Apr . 4 , 2017 , 8 pages . Chung et al . “ Empirical evaluation of gated recurrent neural net works on sequence modeling , " arXiv 1412 . 3555v1 , Dec . 11 , 2014 , Gehring et al . “ Convolutional sequence to sequence learning , " arXiv 1705 . 03122v2 , May 12 , 2017 , 15 pages . pages . Hochreiter et al . “ Gradient flow in recurrent nets : the difficulty of learning long - term dependencies , " A field Guide to Dynamical Recurrent Neural Networks , IEEE Press , 2001a , 15 pages . Hochreiter et al . “ Long short term memory , ” Neural Computation ( 8 ) , Nov . 1997 , 46 pages . Jozefowiz et al . “ Exploring the limits of language modeling , ” arXiv . 02410 , Feb . 7 , 2016 , 11 pages . Kaiser et al . “ Can active memory replace attention ? ” Advances in Neural Information Processing Systems , Dec . 2016 9 pages . Kaiser et al . “ Neural GPUs learn algorithms , ” International Con ference on Learning Representations , arXiv 1511 . 08228v3 , Mar . 15 , , 9 pages . Kalchbrenner et al . “ Neural machine translation in linear time , ” arXiv 1610 . 10099v2 , Mar . 15 , 2017 , 9 pages . Kim et al . " Structured attention networks , ” arXiv 1702 . 00887v3 , Feb . 16 , 2017 , 21 pages . Kingma et al . “ Adam : A method for stochastic optimization , ” arXiv . 6980v8 _ Jul . 23 , 2015 , 15 pages . Kuchaiev et al . “ Factorization tricks for Istm networks , ” arXiv . 10722v3 , Feb . 24 , 2018 , 6 pages . Lim et al . “ A structures self - attentive sentence embedding , ” arXiv . 03130v1 , Mar . 9 , 2017 , 15 pages . Luong et al . “ Effective approaches to attention based neural machine translation , ” arXiv 1508 . 04025v2 , Sep . 20 , 2015 , 11 pages . Parikh et al . “ A decomposable attention model for natural language inference , ” Proceedings of the Empirical Methods in Natural Lan guage Processing conference , Nov . 2016 , 7 pages . Paulus et al . “ A deep reinforced model for abstractive summariza tion , " arXiv 1705 . 04304v3 , Nov . 13 , 2017 , 12 pages . Sennrich et al . " Neural Machine Translation of rare words with subword units , ” arXiv 1508 . 07909v5 , Jun . 10 , 2016 , 11 pages . Shazeer et al . “ Outrageously large neural networks : The sparsely gated mixture - of - experts layer , ” arXiv 1701 . 06538v1 , Jan . 23 , Srivastava et al . “ Dropout : a simple way to prevent neural network from overfitting , ” Journal of Machine Learning Research , 15 ( 1 ) , Jan . 2014 , 30 pages . Sutskever et al . “ Sequence to sequence learning with neural net works , ” Advances in Neural Information Processing Systems , Dec . Szegedy et al . “ Rethinking the inception architecture for computer vision , ” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , Jun . 2016 , 9 pages . Wu et al . “ Google ' s neural machine translation system : Bridging the gap between human and machine translation , ” arXiv 1609 . 08144v2 , , 9 pages . , 19 pages . Oct . 8 , 2016 , 23 pages . * cited by examiner U . S . Patent Oct . 22 , 2019 Sheet 1 of 3 US 10 , 452 , 978 B2 Neural Network System Attention - Based Neural Network mm 176 - 150 Output Sequence 1 Input Sequence FIG . 1 as one on Oct . 22 , 2019 atent Sheet 2 of 3 US 10 , 452 , 978 B2 - * * * : FIG . 2 U . S . Patent Oct . 22 , 2019 Sheet 3 of 3 US 10 , 452 , 978 B2 Receive Input Sequence Process input sequence using encoder Process encoded representations using decoder FIG . 3 US 10 , 452 , 978 B2 cation No . 62 / 510 , 256 , filed on May 23 , 2017 , and U . S . 10 cies between distant positions during training . In the pres in the network , i . e . , the next hidden layer or the output layer . 25 translation task despite being easier to train and quicker to ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS CROSS - REFERENCE TO RELATED APPLICATIONS This application is a continuation of and claims priority to PCT Application No . PCT / US2018 / 034224 , filed on May , 2018 , which claims priority to U . S . Provisional Appli - Provisional Application No . 62 / 541 , 594 , filed on Aug . 4 , . The entire contents of the foregoing applications are hereby incorporated by reference . BACKGROUND This specification relates to transducing sequences using neural networks . Neural networks are machine learning models that 20 employ one or more layers of nonlinear units to predict an output for a received input . Some neural networks include one or more hidden layers in addition to an output layer . The output of each hidden layer is used as input to the next layer Each layer of the network generates an output from a received input in accordance with current values of a respec tive set of parameters . SUMMARY This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates an output sequence that includes a respective output at each of multiple positions in an output order from an input sequence that includes a respective input at each of multiple positions in an input order , i . e . , transduces the input sequence into the output sequence . In particular , the system generates the output sequence using an encoder neural network and a decoder 40 neural network that are both attention - based . Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages . Many existing approaches to sequence transduction using 45 neural networks use recurrent neural networks in both the encoder and the decoder . While these kinds of networks can achieve good performance on sequence transduction tasks , their computation is sequential in nature , i . e . , a recurrent neural network generates an output at a current time step 50 conditioned on the hidden state of the recurrent neural network at the preceding time step . This sequential nature precludes parallelization , resulting in long training and inference times and , accordingly , workloads that utilize a large amount of computational resources . On the other hand , because the encoder and the decoder of the described sequence transduction neural network are attention - based , the sequence transduction neural network can transduce sequences quicker , be trained faster , or both , Moreover , the sequence transduction neural network can transduce sequences more accurately than existing networks that are based on convolutional layers or recurrent layers , even though training and inference times are shorter . In particular , in conventional models , the number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions , e . g . , either linearly or logarithmically depending on the model architecture . This makes it more difficult to learn dependen ently described sequence transduction neural network , this number of operations is reduced to a constant number of operations because of the use of attention ( and , in particular , self - attention ) while not relying on recurrence or convolu tions . Self - attention , sometimes called intra - attention , is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence . The use of attention mechanisms allows the sequence transduction neural network to effectively learn dependencies between distant positions during training , improving the accuracy of the sequence transduction neural network on various transduction tasks , e . g . , machine trans lation . In fact , the described sequence transduction neural network can achieve state - of - the - art results on the machine generate outputs than conventional machine translation neu ral networks . The sequence transduction neural network can also exhibit improved performance over conventional machine translation neural networks without task - specific tuning through the use of the attention mechanism . The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below . Other features , aspects , and advantages of the subject matter will become apparent from the description , the drawings , and the claims . BRIEF DESCRIPTION OF THE DRAWINGS FIG . 1 shows an example neural network system . FIG . 2 is a diagram showing attention mechanisms that are applied by the attention sub - layers in the subnetworks of the encoder neural network and the decoder neural network . FIG . 3 is a flow diagram of an example process for generating an output sequence from an input sequence . Like reference numbers and designations in the various drawings indicate like elements . DETAILED DESCRIPTION This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates an output sequence that includes a respective output at each of multiple positions in an output order from an input sequence that includes a respective input at each of multiple positions in an input order , i . e . , transduces the input sequence into the output sequence . For example , the system may be a neural machine trans lation system . That is , if the input sequence is a sequence of because the operation of the network can be more easily 60 words in an original language , e . g . , a sentence or phrase , the parallelized . That is , because the described sequence trans - duction neural network relies entirely on an attention mecha - nism to draw global dependencies between input and output and does not employ any recurrent neural network layers , output sequence may be a translation of the input sequence into a target language , i . e . , a sequence of words in the target language that represents the sequence of words in the original language . the problems with long training and inference times and high 65 As another example , the system may be a speech recog resource usage caused by the sequential nature of recurrent neural network layers are mitigated . nition system . That is , if the input sequence is a sequence of audio data representing a spoken utterance , the output US 10 , 452 , 978 B2 sequence may be a sequence of graphemes , characters , or words that represents the utterance , i . e . , is a transcription of the input sequence . As another example , the system may be a natural lan - based . In some cases , neither the encoder nor the decoder include any convolutional layers or any recurrent layers . The encoder neural network 110 includes an embedding layer 120 and a sequence of one or more encoder subnet guage processing system . For example , if the input sequence 5 works 130 . In particular , as shown in FIG . 1 , the encoder is a sequence of words in an original language , e . g . , a sentence or phrase , the output sequence may be a summary of the input sequence in the original language , i . e . , a sequence that has fewer words than the input sequence but that retains the essential meaning of the input sequence . As another example , if the input sequence is a sequence of words that form a question , the output sequence can be a sequence of words that form an answer to the question . As another example , the system may be part of a com - 15 puter - assisted medical diagnosis system . For example , the input sequence can be a sequence of data from an electronic medical record and the output sequence can be a sequence - 15 of predicted treatments . neural network includes N encoder subnetworks 130 . The embedding layer 120 is configured to , for each network input in the input sequence , map the network input to a numeric representation of the network input in an embedding space , e . g . , into a vector in the embedding space . The embedding layer 120 then provides the numeric repre sentations of the network inputs to the first subnetwork in the sequence of encoder subnetworks 130 , i . e . , to the first encoder subnetwork 130 of the N encoder subnetworks 130 . In particular , in some implementations , the embedding layer 120 is configured to map each network input to an embedded representation of the network input and then combine , e . g . , sum or average , the embedded representation of the network input with a positional embedding of the layer 120 is As another example , the system may be part of an image 20 input position of the network input in the input order to processing system . For example , the input sequence can be an image , i . e . , a sequence of color values from the image , and the output can be a sequence of text that describes the image . As another example , the input sequence can be a generate a combined embedded representation of the net work input . That is , each position in the input sequence has a corresponding embedding and for each network input the embedding layer 120 combines the embedded representation sequence of text or a different context and the output 25 of the network input with the embedding of the network sequence can be an image that describes the context . In particular , the neural network includes an encoder neural network and a decoder neural network . Generally , both the encoder and the decoder are attention - based , i . e . , both apply an attention mechanism over their respective 30 received inputs while transducing the input sequence . In some cases , neither the encoder nor the decoder include any convolutional layers or any recurrent layers . FIG . 1 shows an example neural network system 100 . The input ' s position in the input sequence . Such positional embeddings can enable the model to make full use of the order of the input sequence without relying on recurrence or convolutions . butions In some cases , the positional embeddings are learned . As used in this specification , the term “ learned ” means that an operation or a value has been adjusted during the training of the sequence transduction neural network 108 . Training the sequence transduction neural network 108 is described neural network system 100 is an example of a system 35 below with reference to FIG . 3 . implemented as computer programs on one or more com - puters in one or more locations , in which the systems , components , and techniques described below can be imple - The neural network system receives an input 40 sequence 102 and processes the input sequence 102 to transduce the input sequence 102 into an output sequence mented . . In some other cases , the positional embeddings are fixed and are different for each position . For example , the embed dings can be made up of sine and cosine functions of different frequencies and can satisfy : PE ( pos , 21 ) = sin ( pos / 100002ild model ) PE ( pos , 2i + 1 ) = cos ( pos / 100002i / dmodel ) The input sequence 102 has a respective network input at where pos is the position , i is the dimension within the each of multiple input positions in an input order and the 45 positional embedding , and mode is the dimensionality of the output sequence 152 has a respective network output at each of multiple output positions in an output order . That is , the input sequence 102 has multiple inputs arranged according to an input order and the output sequence 152 has multiple outputs arranged according to an output order . As described above , the neural network system 100 can perform any of a variety of tasks that require processing sequential inputs to generate sequential outputs . The neural network system 100 includes an attention positional embedding ( and of the other vectors processed by the neural network 108 ) . The use of sinusoidal positional embeddings may allow the model to extrapolate to longer sequence lengths , which can increase the range of applica tions for which the model can be employed . The combined embedded representation is then used as the numeric representation of the network input . Each of the encoder subnetworks 130 is configured to receive a respective encoder subnetwork input for each of based sequence transduction neural network 108 , which in 55 the plurality of input positions and to generate a respective turn includes an encoder neural network 110 and a decoder subnetwork output for each of the plurality of input posi neural network 150 . tions . The encoder neural network 110 is configured to receive the input sequence 102 and generate a respective encoded The encoder subnetwork outputs generated by the last encoder subnetwork in the sequence are then used as the representation of each of the network inputs in the input 60 encoded representations of the network inputs . sequence . Generally , an encoded representation is a vector or other ordered collection of numeric values . The decoder neural network 150 is then configured to use the encoded representations of the network inputs to gener - ate the output sequence 152 . Generally , and as will be described in more detail below , both the encoder 110 and the decoder 150 are attention For the first encoder subnetwork in the sequence , the encoder subnetwork input is the numeric representations generated by the embedding layer 120 , and , for each encoder subnetwork other than the first encoder subnetwork in the sequence , the encoder subnetwork input is the encoder subnetwork output of the preceding encoder subnetwork in the sequence . US 10 , 452 , 978 B2 Each encoder subnetwork 130 includes an encoder self attention sub - layer 132 . The encoder self - attention sub - layer is configured to receive the subnetwork input for each of the plurality of input positions and , for each particular input position in the input order , apply an attention mecha - 5 nism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for the particular input position . In particular , for a given output position , the decoder neural network generates an output that defines a probability distribution over possible network outputs at the given output position . The decoder neural network can then select a network output for the output position by sampling from the probability distribution or by selecting the network output with the highest probability . Because the decoder neural network 150 is auto - regres sive , at each generation time step , the decoder 150 operates In some cases , the attention mechanism is a multi - head | 10 on the network outputs that have already been generated before the generation time step , i . e . , the network outputs at attention mechanism . The attention mechanism and how the attention mechanism is applied by the encoder self - attention sub - layer 132 will be described in more detail below with reference to FIG . 2 . In some implementations , each of the encoder subnet works 130 also includes a residual connection layer that combines the outputs of the encoder self - attention sub - layer with the inputs to the encoder self - attention sub - layer to output positions preceding the corresponding output posi tion in the output order . In some implementations , to ensure this is the case during both inference and training , at each generation time step the decoder neural network 150 shifts the already generated network outputs right by one output order position ( i . e . , introduces a one position offset into the already generated network output sequence ) and ( as will be described in more detail below ) masks certain operations so generate an encoder self - attention residual output and a layer 20 that positions can only attend to positions up to and includ normalization layer that applies layer normalization to the encoder self - attention residual output . These two layers are collectively referred to as an “ Add & Norm ” operation in FIG . 1 . ing that position in the output sequence ( and not subsequent positions ) . While the remainder of the description below describes that , when generating a given output at a given output position , various components of the decoder 150 Some or all of the encoder subnetworks can also include 25 operate on data at output positions preceding the given a position - wise feed - forward layer 134 that is configured to operate on each position in the input sequence separately . In particular , for each input position , the feed - forward layer is configured receive an input at the input position and apply a sequence of transformations to the input at the input 30 position to generate an output for the input position . For example , the sequence of transformations can include two or more learned linear transformations each separated by an activation function , e . g . , a non - linear elementwise activation output positions and not on data at any other output positions ) , it will be understood that this type of condition ing can be effectively implemented using the shifting described above . The decoder neural network 150 includes an embedding layer 160 , a sequence of decoder subnetworks 170 , a linear layer 180 , and a softmax layer 190 . In particular , as shown in FIG . 1 , the decoder neural network includes N decoder subnetworks 170 . However , while the example of FIG . 1 function , e . g . , a ReLU activation function , which can allow 35 shows the encoder 110 and the decoder 150 including the for faster and more effective training on large and complex datasets . The inputs received by the position - wise feed forward layer 134 can be the outputs of the layer normal - ization layer when the residual and layer normalization layers are included or the outputs of the encoder self - 40 attention sub - layer 132 when the residual and layer normal - ization layers are not included . The transformations applied by the layer 134 will generally be the same for each input position ( but different feed - forward layers in different sub networks will apply different transformations ) . In cases where an encoder subnetwork 130 includes a position - wise feed - forward layer 134 , the encoder subnet - work can also include a residual connection layer that combines the outputs of the position - wise feed - forward same number of subnetworks , in some cases the encoder 110 and the decoder 150 include different numbers of subnet works . That is , the decoder 150 can include more or fewer subnetworks than the encoder 110 . The embedding layer 160 is configured to , at each gen eration time step , for each network output at an output position that precedes the current output position in the output order , map the network output to a numeric repre sentation of the network output in the embedding space . The embedding layer 160 then provides the numeric represen tations of the network outputs to the first subnetwork 170 in the sequence of decoder subnetworks , i . e . , to the first decoder subnetwork 170 of the N decoder subnetworks . In particular , in some implementations , the embedding layer with the inputs to the position - wise feed - forward layer 50 layer 160 is configured to map each network output to an to generate an encoder position - wise residual output and a layer normalization layer that applies layer normalization to the encoder position - wise residual output . These two layers are also collectively referred to as an “ Add & Norm ” embedded representation of the network output and combine the embedded representation of the network output with a positional embedding of the output position of the network output in the output order to generate a combined embedded operation in FIG . 1 . The outputs of this layer normalization 55 representation of the network output . The combined embed layer can then be used as the outputs of the encoder subnetwork 130 . Once the encoder neural network 110 has generated the encoded representations , the decoder neural network 150 is configured to generate the output sequence in an auto - 60 120 . ded representation is then used as the numeric representation of the network output . The embedding layer 160 generates the combined embedded representation in the same manner as described above with reference to the embedding layer regressive manner . That is , the decoder neural network 150 generates the output sequence , by at each of a plurality of generation time steps , generating a network output for a corresponding Each decoder subnetwork 170 is configured to , at each generation time step , receive a respective decoder subnet work input for each of the plurality of output positions preceding the corresponding output position and to generate output position conditioned on ( i ) the encoded representa - 65 a respective decoder subnetwork output for each of the tions and ( ii ) network outputs at output positions preceding the output position in the output order . plurality of output positions preceding the corresponding output position ( or equivalently , when the output sequence US 10 , 452 , 978 B2 has been shifted right , each network output at a position up to and including the current output position ) . In particular , each decoder subnetwork 170 includes two different attention sub - layers : a decoder self - attention sub - layer 172 and an encoder - decoder attention sub - layer 174 . 5 Each decoder self - attention sub - layer 172 is configured to , at each generation time step , receive an input for each output position preceding the corresponding output position and , for each of the particular output positions , apply an included . residual and layer normalization layers are included or the outputs of the last attention sub - layer in the subnetwork 170 when the residual and layer normalization layers are not In cases where a decoder subnetwork 170 includes a position - wise feed - forward layer 176 , the decoder subnet work can also include a residual connection layer that combines the outputs of the position - wise feed - forward layer with the inputs to the position - wise feed - forward layer attention mechanism over the inputs at the output positions 10 to generate a decoder position - wise residual output and a preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the par - ticular output position . That is , the decoder self - attention layer normalization layer that applies layer normalization to the decoder position - wise residual output . These two layers are also collectively referred to as an “ Add & Norm ” operation in FIG . 1 . The outputs of this layer normalization sub - layer 172 applies an attention mechanism that is masked 15 layer can then be used as the outputs of the decoder so that it does not attend over or otherwise process any data that is not at a position preceding the current output position subnetwork 170 . in the output sequence . Each encoder - decoder attention sub - layer 174 , on the At each generation time step , the linear layer 180 applies a learned linear transformation to the output of the last decoder subnetwork 170 in order to project the output of the other hand , is configured to , at each generation time step , 20 last decoder subnetwork 170 into the appropriate space for receive an input for each output position preceding the corresponding output position and , for each of the output positions , apply an attention mechanism over the encoded representations at the input positions using one or more processing by the softmax layer 190 . The softmax layer 190 then applies a softmax function over the outputs of the linear layer 180 to generate the probability distribution over the possible network outputs at the generation time step . As queries derived from the input for the output position to 25 described above , the decoder 150 can then select a network generate an updated representation for the output position . Thus , the encoder - decoder attention sub - layer 174 applies attention over encoded representations while the encoder self - attention sub - layer 172 applies attention over inputs at output positions . The attention mechanism applied by each of these atten - tion sub - layers will be described in more detail below with reference to FIG . 2 . In FIG . 1 , the decoder self - attention sub - layer 172 is output from the possible network outputs using the prob ability distribution . FIG . 2 is a diagram 200 showing attention mechanisms that are applied by the attention sub - layers in the subnet works of the encoder neural network 110 and the decoder neural network 150 . Generally , an attention mechanism maps a query and a set of key - value pairs to an output , where the query , keys , and values are all vectors . The output is computed as a weighted shown as being before the encoder - decoder attention sub - 35 sum of the values , where the weight assigned to each value layer in the processing order within the decoder subnetwork . In other examples , however , the decoder self - attention sub - layer 172 may be after the encoder - decoder attention sub - layer 174 in the processing order within the decoder is computed by a compatibility function of the query with the corresponding key . More specifically , each attention sub - layer applies a scaled dot - product attention mechanism 230 . In scaled dot subnetwork 170 or different subnetworks may have different 40 product attention , for a given query , the attention sub - layer processing orders . In some implementations , each decoder subnetwork 170 includes , after the decoder self - attention sub - layer 172 , after the encoder - decoder attention sub - layer 174 , or after each of computes the dot products of the query with all of the keys , divides each of the dot products by a scaling factor , e . g . , by the square root of the dimensions of the queries and keys , and then applies a softmax function over the scaled dot the two sub - layers , a residual connection layer that com - 45 products to obtain the weights on the values . The attention bines the outputs of the attention sub - layer with the inputs to the attention sub - layer to generate a residual output and a layer normalization layer that applies layer normalization to the residual output . FIG . 1 shows these two layers being sub - layer then computes a weighted sum of the values in accordance with these weights . Thus , for scaled dot - product attention the compatibility function is the dot product and the output of the compatibility function is further scaled by inserted after each of the two sub - layers , both referred to as 50 the scaling factor . an “ Add & Norm ” operation . Some or all of the decoder subnetwork 170 also include a position - wise feed - forward layer 176 that is configured to a similar manner as the position - wise feed - operate in In operation and as shown in the left hand side of FIG . 2 , the attention sub - layer computes the attention over a set of queries simultaneously . In particular , the attention sub - layer packs the queries into a matrix Q , packs the keys into a forward layer 134 from the encoder 110 . In particular , the 55 matrix K , and packs the values into a matrix V . To pack a set layer 176 is configured to , at each generation time step : for each output position preceding the corresponding output position : receive an input at the output position , and apply a sequence of transformations to the input at the output of vectors into a matrix , the attention sub - layer can generate a matrix that includes the vectors as the rows of the matrix . The attention sub - layer then performs a matrix multiply ( MatMul ) between the matrix Q and the transpose of the position to generate an output for the output position . For 60 matrix K to generate a matrix of compatibility function example , the sequence of transformations can include two or more learned linear transformations each separated by an activation function , e . g . , a non - linear elementwise activation function , e . g . , a ReLU activation function . The inputs received by the position - wise feed - forward layer 176 can be 65 the outputs of the layer normalization layer ( following the last attention sub - layer in the subnetwork 170 ) when the outputs . The attention sub - layer then scales the compatibility function output matrix , i . e . , by dividing each element of the matrix by the scaling factor . The attention sub - layer then applies a softmax over the scaled output matrix to generate a matrix of weights and performs a matrix multiply ( MatMul ) between the weight US 10 , 452 , 978 B2 matrix and the matrix V to generate an output matrix that includes the output of the attention mechanism for each of For sub - layers that use masking , i . e . , decoder attention the values . all positions in the input order . Thus , there is a respective key , value , and query for each position in the input order . When the attention sub - layer is a decoder self - attention sub - layer , each position in the decoder attends to all posi sub - layers , the attention sub - layer masks the scaled output 5 tions in the decoder preceding that position . Thus , all of the matrix before applying the softmax . That is , the attention sub - layer masks out ( sets to negative infinity ) , all values in the scaled output matrix that correspond to positions after the current output position . keys , values , and queries come from the same place , in this case , the output of the previous subnetwork in the decoder , or , for the decoder self - attention sub - layer in the first decoder subnetwork , the embeddings of the outputs already In some implementations , to allow the attention sub - 10 generated . Thus , there is a respective key , value , and query layers to jointly attend to information from different repre - sentation subspaces at different positions , the attention sub layers employ multi - head attention , as illustrated on the right hand side of FIG . 2 . position . for each position in the output order before the current When the attention sub - layer is an encoder - decoder atten tion sub - layer , the queries come from the previous compo In particular , to implement multi - ahead attention , the 15 nent in the decoder and the keys and values come from the attention sub - layer applies h different attention mechanisms in parallel . In other words , the attention sub - layer includes h different attention layers , with each attention layer within the same attention sub - layer receiving the same original queries Q , original keys K , and original values V . Each attention layer is configured to transform the origi - nal queries , and keys , and values using learned linear transformations and then apply the attention mechanism 230 to the transformed queries , keys , and values . Each attention output of the encoder , i . e . , from the encoded representations generated by the encoder . This allows every position in the decoder to attend over all positions in the input sequence . Thus , there is a respective query for each for each position in the output order before the current position and a respec tive key and a respective value for each position in the input In more detail , when the attention sub - layer is an encoder self - attention sub - layer , for each particular input position in order . layer will generally learn different transformations from 25 the input order , the encoder self - attention sub - layer is con each other attention layer in the same attention sub - layer . In particular , each attention layer is configured to apply a learned query linear transformation to each original query to generate a layer - specific query for each original query , apply figured to apply an attention mechanism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for a learned key linear transformation to each original key to 30 the particular input position . generate a layer - specific key for each original key , and apply a learned value linear transformation to each original value to generate a layer - specific values for each original value . The attention layer then applies the attention mechanism When the encoder self - attention sub - layer implements multi - head attention , each encoder self - attention layer in the encoder self - attention sub - layer is configured to : apply a learned query linear transformation to each encoder subnet described above using these layer - specific queries , keys , and 35 work input at each input position to generate a respective values to generate initial outputs for the attention layer . The attention sub - layer then combines the initial outputs of the attention layers to generate the final output of the attention sub - layer . As shown in FIG . 2 , the attention sub - query for each input position , apply a learned key linear transformation to each encoder subnetwork input at each input position to generate a respective key for each input position , apply a learned value linear transformation to each layer concatenates ( concat ) the outputs of the attention 40 encoder subnetwork input at each input position to generate layers and applies a learned linear transformation to the concatenated output to generate the output of the attention sub - layer . In some cases , the learned transformations applied by the a respective value for each input position , and then apply the attention mechanism ( i . e . , the scaled dot - product attention mechanism described above ) using the queries , keys , and values to determine an initial encoder self - attention output attention sub - layer reduce the dimensionality of the original 45 for each input position . The sub - layer then combines the keys and values and , optionally , the queries . For example , when the dimensionality of the original keys , values , and queries is d and there are h attention layers in the sub - layer , the sub - layer may reduce the dimensionality of the original initial outputs of the attention layers as described above . when the attention sub - layer is a decoder self - attention sub - layer , the decoder self - attention sub - layer is configured to , at each generation time step : receive an input for each keys , values , and queries to d / h . This keeps the computation 50 output position preceding the corresponding output position layer . cost of the multi - head attention mechanism similar to what the cost would have been to perform the attention mecha nism once with full dimensionality while at the same time increasing the representative capacity of the attention sub While the attention mechanism applied by each attention sub - layer is the same , the queries , keys , and values are different for different types of attention . That is , different types of attention sub - layers use different sources for the and , for each of the particular output positions , apply an attention mechanism over the inputs at the output positions preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the par ticular output position . When the decoder self - attention sub - layer implements multi - head attention , each attention layer in the decoder self - attention sub - layer is configured to , at each generation original queries , keys , and values that are received as input 60 time step , apply a learned query linear transformation to the by the attention sub - layer . In particular , when the attention sub - layer is an encoder self - attention sub - layer , all of the keys , values and queries come from the same place , in this case , the output of the input at each output position preceding the corresponding output position to generate a respective query for each output position , apply a learned key linear transformation to each input at each output position preceding the correspond previous subnetwork in the encoder , or , for the encoder 65 ing output position to generate a respective key for each self - attention sub - layer in first subnetwork , the embeddings of the inputs and each position in the encoder can attend to output position , apply a learned value linear transformation to each input at each output position preceding the corre US 10 , 452 , 978 B2 sponding output position to generate a respective key for each output position , and then apply the attention mecha - nism ( i . e . , the scaled dot - product attention mechanism described above ) using the queries , keys , and values to decoder makes use of both the already generated outputs and the encoded representations when generating the given The system can perform the process 300 for input output . determine an initial decoder self - attention output for each of 5 sequences for which the desired output , i . e . , the output the output positions . The sub - layer then combines the initial outputs of the attention layers as described above . When the attention sub - layer is an encoder - decoder atten - tion sub - layer , the encoder - decoder attention sub - layer is sequence that should be generated by the system for the input sequence , is not known . The system can also perform the process 300 on input sequences in a set of training data , i . e . , a set of inputs for configured to , at each generation time step : receive an input 10 which the output sequence that should be generated by the for each output position preceding the corresponding output position and , for each of the output positions , apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the output position to generate an updated repre - system is known , in order to train the encoder and the decoder to determine trained values for the parameters of the encoder and decoder . The process 300 can be performed repeatedly on inputs selected from a set of training data as part of a conventional machine learning training technique sentation for the output position . When the encoder - decoder attention sub - layer imple ments multi - head attention , each attention layer is config ured to , at each generation time step : apply a learned query to train the initial neural network layers , e . g . , a gradient descent with backpropagation training technique that uses a conventional optimizer , e . g . , the Adam optimizer . During training , the system can incorporate any number of tech linear transformation to the input at each output position 20 niques to improve the speed , the effectiveness , or both of the preceding the corresponding output position to generate a respective query for each output position , apply a learned key linear transformation to each encoded representation at each input position to generate a respective key for each training process . For example , the system can use dropout , label smoothing , or both to reduce overfitting . As another example , the system can perform the training using a dis tributed architecture that trains multiple instances of the input position , apply a learned value linear transformation to 25 sequence transduction neural network in parallel . each encoded representation at each input position to gen - erate a respective value for each input position , and then apply the attention mechanism ( i . e . , the scaled dot - product attention mechanism described above ) using the queries , This specification uses the term " configured ” in connec tion with systems and computer program components . For a system of one or more computers to be configured to perform particular operations or actions means that the keys , and values to determine an initial encoder - decoder 30 system has installed on it software , firmware , hardware , or attention output for each input position . The sub - layer then combines the initial outputs of the attention layers as described above . FIG . 3 is a flow diagram of an example process for a combination of them that in operation cause the system to perform the operations or actions . For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include generating an output sequence from an input sequence . For 35 instructions that , when executed by data processing appa convenience , the process 300 will be described as being performed by a system of one or more computers located in one or more locations . For example , a neural network system , e . g . , neural network system 100 of FIG . 1 , appro - ratus , cause the apparatus to perform the operations or actions . Embodiments of the subject matter and the functional operations described in this specification can be imple priately programmed in accordance with this specification , 40 mented in digital electronic circuitry , in tangibly - embodied can perform the process 300 . The system receives an input sequence ( step 310 ) . The system processes the input sequence using the encoder neural network to generate a respective encoded computer software or firmware , in computer hardware , including the structures disclosed in this specification and their structural equivalents , or in combinations of one or more of them . Embodiments of the subject matter described representation of each of the network inputs in the input 45 in this specification can be implemented as one or more sequence ( step 320 ) . In particular , the system processes the input sequence through the embedding layer to generate an embedded representation of each network input and then process the embedded representations through the sequence computer programs , i . e . , one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by , or to control the operation of , data processing apparatus . The computer storage medium of encoder subnetworks to generate the encoded represen - 50 can be a machine - readable storage device , a machine - read tations of the network inputs . The system processes the encoded representations using the decoder neural network to generate an output sequence ( step 330 ) . The decoder neural network is configured to able storage substrate , a random or serial access memory device , or a combination of one or more of them . Alterna tively or in addition , the program instructions can be encoded on an artificially generated propagated signal , e . g . , generate the output sequence from the encoded representa - 55 a machine - generated electrical , optical , or electromagnetic tions in an auto - regressive manner . That is , the decoder neural network generates one output from the output sequence at each generation time step . At a given generation time step at which a given output is being generated , the s ignal , that is generated to encode information for transmis sion to suitable receiver apparatus for execution by a data processing apparatus . The term “ data processing apparatus ” refers to data pro system processes the outputs before the given output in the 60 cessing hardware and encompasses all kinds of apparatus , output sequence through the embedding layer in the decoder to generate embedded representations . The system then processes the embedded representations through the sequence of decoder subnetworks , the linear layer , and the devices , and machines for processing data , including by way of example a programmable processor , a computer , or mul tiple processors or computers . The apparatus can also be , or further include , special purpose logic circuitry , e . g . , an softmax layer to generate the given output . Because the 65 FPGA ( field programmable gate array ) or an ASIC ( appli decoder subnetworks include encoder - decoder attention sub - layers as well as decoder self - attention sub - layers , the cation specific integrated circuit ) . The apparatus can option ally include , in addition to hardware , code that creates an US 10 , 452 , 978 B2 execution environment for computer programs , e . g . , code that constitutes processor firmware , a protocol stack , a database management system , an operating system , or a combination of one or more of them . Global Positioning System ( GPS ) receiver , or a portable storage device , e . g . , a universal serial bus ( USB ) flash drive , to name just a few . Computer readable media suitable for storing computer A computer program , which may also be referred to or 5 program instructions and data include all forms of non described as a program , software , a software application , an app , a module , a software module , a script , or code , can be written in any form of programming language , including compiled or interpreted languages , or declarative or proce - dural languages ; and it can be deployed in any form , " including as a stand alone program or as a module , compo nent , subroutine , or other unit suitable for use in a computing environment . A program may , but need not , correspond to a volatile memory , media and memory devices , including by way of example semiconductor memory devices , e . g . , EPROM , EEPROM , and flash memory devices ; magnetic disks , e . g . , internal hard disks or removable disks ; magneto optical disks ; and CD ROM and DVD - ROM disks . To provide for interaction with a user , embodiments of the subject matter described in this specification can be imple mented on a computer having a display device , e . g . , a CRT file in a file system . A program can be stored in a portion of 15 ( cathode ray tube ) or LCD ( liquid crystal display ) monitor , a file that holds other programs or data , e . g . , one or more scripts stored in a markup language document , in a single file dedicated to the program in question , or in multiple coordinated files , e . g . , files that store one or more modules , for displaying information to the user and a keyboard and a pointing device , e . g . , a mouse or a trackball , by which the user can provide input to the computer . Other kinds of devices can be used to provide for interaction with a user as sub programs , or portions of code . A computer program can 20 well ; for example , feedback provided to the user can be any be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network . form of sensory feedback , e . g . , visual feedback , auditory feedback , or tactile feedback ; and input from the user can be received in any form , including acoustic , speech , or tactile input . In addition , a computer can interact with a user by In this specification , the term " database ” is used broadly 25 sending documents to and receiving documents from a to refer to any collection of data : the data does not need to be structured in any particular way , or structured at all , and it can be stored on storage devices in one or more locations . Thus , for example , the index database can include multiple collections of data , each of which may be organized and accessed differently . Similarly , in this specification the term " engine " is used broadly to refer to a software - based system , subsystem , or process that is programmed to perform one or more specific a functions . Generally , an engine will be implemented as one or more software modules or components , installed on one or more computers in one or more locations . In some cases , one or more computers will be dedicated to a particular device that is used by the user ; for example , by sending web pages to a web browser on a user ' s device in response to requests received from the web browser . Also , a computer can interact with a user by sending text messages or other forms of message to a personal device , e . g . , a smartphone that is running a messaging application , and receiving responsive messages from the user in return . Data processing apparatus for implementing machine learning models can also include , for example , special purpose hardware accelerator units for processing common and compute - intensive parts of machine learning training or production , i . e . , inference , workloads . Machine learning models can be implemented and engine ; in other cases , multiple engines can be installed and 40 deployed using a machine learning framework , e . g . , a Ten running on the same computer or computers . The processes and logic flows described in this specifi - cation can be performed by one or more programmable computers executing one or more computer programs to sorFlow framework , a Microsoft Cognitive Toolkit frame work , an Apache Singa framework , or an Apache MXNet framework . Embodiments of the subject matter described in this perform functions by operating on input data and generating 45 specification can be implemented in a computing system that output . The processes and logic flows can also be performed by special purpose logic circuitry , e . g . , an FPGA or an ASIC , or by a combination of special purpose logic circuitry and one or more programmed computers . includes a back end component , e . g . , as a data server , or that includes a middleware component , e . g . , an application server , or that includes a front end component , e . g . , a client computer having a graphical user interface , a web browser , Computers suitable for the execution of a computer 50 or an app through which a user can interact with an imple program can be based on general or special purpose micro - processors or both , or any other kind of central processing unit . Generally , a central processing unit will receive instructions and data from a read only memory or a random mentation of the subject matter described in this specifica tion , or any combination of one or more such back end , middleware , or front end components . The components of the system can be interconnected by any form or medium of access memory or both . The essential elements of a com - 55 digital data communication , e . g . , a communication network . puter are a central processing unit for performing or execut - ing instructions and one or more memory devices for storing instructions and data . The central processing unit and the memory can be supplemented by , or incorporated in , special Internet . Examples of communication networks include a local area network ( LAN ) and a wide area network ( WAN ) , e . g . , the The computing system can include clients and servers . A purpose logic circuitry . Generally , a computer will also 60 client and server are generally remote from each other and include , or be operatively coupled to receive data from or transfer data to , or both , one or more mass storage devices for storing data , e . g . , magnetic , magneto optical disks , or optical disks . However , a computer need not have such typically interact through a communication network . The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client - server relationship to each other . In some embodi devices . Moreover , a computer can be embedded in another 65 ments , a server transmits data , e . g . , an HTML page , to a user device , e . g . , a mobile telephone , a personal digital assistant ( PDA ) , a mobile audio or video player , a game console , a device , e . g . , for purposes of displaying data to and receiving user input from a user interacting with the device , which acts US 10 , 452 , 978 B2 as a client . Data generated at the user device , e . g . , a result of the user interaction , can be received at the server from the device . While this specification contains many specific imple mentation details , these should not be construed as limita - tions on the scope of any invention or on the scope of what may be claimed , but rather as descriptions of features that may be specific to particular embodiments of particular inventions . Certain features that are described in this speci fication in the context of separate embodiments can also be 10 implemented in combination in a single embodiment . Con versely , various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination . Moreover , although features may be described above as 15 acting in certain combinations and even initially be claimed as such , one or more features from a claimed combination can in some cases be excised from the combination , and the claimed combination may be directed to a subcombination or variation of a subcombination . Similarly , while operations are depicted in the drawings and recited in the claims in a particular order , this should not be understood as requiring that such operations be per formed in the particular order shown or in sequential order , or that all illustrated operations be performed , to achieve 25 desirable results . In certain circumstances , multitasking and parallel processing may be advantageous . Moreover , the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments , and it should 30 be understood that the described program components and systems can generally be integrated together in software product or packaged into multiple software prod a single ucts . Particular embodiments of the subject matter have been 35 described . Other embodiments are within the scope of the following claims . For example , the actions recited in the claims can be performed in a different order and still achieve desirable results . As one example , the processes depicted in particular order shown , or sequential order , to achieve desirable results . In some cases , multitasking and parallel processing may be advantageous . What is claimed is : . A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement a sequence transduction neu ral network for transducing an input sequence having a 50 respective network input at each of a plurality of input positions in an input order into an output sequence having a respective network output at each of a plurality of output positions in an output order , the sequence transduction neural network comprising : an encoder neural network configured to receive the input sequence and generate a respective encoded represen - tation of each of the network inputs in the input sequence , the encoder neural network comprising a sequence of one or more encoder subnetworks , each 60 encoder subnetwork configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions , and each encoder subnetwork comprising : an encoder self - attention sub - layer that is configured to receive the subnetwork input for each of the plurality of input positions and , for each particular input position in the input order : apply a self - attention mechanism over the encoder subnetwork inputs at the plurality of input posi tions to generate a respective output for the par ticular input position , wherein applying a self attention mechanism comprises : determining a query from the subnetwork input at the particular input position , determining keys derived from the subnetwork inputs at the plurality of input posi tions , determining values derived from the sub network inputs at the plurality of input positions , and using the determined query , keys , and values to generate the respective output for the particular input position ; and a decoder neural network configured to receive the encoded representations and generate the output sequence . . The system of claim 1 , wherein the encoder neural network further comprises : an embedding layer configured to : for each network input in the input sequence , map the network input to an embedded representa tion of the network input , and combine the embedded representation of the network input with a positional embedding of the input position of the network input in the input order to generate a combined embedded representation of the network input ; and provide the combined embedded representations of the network inputs as the encoder subnetwork inputs for a first encoder subnetwork in the sequence of encoder subnetworks . . The system of claim 1 , wherein the respective encoded representations of the network inputs are the encoder sub network outputs generated by the last encoder subnetwork in the sequence . . The system of claim 1 , wherein the sequence of one or subnetworks , and wherein , for each encoder subnetwork other than a first encoder subnetwork in the sequence , the encoder subnetwork input is the encoder subnetwork output of a preceding encoder subnetwork in the sequence . . The system of claim , wherein at least one of the encoder subnetworks further comprises : a position - wise feed - forward layer that is configured to : for each input position : receive an input at the input position , and apply a sequence of transformations to the input at the input position to generate an output for the input position . tion function . . The system of claim 5 , wherein the sequence comprises two learned linear transformations separated by an activa . The system of claim 5 , wherein the at least one encoder subnetwork further comprises : a residual connection layer that combines the outputs of the position - wise feed - forward layer with the inputs to the position - wise feed - forward layer to generate an encoder position - wise residual output , and a layer normalization layer that applies layer normaliza tion to the encoder position - wise residual output . . The system of claim 1 , wherein each encoder subnet work further comprises : a residual connection layer that combines the outputs of the encoder self - attention sub - layer with the inputs to the accompanying figures do not necessarily require the 40 more encoder subnetworks includes at least two encoder US 10 , 452 , 978 B2 the encoder self - attention sub - layer to generate an encoder self - attention residual output , and a layer normalization layer that applies layer normaliza tion to the encoder self - attention residual output . provide the combined embedded representations of the network output as input to a first decoder subnetwork in the sequence of decoder subnetworks . . The system of claim 14 , wherein at least one of the . The system of claim , wherein each encoder self - 5 decoder subnetworks comprises : attention sub - layer comprises a plurality of encoder self a position - wise feed - forward layer that is configured to , at attention layers . . The system of claim 9 , wherein each encoder self - attention layer is configured to : apply a learned query linear transformation to each 10 encoder subnetwork input at each input position to generate a respective query for each input position , apply a learned key linear transformation to each encoder subnetwork input at each input position to generate a respective key for each input position , apply a learned value linear transformation to each encoder subnetwork input at each input position to generate a respective value for each input position , and for each input position , determine a respective input - position specific weight 20 for the input position by applying a comparison function between the query for the input position and the keys generated for the plurality of input posi tions , and determine an initial encoder self - attention output for 25 the input position by determining a weighted sum of the values weighted by the corresponding input - position specific weights for the plurality of input positions , the values being generated for the plurality of input positions . . The system of claim 10 , wherein the encoder self attention sub - layer is configured to , for each input position , combine the initial encoder self - attention outputs for the input position generated by the encoder self - attention layers to generate the output for the encoder self - attention sub - 35 layer . . The system of claim 9 , wherein the encoder self attention layers operate in parallel . . The system of claim 1 , wherein the decoder neural at each of a plurality of generation time steps , generating a network output at an output position corresponding to the generation time step conditioned on the encoded represen tations and network outputs at output positions preceding the output position in the output order . . The system of claim 13 , wherein the decoder neural network comprises a sequence of decoder subnetworks , each decoder subnetwork configured to , at each generation time step , receive a respective decoder subnetwork input for each of the plurality of output positions preceding the correspond - 50 ing output position and to generate a respective decoder subnetwork output for each of the plurality of output posi tions preceding the corresponding output position . . The system of claim 14 , wherein the decoder neural network further comprises : an embedding layer configured to , at each generation time step : order : for each network output at output positions preceding the corresponding output position in the output map the network output to an embedded represen tation of the network output , and combine the embedded representation of the network output with a positional embedding of the corre - each generation time step : for each particular output position preceding the cor responding output position : receive an input at the particular output position , and apply a sequence of transformations to the input at the particular output position to generate an output for the particular output position . . The system of claim 16 , wherein the sequence com prises two learned linear transformations separated by an activation function . . The system of claim decoder subnetwork further comprises : , wherein the at least one a residual connection layer that combines the outputs of the position - wise feed - forward layer with the inputs to the position - wise feed - forward layer to generate a residual output , and a layer normalization layer that applies layer normaliza tion to the residual output . . The system of claim 14 , wherein each decoder sub network comprises : an encoder - decoder attention sub - layer that is configured to , at each generation time step : receive an input for each particular output position preceding the corresponding output position and , for each of the particular output positions : apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the par ticular output position to generate an updated representation for the particular output position . . The system of claim , wherein each encoder decoder attention sub - layer comprises a plurality of encoder attention layer is configured to , at each generation time step : apply a learned query linear transformation to the input at each of the particular output positions preceding the corresponding output position to generate a respective query for each particular output position , apply a learned key linear transformation to each encoded representation at each input position to generate a respective key for each input position , apply a learned value linear transformation to each encoded representation at each input position to gen erate a respective value for each input position , and for each particular output position preceding the corre sponding output position , determine a respective output - position specific weight for each of the input positions by applying a com parison function between the query for the particular output position and the keys , and determine an initial encoder - decoder attention output for the particular output position by determining a weighted sum of the values weighted by the corre sponding output - position specific weights for the input position . . The system of claim 20 , wherein the encoder - decoder attention sub - layer is configured to , at each generation time network auto - regressively generates the output sequence , by 40 decoder attention layers , and wherein each encoder - decoder sponding output position of the network output in 65 step , combine the encoder - decoder attention outputs gener the output order to generate a combined embedded representation of the network output ; and a ted by the encoder - decoder attention layers to generate the output for the encoder - decoder attention sub - layer . US 10 , 452 , 978 B2 . The system of claim 20 , wherein the encoder - decoder attention layers operate in parallel . . The system of claim 19 wherein each decoder sub - network further comprises : computers cause the one or more computers to implement a sequence transduction neural network for transducing an input sequence having a respective network input at each of a plurality of input positions in an input order into an output a residual connection layer that combines the outputs of 5 sequence having a respective network output at each of a the encoder - decoder attention sub - layer with the inputs to the encoder - decoder attention sub - layer to generate a residual output , and a layer normalization layer that applies layer normaliza tion to the residual output . . The system of claim 14 , wherein each decoder sub network comprises : a decoder self - attention sub - layer that is configured to , at each generation time step : receive an input for each particular output position 15 preceding the corresponding output position and , for each particular output position : apply an attention mechanism over the inputs at the particular output positions preceding the corre sponding output position using one or more que - 20 ries derived from the input at the particular output position to generate a updated representation for the particular output position . . The system of claim 24 , wherein each decoder self attention sub - layer comprises a plurality of decoder self - 25 attention layers , and wherein each decoder self - attention layer is configured to , at each generation time step : apply a learned query linear transformation to the input at each particular output position preceding the corre sponding output position to generate a respective query 30 for each particular output position , apply a learned key linear transformation to each input at each particular output position preceding the corre sponding output position to generate a respective key for each particular output position , apply a learned value linear transformation to each input at each particular output position preceding the corre sponding output position to generate a respective key for each particular output position , and for each of the particular output positions preceding the 40 corresponding output position , determine a respective output - position specific weight for each particular output position by applying a comparison function between the query for the par ticular output position and the keys , and determine an initial decoder attention output for the particular output position by determining a weighted sum of the values weighted by the corresponding output - position specific weights for the particular output position . . The system of claim , wherein the decoder self attention sub - layer is configured to , at each generation time step , combine the decoder attention outputs generated by the decoder self - attention layers to generate the output for the decoder self - attention sub - layer . . The system of claim 25 , wherein the decoder attention layers operate in parallel . . The system of claim 24 wherein each decoder sub network further comprises : a residual connection layer that combines the outputs of 60 the decoder self - attention sub - layer with the inputs to the decoder self - attention sub - layer to generate a residual output , and a layer normalization layer that applies layer normaliza tion to the residual output . . One or more non - transitory computer storage media storing instructions that when executed by one or more plurality of output positions in an output order , the sequence transduction neural network comprising : an encoder neural network configured to receive the input sequence and generate a respective encoded represen tation of each of the network inputs in the input sequence , the encoder neural network comprising a sequence of one or more encoder subnetworks , each encoder subnetwork configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions , and each encoder subnetwork comprising : an encoder self - attention sub - layer that is configured to receive the subnetwork input for each of the plurality of input positions and , for each particular input position in the input order : apply a self - attention mechanism over the encoder subnetwork inputs at the plurality of input posi tions to generate a respective output for the par ticular input position , wherein applying a self attention mechanism comprises : determining a query from the subnetwork input at the particular input position , determining keys derived from the subnetwork inputs at the plurality of input posi tions , determining values derived from the sub network inputs at the plurality of input positions , and using the determined query , keys , and values to generate the respective output for the particular input position ; and a decoder neural network configured to receive the encoded representations and generate the output sequence . . A method comprising : receiving an input sequence having a respective input at each of a plurality of input positions in an input order ; processing the input sequence through an encoder neural network to generate a respective encoded representa tion of each of the inputs in the input sequence , the encoder neural network comprising a sequence of one or more encoder subnetworks , each encoder subnet work configured to receive a respective encoder sub network input for each of the plurality of input posi tions and to generate a respective subnetwork output for each of the plurality of input positions , and each an encoder self - attention sub - layer that is configured to receive the subnetwork input for each of the plurality of input positions and , for each particular input position in the input order : apply a self - attention mechanism over the encoder subnetwork inputs at the plurality of input posi tions to generate a respective output for the par ticular input position , wherein applying a self attention mechanism comprises : determining a query from the subnetwork input at the particular input position , determining keys derived from the subnetwork inputs at the plurality of input posi tions , determining values derived from the sub network inputs at the plurality of input positions , and using the determined query , keys , and values to generate the respective output for the particular input position ; and encoder subnetwork comprising : US 10 , 452 , 978 B2 processing the encoded representations through a decoder neural network to generate an output sequence having a respective output at each of a plurality of output positions in an output order .