# RAG over US Patent PDFs – Model Card 

## Basic information

- **Group members:** Jack Sullivan (j.sullivan1@gwu.edu)  
- **Date:** 2025-12-10  
- **Model version:** v1.0 (first complete RAG pipeline)
- **License:** MIT  
- **Model implementation code:**  
  - `notebooks/01_extract_text.ipynb`
  - `notebooks/02_chunk.ipynb`
  - `notebooks/03_embeddings.ipynb`
  - `notebooks/04_rag_qa.ipynb`
  https://github.com/wol-jsull/RAGPROJ/tree/main/notebooks

- **Intended use:**
  - **Intended uses:**  
    - Interactive Q&A over a small set of US patent PDFs.  
    - Helping a user identify where concepts such as LLM training, SLM augmentation, multimodal architectures, or transformer mechanisms appear in the patents.
  - **Intended users:**  
    - Students and instructors in DNSC 3288; technically savvy users exploring RAG.
  - **Out-of-scope uses:**  
    - Any legal, commercial, or production patent search, novelty analysis, or prior-art evaluation.  
    - Decisions with legal, financial, or safety consequences.

---

## Training data (for retrieval index)

- **Source of data:**  
  - 33 US patent PDFs selected for the class project sourced from Google Patents 
  - A mix of famous and msicellaneous new AI implementations patents.
  - Topics include: large/small LLM interaction, natural language generation, LLM training loops, dialogue systems, multimodal architectures, and transformer encoders.

- **How data was divided into training/validation:**  
  - No supervised training is performed, and thus data not divided into training/validation.
  - Embedding Models and OpenAI have been pretrained.
  - Either text loaded from pdfs or manually put into txt files.

- **Number of rows:**  
  - Documents (patents): **33**  
  - Text chunks for retrieval: **≈ 2905** (from `patent_chunks.jsonl`)

- **Data dictionary (columns in the chunk index):**
  - `id` – unique chunk identifier (string, e.g., `"US11562147_41"`)  
  - `patent_id` – original patent identifier (string)  
  - `chunk_index` – integer position of chunk within the patent  
  - `text` – cleaned text of the chunk (~300 words)

---

## Test data

- **Source of test data:**  
  - 8 manually written natural-language questions designed to probe core concepts in the patents (e.g., novelty, multimodal integration, resource efficiency, transformer encoder roles).

- **Number of rows:**  
  - **8** test questions.

- **Differences from training data:**  
  - The “training data” is simply the patents used for retrieval.  
  - The “test data” consists of independently written questions whose answers must be retrieved from the patent text.

---

## Model details

- **Inputs to the final system:**
  - User question (string).
  - Patent chunks (~300-word text segments).

- **Targets:**
  - A natural-language answer generated by `gpt-4.1-mini`, grounded in retrieved chunks.

- **Model types:**
  - **Embedding model:** `all-MiniLM-L6-v2` (SentenceTransformers; 384-dim vectors).  
  - **Retriever:** `NearestNeighbors` (scikit-learn), cosine similarity, `k=5`.  
  - **Generator:** `gpt-4.1-mini` via OpenAI Chat Completions API.

- **Software:**
  - Python 3.12  
  - `sentence-transformers`  
  - `numpy`  
  - `scikit-learn` 
  - `openai`

- **Hyperparameters / settings:**
  - Chunk size: **300 words**  
  - Overlap: **50 words**  
  - Embedding batch size: **64**  
  - NearestNeighbors: cosine, `n_neighbors = 5`  
  - RAG generation: `temperature = 0.2`, model = `"gpt-4.1-mini"`

---

## Quantitative analysis

Because this system is a RAG pipeline—not a classifier—traditional accuracy/AUC metrics do not apply.  
We therefore evaluate:

### **1. Retrieval Performance (Manual)**

Across **8 test questions**:

- **Retrieval Success Rate:** **100%**  
  - For every question, at least one of the top-5 retrieved chunks contained ground-truth information.

- **Relevant Chunk Rank:**  
  - In all cases, the relevant chunk appeared at **rank 0** (the top retrieved chunk).

### **2. Answer Support Levels**

| Category | Percentage | Count |
|---------|------------|-------|
| Fully supported | 12.5% | 1 |
| Mostly supported | 62.5% | 5 |
| Moderately supported | 12.5% | 1 |
| Partially supported | 12.5% | 1 |
| Weakly supported | 12.5% | 1 |

*(Multiple categories overlap because we classify per question.)*

### **3. Hallucination Levels**

| Level | Percentage | Count |
|-------|------------|--------|
| None | 12.5% | 1 |
| Mild | 75% | 6 |
| Moderate | 12.5% | 1 |
| Severe | 0% | 0 |

Most hallucinations were minor expansions of what was implied but not stated.  
One question (#6) resulted in moderate hallucination because retrieval pulled patents unrelated to multimodal inputs. Overall still well grounded at least in chunks pulled.

---

## Ethical considerations and limitations

### **Limitations of This System**

- **Small corpus**  
  Only ~33 patents; retrieval coverage is inherently limited.

- **Text extraction issues**  
  Several patents had broken or missing text due to:
  - Failing OCR  
  - Scanned PDFs  
  - One-line extraction artifacts  
  These were manually corrected, introducing **bias toward cleaner text**.

- **Embedding model size**  
  `all-MiniLM-L6-v2` is efficient but less powerful than large multilingual transformers; relevant chunks may not always be top-ranked on harder queries.

- **Generator hallucinations**  
  Even with grounding instructions, `gpt-4.1-mini` sometimes fabricated small details.

- **Not legally reliable**  
  Patent interpretation requires expert analysis; this system is purely educational.

---

## Potential negative impacts

- **Misinterpretation**  
  If someone uses this system for **real patent search**, they may receive incomplete or incorrect information.

- **False confidence in answers**  
  Mild hallucinations may look “official” even when partially unsupported.

- **Data quality biases**  
  Patents requiring manual text correction may be overrepresented in retrieval results.

---

## Uncertainties

- System robustness was tested only on **8 questions**; broader evaluation may identify cases with worse grounding.
- No adversarial tests were performed (e.g., tricky or misleading questions).
- No evaluation of long-context or multi-hop reasoning.

---

## Unexpected results

- Some patents with long formulas, diagrams, or image-heavy layouts produced **zero extractable text**, requiring manual patching. Patent text pasted in with Adobe OCR.
- Retrieval strongly favored a few patents with very dense LLM descriptions, potentially skewing results. Some patents more relevant than others.

---

## Unnecessary Summary

This RAG system successfully demonstrates how embedding-based retrieval and lightweight generation can support question answering over a small and messy patent corpus. Retrieval performance was strong, with relevant text consistently ranked first, but answer support varied, and mild hallucinations were common. The system is appropriate for **educational exploration** but should not be used for legal or production purposes.

---

## Pipeline
User Question → Embedding → Retriever (k=5) → Relevant Chunks
                                          ↓
                            Generator (gpt-4.1-mini)
                                          ↓
                               Final Answer (RAG)

---

## AI Assistance Disclaimer:

Portions of this project, including select code snippets, formatting suggestions, and text drafting, were assisted by ChatGPT (OpenAI). I reviewed, modified, and approved all generated material, and I accept all work as my own and am fully responsible for the outputs, interpretations, and decisions presented in this repository.