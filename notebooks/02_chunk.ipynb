{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc851af",
   "metadata": {},
   "source": [
    "# 02_chunk.ipynb  \n",
    "### Text Chunking for Patent Documents\n",
    "\n",
    "This notebook loads cleaned patent text files, splits them into overlapping word-based chunks, and writes the chunks to a JSONL file for downstream embedding and retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34a9b1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "Define project paths and import libraries for loading text files and writing chunk metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166881cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/sully/RAGPROJ/data/processed/txt'),\n",
       " WindowsPath('C:/Users/sully/RAGPROJ/data/processed/chunks'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "TXT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"txt\"\n",
    "CHUNK_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"chunks\"\n",
    "\n",
    "CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TXT_DIR, CHUNK_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e3663",
   "metadata": {},
   "source": [
    "## Load Cleaned Patent Texts\n",
    "Load all `.txt` files from the processed text directory into a dictionary keyed by patent ID. Empty files are skipped with a warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f746a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 text files.\n",
      "Loaded 33 non-empty documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['US10452978', 'US10740433', 'US10902563', 'US11003865', 'US11023715']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_all_patent_texts(txt_dir: Path = TXT_DIR):\n",
    "    txt_files = sorted(txt_dir.glob(\"*.txt\"))\n",
    "    print(f\"Found {len(txt_files)} text files.\")\n",
    "    \n",
    "    docs = {}\n",
    "    for txt_file in txt_files:\n",
    "        patent_id = txt_file.stem  # e.g. \"US11562147\"\n",
    "        text = txt_file.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "        if not text:\n",
    "            print(f\"WARNING: {txt_file.name} is empty\")\n",
    "            continue\n",
    "        docs[patent_id] = text\n",
    "    print(f\"Loaded {len(docs)} non-empty documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = load_all_patent_texts()\n",
    "list(docs.keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181018c",
   "metadata": {},
   "source": [
    "## Chunking Function\n",
    "Split each document into overlapping word-based chunks using a fixed window size and overlap.  \n",
    "Default: 300-word chunks with 50-word overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89b4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size_words: int = 300,\n",
    "    overlap_words: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split text into overlapping word-based chunks.\n",
    "    Returns a list of chunk strings.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    if not words:\n",
    "        return chunks\n",
    "    \n",
    "    start = 0\n",
    "    n = len(words)\n",
    "    \n",
    "    while start < n:\n",
    "        end = start + chunk_size_words\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = end - overlap_words  # step back for overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba73b29",
   "metadata": {},
   "source": [
    "## Sample Chunk Preview\n",
    "Take one example patent and view how it is split into chunks to verify the chunking behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc0be0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,\n",
       " 'US010452978B2 ( 12 ) United States Patent Shazeer et al . ( 10 ) Patent No . : US 10 , 452 , 978 B2 ( 45 ) Date of Patent : Oct . 22 , 2019 ( 54 ) ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS ) U . S . Ci . ( 71 ) Applicant : Google LLC , Mountain View , CA ( US ) ( 58 ) Field of Classifi')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id = next(iter(docs.keys()))\n",
    "sample_chunks = chunk_text(docs[sample_id])\n",
    "len(sample_chunks), sample_chunks[0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ef1c5",
   "metadata": {},
   "source": [
    "## Create Chunks for All Documents\n",
    "Apply the chunking function to every loaded patent and write the results to a JSONL file.  \n",
    "Each record includes an ID, patent ID, chunk index, and chunk text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de74b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US10452978: 60 chunks\n",
      "US10740433: 46 chunks\n",
      "US10902563: 67 chunks\n",
      "US11003865: 76 chunks\n",
      "US11023715: 164 chunks\n",
      "US11238332: 61 chunks\n",
      "US11295552: 117 chunks\n",
      "US11328398: 19 chunks\n",
      "US11562147: 44 chunks\n",
      "US11636570: 103 chunks\n",
      "US11749857: 2 chunks\n",
      "US11900261: 53 chunks\n",
      "US11921824: 51 chunks\n",
      "US11961514: 129 chunks\n",
      "US11989527: 233 chunks\n",
      "US11991338: 30 chunks\n",
      "US12148421: 125 chunks\n",
      "US12182506: 73 chunks\n",
      "US12217382: 41 chunks\n",
      "US12271791B2: 54 chunks\n",
      "US12282696B2: 301 chunks\n",
      "US20210183484A1: 70 chunks\n",
      "US20220101113A1: 256 chunks\n",
      "US20230252224A1: 165 chunks\n",
      "US20240185001A1: 84 chunks\n",
      "US20240256792A1: 91 chunks\n",
      "US20240346254A1: 48 chunks\n",
      "US8332207: 44 chunks\n",
      "US8812291: 44 chunks\n",
      "US9037464: 36 chunks\n",
      "US9740680: 47 chunks\n",
      "US_12380282_B2: 113 chunks\n",
      "US_12417081_B2: 58 chunks\n",
      "\n",
      "✅ Wrote 2905 chunks to C:\\Users\\sully\\RAGPROJ\\data\\processed\\chunks\\patent_chunks.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/sully/RAGPROJ/data/processed/chunks/patent_chunks.jsonl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_all_chunks(\n",
    "    docs: dict,\n",
    "    out_dir: Path = CHUNK_DIR,\n",
    "    chunk_size_words: int = 300,\n",
    "    overlap_words: int = 50,\n",
    "    out_filename: str = \"patent_chunks.jsonl\",\n",
    "):\n",
    "    out_path = out_dir / out_filename\n",
    "    \n",
    "    total_chunks = 0\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for patent_id, text in docs.items():\n",
    "            chunks = chunk_text(\n",
    "                text,\n",
    "                chunk_size_words=chunk_size_words,\n",
    "                overlap_words=overlap_words,\n",
    "            )\n",
    "            print(f\"{patent_id}: {len(chunks)} chunks\")\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                rec = {\n",
    "                    \"id\": f\"{patent_id}_{i}\",\n",
    "                    \"patent_id\": patent_id,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"text\": chunk,\n",
    "                }\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "                total_chunks += 1\n",
    "    \n",
    "    print(f\"\\n✅ Wrote {total_chunks} chunks to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "chunks_path = create_all_chunks(docs)\n",
    "chunks_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d00f13",
   "metadata": {},
   "source": [
    "## Inspect Sample Chunk Records\n",
    "Read a few lines from the JSONL file to confirm that the chunk structure and metadata look correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d02b5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'US10452978_0',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 0,\n",
       "  'text': 'US010452978B2 ( 12 ) United States Patent Shazeer et al . ( 10 ) Patent No . : US 10 , 452 , 978 B2 ( 45 ) Date of Patent : Oct . 22 , 2019 ( 54 ) ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS ) U . S . Ci . ( 71 ) Applicant : Google LLC , Mountain View , CA ( US ) ( 58 ) Field of Classification Search CPC . . . . . . . . . . . . . . . . . GOON 3 / 08 ( 2013 . 01 ) ; G06N 3 / 04 ( 2013 . 01 ) ; G06N 3 / 0454 ( 2013 . 01 ) CPC USPC . . . . . . . . . . . . . . . . . . . . . . . . . GOOF 3 / 015 . . . . . . 706 / 15 , 45 See application file for complete search history . ( 72 ) Inventors : Noam M . Shazeer , Palo Alto , CA ( US ) ; Aidan Nicholas Gomez , Toronto ( CA ) ; Lukasz Mieczyslaw Kaiser , Mountain View , CA ( US ) ; Jakob D . Uszkoreit , Portola Valley , CA ( US ) ; Llion Owen Jones , San Francisco , CA ( US ) ; Niki J . Parmar , Sunnyvale , CA ( US ) ; Illia Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this'},\n",
       " {'id': 'US10452978_1',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 1,\n",
       "  'text': 'Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U . S . C . 154 ( b ) by 0 days . ( 21 ) Appl . No . : 16 / 021 , 971 ( 22 ) Filed : Jun . 28 , 2018 ( 65 ) Prior Publication Data US 2018 / 0341860 A1 Nov . 29 , 2018 Related U . S . Application Data ( 63 ) Continuation of application PCT / US2018 / 034224 , filed on May 23 , 2018 . No . ( 60 ) Provisional application No . 62 / 541 , 594 , filed on Aug . , 2017 , provisional application No . 62 / 510 , 256 , filed on May 23 , 2017 . ( 51 ) Int . Cl . G06F 15 / 18 GO6N 3 / 08 GOON 3 / 04 ( 2006 . 01 ) ( 2006 . 01 ) ( 2006 . 01 ) ( 56 ) References Cited PUBLICATIONS Luong et al . , Effective Approaches to Attention - based Neural Machine Translation , ( 2015 ) Conf . on Empirical Methods in Natural Lan guage Processing at p . 1412 - 1421 ( Year : 2015 ) . * ( Continued ) Primary Examiner - David R Vincent ( 74 ) Attorney , Agent , or Firm — Fish & Richardson P . C . ( 57 ) ABSTRACT Methods , systems , and apparatus , including computer pro grams encoded on a'},\n",
       " {'id': 'US10452978_2',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 2,\n",
       "  'text': 'Year : 2015 ) . * ( Continued ) Primary Examiner - David R Vincent ( 74 ) Attorney , Agent , or Firm — Fish & Richardson P . C . ( 57 ) ABSTRACT Methods , systems , and apparatus , including computer pro grams encoded on a computer storage medium , for gener ating an output sequence from an input sequence . In one aspect , one of the systems includes an encoder neural network configured to receive the input sequence and gen erate encoded representations of the network inputs , the encoder neural network comprising a sequence of one or more encoder subnetworks , each encoder subnetwork con figured to receive a respective encoder subnetwork input for each of the input positions and to generate a respective subnetwork output for each of the input positions , and each encoder subnetwork comprising : an encoder self - attention sub - layer that is configured to receive the subnetwork input for each of the input positions and , for each particular input position in the input order : apply an attention mechanism over the encoder subnetwork inputs using one or more queries derived from the encoder subnetwork input at the particular input position . Claims , 3 Drawing Sheets Neural Network System Attention - Based Neural Network Output Sequence 1 er 176 S160 Input Sequence US 10 , 452 , 978 B2 Page 2 ( 56 ) References Cited PUBLICATIONS Cheng et al . , Long Short - Term Memory - Networks for Machine Reading ( 2016 ) , Conf . on Empirical Methods in Natural Language Processing , available from Internet < https : / / arxiv . org / abs / 1601 . > at p . 551 - 561 ( Year : 2016 ) .'},\n",
       " {'id': 'US10452978_3',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 3,\n",
       "  'text': 'Term Memory - Networks for Machine Reading ( 2016 ) , Conf . on Empirical Methods in Natural Language Processing , available from Internet < https : / / arxiv . org / abs / 1601 . > at p . 551 - 561 ( Year : 2016 ) . * Lin et al . , A Structured Self - Attentive Sentence Embedding ( Mar . ) available from Internet < https : / / arxiv . org / abs / 1703 . 03130 > , ICLR 2017 at p . 1 - 15 ( Year : 2017 ) . * Sukhbaatar et al . , End - to - End Memory Networks , ( 2015 ) available from Internet < https : / / arxiv . org / pdf / 1503 . 08895 . pdf > at p . 1 - 11 ( Year : 2015 ) . * Internet Ba et al . , Layer Normalization , ( 2016 ) available from https : / / arxiv . org / abs / 1607 . 06450 at p . 1 - 14 ( Year : 2016 ) . * Daniluk et al . , Frustratingly Short Attention Spans in Neural Lan guage Modeling , ( Feb . 2017 ) ICLR 2017 , available from Internet < https : / / arxiv . org / abs / 1702 . 04521 > at p . 1 - 10 ( Year : 2017 ) . * PCT International Search Report and Written Opinion issued in International Application No . PCT / US2018 / 034224 , dated Sep . 24 , , 14 pages . , 15 pages . , 14 pages . pages . Vaswan et al . “ Attention is All You Need , ” 31st Conference on Neural Information Processing'},\n",
       " {'id': 'US10452978_4',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 4,\n",
       "  'text': 'Opinion issued in International Application No . PCT / US2018 / 034224 , dated Sep . 24 , , 14 pages . , 15 pages . , 14 pages . pages . Vaswan et al . “ Attention is All You Need , ” 31st Conference on Neural Information Processing Systems , Jun . 12 , 2017 , arXiv1706 . Ba et al . “ Layer Normalization , ” arXiv 1607 . 06450v1 , Jul . 21 , Bahdanau et al . “ Neural Machine Translation by Jointly Learning to Align and Translate , \" arXiv 1409 . 0473v7 , mailed on May 19 , 2016 , Britz et al “ Massive exploration of neural machine translation architectures , \" arXiv 1703 . 03906v2 , Mar . 21 , 2017 , 9 pages . Cheng et al . “ Long short - term memory - networks for machine reading , \" arXiv 1601 . 06733v7 , Sep . 20 , 2016 , 11 pages . Cho et al . “ Learning phrase representations using rnn encoder decoder for statistical machine translation , ” arXiv 1406 . 1078v3 , Sep . 3 , 2014 , 15 pages . Chollet . “ Xception : Deep Learning with depthwise separable con volution , ” arXiv 1610 . 02357v3 , Apr . 4 , 2017 , 8 pages . Chung et al . “ Empirical evaluation of gated recurrent neural net works on sequence modeling , \" arXiv 1412 . 3555v1 , Dec . 11 , 2014 , Gehring et al . “ Convolutional sequence to sequence learning , \" arXiv 1705 . 03122v2 , May 12 , 2017 , 15 pages . pages . Hochreiter et al . “ Gradient flow in recurrent nets : the difficulty of learning long - term dependencies'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lines = []\n",
    "\n",
    "with chunks_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(5):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        sample_lines.append(json.loads(line))\n",
    "\n",
    "sample_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4ddf7",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Compute the number of documents and total number of chunks created.  \n",
    "These values are useful for documentation and the model card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe66ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 33\n",
      "Total chunks: 2905\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(docs)\n",
    "print(\"Documents:\", n_docs)\n",
    "\n",
    "all_chunks = []\n",
    "# or just len from chunks file:\n",
    "import json, pathlib\n",
    "path = pathlib.Path(\"../data/processed/chunks/patent_chunks.jsonl\")\n",
    "with path.open() as f:\n",
    "    all_chunks = [json.loads(l) for l in f]\n",
    "print(\"Total chunks:\", len(all_chunks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
