{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc6fd82",
   "metadata": {},
   "source": [
    "# 04_rag_qa.ipynb  \n",
    "### Retrieval-Augmented Question Answering Over Patent Corpus\n",
    "\n",
    "This notebook loads the embeddings and metadata created earlier, rebuilds a nearest neighbor index, and performs Retrieval-Augmented Generation (RAG) using an OpenAI model. The notebook concludes with a manual evaluation of eight test questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987936f8",
   "metadata": {},
   "source": [
    "## Load Embeddings and Metadata\n",
    "\n",
    "We load two files created in notebook 03:\n",
    "\n",
    "embeddings.npy — dense vector representations of all  — chunk_metadata.jsonl - patent IDs, chunk indices, and the text of each chunk  \n",
    "\n",
    "These are used to rebuild the retrieval system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce14dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/sully/RAGPROJ/embeddings/embeddings.npy'),\n",
       " WindowsPath('C:/Users/sully/RAGPROJ/embeddings/chunk_metadata.jsonl'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from openai import OpenAI  # OpenAI Python SDK\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "EMB_DIR = PROJECT_ROOT / \"embeddings\"\n",
    "\n",
    "EMB_PATH = EMB_DIR / \"embeddings.npy\"\n",
    "META_PATH = EMB_DIR / \"chunk_metadata.jsonl\"\n",
    "\n",
    "EMB_PATH, META_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9a132",
   "metadata": {},
   "source": [
    "## Load Embeddings and Metadata\n",
    "\n",
    "We load the embedding matrix (`embeddings.npy`) and the chunk metadata (`chunk_metadata.jsonl`) created in Notebook 03.  \n",
    "These files contain:\n",
    "\n",
    "- Vector embeddings for each chunk  \n",
    "- Patent ID and chunk text for each embedding  \n",
    "\n",
    "This step prepares the retrieval index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00239d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (2905, 384)\n",
      "Loaded metadata records: 2905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'US10452978_0',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 0,\n",
       "  'text': 'US010452978B2 ( 12 ) United States Patent Shazeer et al . ( 10 ) Patent No . : US 10 , 452 , 978 B2 ( 45 ) Date of Patent : Oct . 22 , 2019 ( 54 ) ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS ) U . S . Ci . ( 71 ) Applicant : Google LLC , Mountain View , CA ( US ) ( 58 ) Field of Classification Search CPC . . . . . . . . . . . . . . . . . GOON 3 / 08 ( 2013 . 01 ) ; G06N 3 / 04 ( 2013 . 01 ) ; G06N 3 / 0454 ( 2013 . 01 ) CPC USPC . . . . . . . . . . . . . . . . . . . . . . . . . GOOF 3 / 015 . . . . . . 706 / 15 , 45 See application file for complete search history . ( 72 ) Inventors : Noam M . Shazeer , Palo Alto , CA ( US ) ; Aidan Nicholas Gomez , Toronto ( CA ) ; Lukasz Mieczyslaw Kaiser , Mountain View , CA ( US ) ; Jakob D . Uszkoreit , Portola Valley , CA ( US ) ; Llion Owen Jones , San Francisco , CA ( US ) ; Niki J . Parmar , Sunnyvale , CA ( US ) ; Illia Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this'},\n",
       " {'id': 'US10452978_1',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 1,\n",
       "  'text': 'Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U . S . C . 154 ( b ) by 0 days . ( 21 ) Appl . No . : 16 / 021 , 971 ( 22 ) Filed : Jun . 28 , 2018 ( 65 ) Prior Publication Data US 2018 / 0341860 A1 Nov . 29 , 2018 Related U . S . Application Data ( 63 ) Continuation of application PCT / US2018 / 034224 , filed on May 23 , 2018 . No . ( 60 ) Provisional application No . 62 / 541 , 594 , filed on Aug . , 2017 , provisional application No . 62 / 510 , 256 , filed on May 23 , 2017 . ( 51 ) Int . Cl . G06F 15 / 18 GO6N 3 / 08 GOON 3 / 04 ( 2006 . 01 ) ( 2006 . 01 ) ( 2006 . 01 ) ( 56 ) References Cited PUBLICATIONS Luong et al . , Effective Approaches to Attention - based Neural Machine Translation , ( 2015 ) Conf . on Empirical Methods in Natural Lan guage Processing at p . 1412 - 1421 ( Year : 2015 ) . * ( Continued ) Primary Examiner - David R Vincent ( 74 ) Attorney , Agent , or Firm — Fish & Richardson P . C . ( 57 ) ABSTRACT Methods , systems , and apparatus , including computer pro grams encoded on a'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embeddings_and_metadata(emb_path: Path, meta_path: Path):\n",
    "    embeddings = np.load(emb_path)\n",
    "    chunks = []\n",
    "    with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            chunks.append(json.loads(line))\n",
    "    print(f\"Loaded embeddings: {embeddings.shape}\")\n",
    "    print(f\"Loaded metadata records: {len(chunks)}\")\n",
    "    return embeddings, chunks\n",
    "\n",
    "embeddings, chunks = load_embeddings_and_metadata(EMB_PATH, META_PATH)\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5224f",
   "metadata": {},
   "source": [
    "## Build Nearest Neighbor Index\n",
    "\n",
    "Using the embeddings, we build a `sklearn.neighbors.NearestNeighbors` index with cosine distance.  \n",
    "This index allows fast retrieval of the most relevant chunks for any query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6654e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_index(embeddings: np.ndarray, n_neighbors: int = 5):\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "    nn.fit(embeddings)\n",
    "    return nn\n",
    "\n",
    "nn_index = build_nn_index(embeddings, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8093756",
   "metadata": {},
   "source": [
    "## Search Function\n",
    "\n",
    "The `search_chunks` function embeds a query, retrieves the nearest chunks from the index, and returns:\n",
    "\n",
    "- Rank  \n",
    "- Similarity score  \n",
    "- Patent ID  \n",
    "- Chunk index  \n",
    "- Full text of the chunk  \n",
    "\n",
    "This is the retrieval component of the RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de215854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(\n",
    "    query: str,\n",
    "    embed_model,          # SentenceTransformer model from 03, or reload it\n",
    "    nn_index,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    # embed query\n",
    "    q_emb = embed_model.encode([query])\n",
    "    \n",
    "    # retrieve\n",
    "    distances, indices = nn_index.kneighbors(q_emb, n_neighbors=top_k)\n",
    "    \n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "        rec = chunks[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": 1 - float(dist),  # cosine similarity approx\n",
    "            \"id\": rec[\"id\"],\n",
    "            \"patent_id\": rec[\"patent_id\"],\n",
    "            \"chunk_index\": rec[\"chunk_index\"],\n",
    "            \"text\": rec[\"text\"],\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece10523",
   "metadata": {},
   "source": [
    "## Load Embedding Model and Test Retrieval\n",
    "\n",
    "We load the same sentence transformer used earlier (`all-MiniLM-L6-v2`)  \n",
    "and perform a test query to confirm that chunk retrieval works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9615875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sully\\RAGPROJ\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f169ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] US12148421 (sim=0.580)\n",
      "part of a dialog session between a user of a client device and an automated assistant implemented by the client device: receiving a stream of audio data that captures a spoken utterance ofthe user, the stream of audio data being generated by one or more microphones of the client device, and the spok ...\n",
      "\n",
      "[1] US11562147 (sim=0.577)\n",
      "an utterance of the human user in the dialogue history or a language model response. 14. The system of claim 11, wherein a position level encoding layer from the plurality of text encoding layers generates the position level encoding, wherein the position level encoding identifies a token ordering i ...\n",
      "\n",
      "[2] US11562147 (sim=0.575)\n",
      "visual dialogue model 55 receives the image 110, the dialogue history 120 and the question 130 as input and generates the answer 150 base on the received input. Prior approaches have attempted to implement visual dialogue, where a dialogue machine agent is tasked to answer a series of questions grou ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"How does this invention handle language model dialogue?\"\n",
    "hits = search_chunks(test_query, embed_model, nn_index, embeddings, chunks, top_k=3)\n",
    "\n",
    "for h in hits:\n",
    "    print(f\"[{h['rank']}] {h['patent_id']} (sim={h['score']:.3f})\")\n",
    "    print(h[\"text\"][:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5ee55",
   "metadata": {},
   "source": [
    "## Initialize OpenAI Client\n",
    "\n",
    "We verify that the `OPENAI_API_KEY` is set in the environment and instantiate the OpenAI SDK client.  \n",
    "This will be used for the generation step of the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aed2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "# Note will not include output\n",
    "# Because I would prefer my SK key to be private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e31f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # uses OPENAI_API_KEY env var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eb75e",
   "metadata": {},
   "source": [
    "## Build Context String\n",
    "\n",
    "The retrieved chunks are formatted into a structured text block  \n",
    "that becomes the context for the RAG answer generation.  \n",
    "Each chunk includes:\n",
    "\n",
    "- Patent ID  \n",
    "- Chunk index  \n",
    "- Similarity score  \n",
    "- Chunk text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cecb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_string(retrieved_chunks):\n",
    "    pieces = []\n",
    "    for r in retrieved_chunks:\n",
    "        header = f\"[{r['patent_id']} | chunk {r['chunk_index']} | score={r['score']:.3f}]\"\n",
    "        pieces.append(header + \"\\n\" + r[\"text\"])\n",
    "    return \"\\n\\n---\\n\\n\".join(pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4108a9",
   "metadata": {},
   "source": [
    "## RAG Answer Function\n",
    "\n",
    "This function performs the full RAG workflow:\n",
    "\n",
    "1. Retrieve top-k relevant chunks  \n",
    "2. Construct a combined context string  \n",
    "3. Send a prompt to the OpenAI model with a strict instruction  \n",
    "4. Return:\n",
    "   - The generated answer  \n",
    "   - Retrieved chunks (for evaluation)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea67f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(\n",
    "    question: str,\n",
    "    embed_model,\n",
    "    nn_index,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    client,\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    # 1. Retrieve\n",
    "    retrieved = search_chunks(\n",
    "        question,\n",
    "        embed_model,\n",
    "        nn_index,\n",
    "        embeddings,\n",
    "        chunks,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    context = build_context_string(retrieved)\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant answering questions about a small set of US patents. \"\n",
    "        \"Answer the user's question using ONLY the information in the provided context. \"\n",
    "        \"If the answer is not in the context, say you don't know based on these documents.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        f\"Context (patent chunks):\\n{context}\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53d25",
   "metadata": {},
   "source": [
    "## Example RAG Call\n",
    "\n",
    "We run the RAG pipeline on a sample question to verify that the  \n",
    "end-to-end system (retrieval + generation) works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa611ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "What is the main novelty of these inventions related to language model training or dialogue systems?\n",
      "\n",
      "ANSWER:\n",
      "The main novelties of these inventions related to language model training or dialogue systems are:\n",
      "\n",
      "1. **Dataset Generation Using Large Language Models (US20240185001A1)**: This invention introduces a system and technique for generating training datasets for task-oriented dialogue systems by combining template queries with domain-specific tokens sampled from a data store. A large language model then generates natural language queries based on these prompts, which are used to train conversational machine-learning models specialized for domain-specific tasks. This approach automates and enhances the creation of relevant training data tailored to specific conversational domains, reducing reliance on human-generated data.\n",
      "\n",
      "2. **Natural Language Training and Augmentation with Large Language Models (US20240346254A1)**: This invention describes methods where a large language model is used to improve natural language generation systems by training and augmenting smaller language models. The large language model processes training datasets to produce outputs, which are analyzed to train a natural language generation system that mimics the large model's output. The large model then evaluates and iteratively improves this system. Additionally, the large model provides external information to augment smaller models, enhancing their contextual understanding and output quality.\n",
      "\n",
      "3. **Visual Dialogue Model Training (US11562147)**: This invention focuses on training visual dialogue models that can engage in meaningful conversations about visual content. It supports both discriminative and generative training settings without requiring large-scale external vision-language datasets, enabling improved performance and transfer learning capabilities in visual dialogue tasks.\n",
      "\n",
      "4. **Language-Based Learning and Reasoning (US11989527)**: This invention proposes a novel approach where learning is represented as concepts and ideas expressed in natural language (rather than traditional statistical model weights). The system stores learned knowledge in a form translatable into natural language, enabling reasoning, explanation, and conversational interaction with users. Learning sources include user-provided natural language and reasoning chains, facilitating a more interpretable and interactive dialogue system.\n",
      "\n",
      "In summary, these inventions collectively advance language model training and dialogue systems by automating domain-specific dataset generation, leveraging large language models to train and augment smaller models, enabling effective visual dialogue without massive datasets, and introducing language-based learning representations that support reasoning and explainability in conversational AI.\n",
      "\n",
      "\n",
      "RETRIEVED CHUNKS (for debugging):\n",
      "- US20240185001A1 chunk 0 (sim=0.575)\n",
      "- US20240185001A1 chunk 5 (sim=0.537)\n",
      "- US20240346254A1 chunk 0 (sim=0.530)\n",
      "- US11562147 chunk 4 (sim=0.524)\n",
      "- US11989527 chunk 85 (sim=0.521)\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the main novelty of these inventions related to language model training or dialogue systems?\"\n",
    "\n",
    "answer, retrieved = rag_answer(\n",
    "    question,\n",
    "    embed_model,\n",
    "    nn_index,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    client,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "print(\"QUESTION:\")\n",
    "print(question)\n",
    "print(\"\\nANSWER:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\\nRETRIEVED CHUNKS (for debugging):\")\n",
    "for r in retrieved:\n",
    "    print(f\"- {r['patent_id']} chunk {r['chunk_index']} (sim={r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63f8e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "- US20240185001A1 | chunk 0 | sim=0.575\n",
      "US 20240185001A1 (19) United States (12) Patent Application Publication Nagaraju et al. (54) DATASET GENERATION USING LARGE LANGUAGE MODELS (71) Applicant: NVIDIA Corporation, Santa Clara, CА (US) (72) Inventors: Divija Nagaraju, Mountain View, СА (US); Christopher Parisien, Toronto (CA) (21) Appl.  ...\n",
      "\n",
      "- US20240185001A1 | chunk 5 | sim=0.537\n",
      "that were previously performed by humans. In addition to designing efficient and effective machine-learning model (MLM) architectures, the successful deployment or application of the MLMs also depends heavily on the training techniques employed. For example, training an MLM to perform a specific tas ...\n",
      "\n",
      "- US20240346254A1 | chunk 0 | sim=0.530\n",
      "(19) United States (12) Patent Application Publication LIU et al. (54) NATURAL LANGUAGE TRAINING AND/OR AUGMENTATION WITH LARGE LANGUAGE MODELS (71) Applicant: MICROSOFT TECHNOLOGY LICENSING, LLC, Redmond, WA (US) (72) Inventors: Yang LIU, Bellevue, WA (US); Yichong XU, Bellevue, WA (US); Dan ITER,  ...\n",
      "\n",
      "- US11562147 | chunk 4 | sim=0.524\n",
      "both discriminative and generative settings whereas the prior approaches in visual dialogue are restricted to only pretraining with discriminative settings, and 2) not requiring to pretrain on large-scale external vision-language datasets as opposed to the prior approaches with inferior performance  ...\n",
      "\n",
      "- US11989527 | chunk 85 | sim=0.521\n",
      "to be detrimental from this test process can be ignored for production use. Learning Examples of the present invention including examples implementing any of the applications described herein or other applications can learn, representing what they have learned in UL (or similar) and then utilising t ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nRETRIEVED CHUNKS:\")\n",
    "for r in retrieved:\n",
    "    print(f\"- {r['patent_id']} | chunk {r['chunk_index']} | sim={r['score']:.3f}\")\n",
    "    print(r['text'][:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a9f31",
   "metadata": {},
   "source": [
    "## Evaluation Questions\n",
    "\n",
    "We define a set of eight evaluation questions designed to test retrieval grounding  \n",
    "across themes such as model training, multimodal processing, dialogue systems,  \n",
    "and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8118113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is the main innovation described in these patents for improving natural language model training or generation?\",\n",
    "    \"How do the patents describe combining large language models with smaller models for improved efficiency?\",\n",
    "    \"What role does a transformer encoder play in the systems described?\",\n",
    "    \"How is dialogue history or multi-turn conversation handled within the patented architectures?\",\n",
    "    \"Do the patents describe any specialized pretraining objectives or methods for tuning language models?\",\n",
    "    \"What mechanisms do the patents propose for integrating multimodal inputs such as text and images?\",\n",
    "    \"How do the patents describe improving resource efficiency or reducing computational cost in language model processing?\",\n",
    "    \"What system components or modules are mentioned as contributing to natural language generation quality?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811c6f2",
   "metadata": {},
   "source": [
    "## Run Evaluation Questions\n",
    "\n",
    "This helper function runs RAG for each evaluation question  \n",
    "and prints detailed retrieval outputs.  \n",
    "Manual scoring will occur afterward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48acd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUESTION 1: What is the main innovation described in these patents for improving natural language model training or generation?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The main innovation described in these patents for improving natural language model training or generation involves leveraging large language models (LLMs) to enhance both training and augmentation of smaller natural language generation systems. Specifically, as detailed in US20240346254A1, the approach includes:\n",
      "\n",
      "1. Using a large language model to process a training dataset and produce natural language outputs.\n",
      "2. Having the natural language generation system analyze both the training data and the LLM's output to generate its own outputs that mimic the LLM.\n",
      "3. Employing the large language model to evaluate the generation system's outputs and iteratively adjust and improve their quality through a feedback loop.\n",
      "4. Augmenting smaller language models by retrieving external information via the large language model to provide additional context and a language framework, thereby enhancing the smaller model's overall output quality.\n",
      "\n",
      "Additionally, other patents (e.g., US20230252224A1) describe innovations in prompt engineering and the use of generative AI to streamline complex document drafting tasks such as patent applications, enabling more efficient, accurate, and user-friendly generation of professional-quality text.\n",
      "\n",
      "In summary, the key innovation is the integration of large language models as both trainers and augmenters of smaller models, combined with iterative evaluation and feedback, as well as advanced prompt engineering, to significantly improve the quality, efficiency, and contextual relevance of natural language generation and training processes.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US20240346254A1 chunk=0 sim=0.701\n",
      "(19) United States (12) Patent Application Publication LIU et al. (54) NATURAL LANGUAGE TRAINING AND/OR AUGMENTATION WITH LARGE LANGUAGE MODELS (71) Applicant: MICROSOFT TECHNOLOGY LICENSING, LLC, Redmond, WA (US) (72) Inventors: Yang LIU, Bellevue, WA (US); Yichong XU, Bellevue, WA (US); Dan ITER, Austin, TX (US); Chenguang ZHU, Bellevue, WA (US); Nanshan ZENG, Bellevue, WA (US); Shuohang WANG. B...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US_12417081_B2 chunk=0 sim=0.621\n",
      "US012417081B2 (12) United States Patent Ferrucci et al. (54) MACHINE-LEARNING ASSISTED NATURAL LANGUAGE PROGRAMMING SYSTЕM (71) Applicant: Elemental Cognition Inc., New York, NY (US) (72) Inventors: David A. Ferrucci, Wilton, CT (US); Marcello Balduccini, Wynnewood, PA (US); Andrew E. Beck, Westport, CT (US); Gregory Burnham, Brooklyn, NY (US); Gregory Gelfond, Omaha, NE (US); Clifton James McFate...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US20230252224A1 chunk=150 sim=0.611\n",
      "and tell, Provide Quality data, and Change settings. [0441] The patent text generation can be done in one of three ways: [0442] Zero-shot Learning: No examples given for training. [0443] One-shot Learning: This is the only example for the training purpose [0444] Few-shot learning: a few examples are provided to train the transformer model with the prompt. [0445] The prompt-engineering guides the t...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US_12380282_B2 chunk=0 sim=0.597\n",
      "US012380282B2 (12) United States Patent Leary et al. (54) NATURAL LANGUAGE PROCESSING APPLICATIONS USING LARGE LANGUAGE MODELS (71) Applicant: Nvidia Corporation, Santa Clara, CA (US) (72) Inventors: Ryan Leary, Woodstock, GA (US); Jonathan Cohen, Mountain View, CA (US) (73) Assignee: Nvidia Corporation, Santa Clara, CA (US) (*) Notice: Subject to any disclaimer, the term ofthis patent is extended...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US20230252224A1 chunk=152 sim=0.587\n",
      "language models can be used to generate text based on a set of input parameters, such as invention type and technical field. This can be useful in generating initial drafts of a patent application, or in generating sections of the patent application, such as the background section or the description of the invention. [0457] The document drafting software leverages the power of its innovative langu...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 2: How do the patents describe combining large language models with smaller models for improved efficiency?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The patents describe combining large language models (LLMs) with smaller language models (SMLs) for improved efficiency by leveraging the strengths of both types of models. Specifically:\n",
      "\n",
      "1. Large language models, while powerful and capable of understanding vague contexts and mimicking human intuition and domain knowledge, are resource-intensive and complex to train and deploy, especially in specific domains.\n",
      "\n",
      "2. Smaller language models consume fewer resources and offer greater agility and adaptability but lack the contextual intelligence of large models.\n",
      "\n",
      "3. To combine these advantages, the disclosed techniques use large language models to train and/or augment small language models. This can be done in two main ways:\n",
      "\n",
      "   - Training: The large language model processes a training dataset to produce natural language outputs. The small language model then learns to mimic these outputs through imitation learning and reinforcement learning, with the large model evaluating and iteratively improving the small model's outputs.\n",
      "\n",
      "   - Augmentation: The large language model retrieves external information and generates augmentation inputs that provide additional context and a language framework. These inputs are supplied to the small language model to enhance its natural language task performance.\n",
      "\n",
      "This approach allows the small language model to benefit from the large model's contextual understanding and domain knowledge while maintaining lower resource consumption and greater efficiency.\n",
      "\n",
      "References: US20240346254A1 chunks 0 and 5.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US20240346254A1 chunk=0 sim=0.587\n",
      "(19) United States (12) Patent Application Publication LIU et al. (54) NATURAL LANGUAGE TRAINING AND/OR AUGMENTATION WITH LARGE LANGUAGE MODELS (71) Applicant: MICROSOFT TECHNOLOGY LICENSING, LLC, Redmond, WA (US) (72) Inventors: Yang LIU, Bellevue, WA (US); Yichong XU, Bellevue, WA (US); Dan ITER, Austin, TX (US); Chenguang ZHU, Bellevue, WA (US); Nanshan ZENG, Bellevue, WA (US); Shuohang WANG. B...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US20240346254A1 chunk=5 sim=0.571\n",
      "language models can also incur technical costs that may be detrimental in various usage scenarios. For instance, a provider may wish to deploy a large language model in a specific natural language domain (e.g., records management). Unfortunately, due to the high complexity of large language models, deploying a large language model in such a specific context can be impractical. For example, trainin...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US8332207 chunk=3 sim=0.553\n",
      "11, 2008 the whole document. Placeway, et al., “The Estimation of Powerful Language Models From Small and Large Corpora'. Plenary, Special, Audio, Underwa ter Acoustics, VLSI, Neural Networks. Minneapolis, Apr. 27-30, 1993; Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)), NewYork, IEEE, US, vol. 2, Apr. 27, 1993, pp. 33-36, XPO 10110386, ISBN: 978-...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US8332207 chunk=1 sim=0.546\n",
      "filed on Mar. 26, 2007. P.L.C. (57) (51) Int. Cl. G06F 17/27 (2006.01) (52) U.S. Cl. ............ 7949; 7025; 704,25570425 704,240, 704/2; 704/10; 704/1:370/270 (58) Field of Classification Search ................ 704/1, 10, 704/240, 257, 255, 251, 2: 707/999.102; See application file for complete search history. 370/270 (56) References Cited U.S. PATENT DOCUMENTS ABSTRACT Systems, methods, and co...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US9740680 chunk=3 sim=0.542\n",
      "- 1 ) Word Store Word ( t + 1 ) Word Store - Word ( t + N ) US 9 , 740 , 680 B1 Page 2 ( 56 ) References Cited U . S . PATENT DOCUMENTS , 721 , 702 B2 * / 2004 Schneider , 406 , 417 B1 * / 2008 Hain , 566 , 102 B1 / 0262467 Al / 2013 Bangalore et al . / 2013 Zhang et al . GIOL 15 / 22 / 231 GIOL 13 / 08 / 260 OTHER PUBLICATIONS Bengio et al . , “ A neural probabilistic language model , ” Journal o...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 3: What role does a transformer encoder play in the systems described?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The transformer encoder in the described systems serves to process input data by applying multiple layers of neural network components, including self-attention mechanisms and feed-forward neural networks, to generate encoded representations of the input. Specifically:\n",
      "\n",
      "- The encoder takes sequential input data (such as tokens from natural language, event-based performance encodings, or image patches) and applies self-attention layers to capture relationships and dependencies within the input sequence. This allows the model to weigh the significance of different parts of the input relative to each other.\n",
      "\n",
      "- In language models, the encoder processes tokenized input text, learning position embeddings and contextual relevance, producing hidden representations that capture semantic and syntactic information.\n",
      "\n",
      "- In music generation systems, the encoder (e.g., a concept encoder and a melody encoder) encodes different aspects of the input performance and melody, producing intermediate representations that are aggregated and fed into the decoder for generation tasks.\n",
      "\n",
      "- In image classification (e.g., Vision Transformer or ViT), the encoder transforms image patches (tokens) with positional embeddings through multiple self-attention layers to embed global and local information across the image, enabling classification.\n",
      "\n",
      "- The encoder output is a set of hidden representations that can be passed to a decoder in encoder-decoder architectures or used directly for classification or other downstream tasks.\n",
      "\n",
      "Overall, the transformer encoder's role is to transform input sequences into rich, context-aware encoded representations that capture relevant features and dependencies, which can then be used by subsequent components (such as decoders or classifiers) to perform tasks like generation, classification, or prediction.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US12182506 chunk=28 sim=0.644\n",
      "are types of transformers. A transformer is a type of neural network architecture that uses self-attention mechanisms in order to generate predicted output based on input data that has some sequential meaning (i.e., the order of the input data is meaningful, which is the 50 case for most text input). Although transformer-based language models are described herein, it should be understood that the ...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US20230252224A1 chunk=56 sim=0.639\n",
      "to embed the respective inputs. These two intermediate representations of melody and performance are then aggregated to form a single vector input into the decoder. [0187] Instead of using self-attention to operate over absolute positional encodings of each token in a given sequence done in one embodiment, the preferred embodiment's Transformer replaces this mechanism with relative attention and a...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US11921824 chunk=10 sim=0.630\n",
      "encoder may learn a position embedding for each token in a frame of input data (e.g., 2D input data and/or 3D input data), and a position embedding for each One set of (Wo, WK. W,) matrices is referred to herein as 50 an attention head, and each layer in a transformer model has multiple attention heads. While one attention head attends to the tokens that are relevant to each token, with multiple a...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US12282696B2 chunk=159 sim=0.607\n",
      "The result, with the position embedding is fed to the transformer. As in the case of BERT, a fundamental role in classification tasks is played by the class token. A special token that is used as the only input of the final MLP Head as it has been influenced by all the others. The architecture for image classification is the most common and uses only the Transformer Encoder in order to transform t...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US11900261 chunk=28 sim=0.601\n",
      "encoder-decoder neural transformer model, the first encoder block of the neural transformer model takes the context tensor as input and passes it through the multiple layers of multi-head attention, layer normalization and feedforward neural network to finally produce a set of hidden representations If there are additional encoder blocks, the output of each encoder block is passed onto the next en...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 4: How is dialogue history or multi-turn conversation handled within the patented architectures?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The patented architectures handle dialogue history or multi-turn conversation by maintaining and utilizing the context of the dialog session across turns. Specifically:\n",
      "\n",
      "- Dialog sessions are generally turn-based, where the user provides a spoken utterance and the automated assistant responds, and this sequence continues for multiple turns (US12148421 chunk 8). However, the system aims to make the conversation more natural by considering the context of prior utterances in formulating responses, similar to how humans keep track of the conversation flow.\n",
      "\n",
      "- Dialogue history can include multiple turns, each consisting of a pair of a human utterance and the corresponding machine response (US11562147 chunk 12). This dialogue history is used as input along with the current human utterance to generate a coherent and contextually relevant response.\n",
      "\n",
      "- The system processes the stream of audio data capturing the user's spoken utterance and generates assistant outputs responsive to that utterance. It then processes these outputs along with the dialog session context to generate modified assistant outputs using large language model (LLM) outputs that are based on the context of the dialog session and the assistant outputs (US12148421 chunk 96).\n",
      "\n",
      "- The system can generate additional assistant queries related to the spoken utterance based on the dialog context, and further process these to produce additional modified assistant outputs, thereby maintaining continuity and coherence across multiple turns (US12148421 chunk 96).\n",
      "\n",
      "- Dialogue history and context are indexed and stored, potentially including embeddings of assistant queries, to facilitate retrieval and use in generating responses in ongoing dialog sessions (US12148421 chunk 112).\n",
      "\n",
      "In summary, the architectures maintain a structured dialogue history of turns, use this history as context for processing current utterances, employ LLMs to generate context-aware responses, and manage indexing of dialogue context to support multi-turn, natural conversations.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US12148421 chunk=8 sim=0.469\n",
      "of spoken utterances directed to the automated assistant and wasting computational resources of a client device utilized in processing these a spoken utterance, and the 30 automated assistant can respond to the spoken utterance using the aforementioned pipeline of components. The user can continue the dialog session by providing an additional spoken utterance, and the automated assistant can respo...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US11562147 chunk=12 sim=0.461\n",
      "visual communication (e.g., gestures), and/or the like. In response, computing device 300 may provide one or more system responses (e.g., providing a response dialogue to user 360, performing a task on behalf of user 360, requesting additional information, and/or the like). In some embodiments, the computing device 300 may receive input that may include image (e.g., the image 350) and text data (e...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US11989527 chunk=131 sim=0.457\n",
      "command that the user directs to them. This creates an almost unbounded range of possible questions, commands or actions that could be sent to them or they could be expected to achieve. Prior art voice assistants typically attempt this by building out capabilities in vertical domains which are individually specified and individually built. For example, a typical domain in a voice assistant might b...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US12148421 chunk=112 sim=0.440\n",
      "current dialog session that corresponds to the corresponding context of the corresponding prior dialog session for the given assistant query; and causing the automated assistant to utilize the corresponding LLM output in generating assistant output to be provided for presentation to the user responsive to the spoken utterance. These and other implementations of technology disclosed herein can opti...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US12148421 chunk=96 sim=0.430\n",
      "that a particular geographic location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and/or used. In some implementations, a method implemented by one or more processors is provided, and includes, as part of a dialog session between a user of a client device and an automated assistant implemented by the client device: receiving a st...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 5: Do the patents describe any specialized pretraining objectives or methods for tuning language models?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "Yes, the patents describe specialized pretraining objectives and methods for tuning language models. Specifically, US20240346254A1 discusses using a large language model (LLM) to train and/or augment smaller language models (SMLs) through imitation learning and reinforcement learning. The LLM processes a training dataset to produce natural language outputs, which the smaller model then mimics. The LLM evaluates the smaller model's outputs and iteratively adjusts to improve output quality. This approach leverages the benefits of both large and small models, addressing challenges such as resource consumption and domain-specific adaptation. Additionally, US12182506 mentions fine-tuning a trained ML model by further training on task-specific data samples to better model a specific task, which is a method of tuning language models. US20240256792A1 also discusses transformer-based language models and their training but does not specify unique pretraining objectives beyond general transformer architectures.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US20240346254A1 chunk=0 sim=0.543\n",
      "(19) United States (12) Patent Application Publication LIU et al. (54) NATURAL LANGUAGE TRAINING AND/OR AUGMENTATION WITH LARGE LANGUAGE MODELS (71) Applicant: MICROSOFT TECHNOLOGY LICENSING, LLC, Redmond, WA (US) (72) Inventors: Yang LIU, Bellevue, WA (US); Yichong XU, Bellevue, WA (US); Dan ITER, Austin, TX (US); Chenguang ZHU, Bellevue, WA (US); Nanshan ZENG, Bellevue, WA (US); Shuohang WANG. B...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US12182506 chunk=24 sim=0.516\n",
      "in real-world applications (also referred to as \"inference\"). In some examples, a trained ML model may be fine-tuned, meaning that the values of the learned parameters may be adjusted slightly in order for the ML model to better model a specific task. Fine-tuning of a ML model typically involves further training the ML model on a number of data samples (which may be smaller in number/cardinality t...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US8332207 chunk=1 sim=0.508\n",
      "filed on Mar. 26, 2007. P.L.C. (57) (51) Int. Cl. G06F 17/27 (2006.01) (52) U.S. Cl. ............ 7949; 7025; 704,25570425 704,240, 704/2; 704/10; 704/1:370/270 (58) Field of Classification Search ................ 704/1, 10, 704/240, 257, 255, 251, 2: 707/999.102; See application file for complete search history. 370/270 (56) References Cited U.S. PATENT DOCUMENTS ABSTRACT Systems, methods, and co...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US20240346254A1 chunk=5 sim=0.497\n",
      "language models can also incur technical costs that may be detrimental in various usage scenarios. For instance, a provider may wish to deploy a large language model in a specific natural language domain (e.g., records management). Unfortunately, due to the high complexity of large language models, deploying a large language model in such a specific context can be impractical. For example, trainin...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US20240256792A1 chunk=20 sim=0.494\n",
      "in ML-based language models are now discussed. It may be noted that, while the term \"language model\" has been commonly used to refer to a MLbased language model, there could exist non-ML language models. In the present disclosure, the term \"language model\" may be used as shorthand for ML-based language model (i.e., a language model that is implemented using a neural network or other ML architectur...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 6: What mechanisms do the patents propose for integrating multimodal inputs such as text and images?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The patents propose mechanisms for integrating multimodal inputs such as text and images primarily through the use of Artificial Neural Networks (ANNs) and Deep Neural Networks (DNNs), including Vision Transformer (ViT) models. Specifically:\n",
      "\n",
      "1. Semantic Appearance Transfer (US12282696B2): This patent describes capturing multiple images (first and second images) that are semantically related but have different appearances. These images are processed by appearance analyzers to generate outputs representing the appearances. A comparator compares these outputs, and a generator (using an ANN) is trained to minimize differences between these outputs, effectively transferring semantic appearance from one image to another. This process involves analyzing and integrating different visual inputs to generate a coherent output.\n",
      "\n",
      "2. Expression Recognition (US11023715): This patent discusses processing both three-dimensional and two-dimensional images of a target face, performing transformations and alignments based on feature points, and feeding color information into a neural network. Although it focuses on image inputs, the processing involves aligning and transforming multimodal image data for recognition tasks.\n",
      "\n",
      "3. Use of Vision Transformer (ViT) Models (US12282696B2): The detailed description mentions training generators for semantic appearance transfer using combined appearance/structure analyzers and specifically using DINO-ViT models as analyzers. Vision Transformers are known for their ability to process image data effectively and can be integrated with other modalities.\n",
      "\n",
      "4. Natural Language Processing Applications Using Large Language Models (US12380282B2): While the detailed mechanisms are not fully disclosed in the provided text, this patent relates to natural language processing using large language models, which suggests integration of text inputs. The combination of this with image processing patents indicates a framework where large language models and vision models (like ViT) could be combined for multimodal input processing.\n",
      "\n",
      "In summary, the patents propose integrating multimodal inputs by employing neural network architectures that analyze and compare representations of different modalities (e.g., images with different appearances or images and text), training generators to minimize differences between these representations, and utilizing advanced models such as Vision Transformers and large language models to process and fuse these inputs effectively.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US12282696B2 chunk=295 sim=0.566\n",
      "invention in the form disclosed. The present invention should not be considered limited to the particular embodiments described above, but rather should be understood to cover all aspects of the invention as fairly set out in the attached claims. Various modifications, equivalent processes, as well as numerous structures to which the present invention may be applicable, will be readily apparent to...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US11023715 chunk=6 sim=0.542\n",
      "input module Fourth neural network FIG . 6 U.S. Patent Jun . 1 , 2021 Sheet 7 of 8 US 11,023,715 B2 Computer readable storage medium First processor Bus FIG . 7 U.S. Patent Jun . 1 , 2021 Sheet 8 of 8 US 11,023,715 B2 Memory Second processor Bus FIG . 8 US 11,023,715 B2 METHOD AND APPARATUS FOR EXPRESSION RECOGNITION CROSS - REFERENCE TO RELATED APPLICATIONS This application claims priority to Chi...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US12282696B2 chunk=224 sim=0.494\n",
      "out in the claims filed with the application. Such combinations have particular advantages not specifically recited in the above summary. BRIEF DESCRIPTION OF THE DRAWINGS The invention is herein described, by way of non-limiting examples only, with reference to the accompanying drawings, wherein like designations denote like elements. Understanding that these drawings only provide information con...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US_12380282_B2 chunk=0 sim=0.488\n",
      "US012380282B2 (12) United States Patent Leary et al. (54) NATURAL LANGUAGE PROCESSING APPLICATIONS USING LARGE LANGUAGE MODELS (71) Applicant: Nvidia Corporation, Santa Clara, CA (US) (72) Inventors: Ryan Leary, Woodstock, GA (US); Jonathan Cohen, Mountain View, CA (US) (73) Assignee: Nvidia Corporation, Santa Clara, CA (US) (*) Notice: Subject to any disclaimer, the term ofthis patent is extended...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US11991338 chunk=0 sim=0.470\n",
      "US011991338B2 (12) United States Patent Legallais et al. (10) Patent No.: US 11,991,338 В2 (45) Date of Patent: (52) U.S. Cl. May 21, 2024 (54) METHOD CONFIGURED TO BЕ IMPLEMENTED AT A TERMINAL ADAPTED TO RECEIVE AN IMMERSIVE VIDEO SPATIALLY TILED WITH A SET OF TILES, AND CORRESPONDING TERMINAL (71) Applicant: INTERDIGITAL CE PATENT HOLDINGS, Paris (FR) (72) Inventors: Yvon Legallais, Rennes (FR);...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 7: How do the patents describe improving resource efficiency or reducing computational cost in language model processing?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The patents describe several approaches to improving resource efficiency or reducing computational cost in language model processing:\n",
      "\n",
      "1. **Use of Backoff N-grams with Backoff Scores (US8332207):**  \n",
      "   The system includes a language model comprising a collection of n-grams, each with a relative frequency and an order corresponding to the number of tokens. Each n-gram corresponds to a backoff n-gram of order n-1 and has associated backoff scores. These backoff scores are determined as a function of a backoff factor and the relative frequency of the corresponding backoff n-gram. This hierarchical backoff approach allows the model to efficiently estimate probabilities for rare or unseen n-grams by backing off to lower-order n-grams, thereby reducing the computational complexity and data sparsity issues in language modeling.\n",
      "\n",
      "2. **Augmentation of Small Language Models Using Large Language Models (US20240346254A1):**  \n",
      "   The large language model is used to augment smaller language models by retrieving external information and generating augmentation inputs that provide context and a language framework. This augmentation enables the smaller model to perform natural language tasks more effectively without the need for the smaller model itself to be as large or computationally intensive as the large language model. This approach reduces computational costs by leveraging the strengths of large models to enhance smaller, more efficient models.\n",
      "\n",
      "3. **Iterative Training and Evaluation to Improve Output Quality (US20240346254A1):**  \n",
      "   The large language model processes training data to produce outputs, which are then used to train a natural language generation system to mimic the large model’s output. The large model evaluates the smaller system’s outputs and iteratively adjusts them to improve quality. This iterative training can lead to more efficient models that achieve high-quality outputs with potentially less computational overhead during inference.\n",
      "\n",
      "Overall, these patents describe methods that either reduce the complexity of probability estimation in language models (through backoff n-grams) or leverage large models to improve smaller models’ performance (through augmentation and iterative training), thereby improving resource efficiency and reducing computational costs in language model processing.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US8332207 chunk=1 sim=0.579\n",
      "filed on Mar. 26, 2007. P.L.C. (57) (51) Int. Cl. G06F 17/27 (2006.01) (52) U.S. Cl. ............ 7949; 7025; 704,25570425 704,240, 704/2; 704/10; 704/1:370/270 (58) Field of Classification Search ................ 704/1, 10, 704/240, 257, 255, 251, 2: 707/999.102; See application file for complete search history. 370/270 (56) References Cited U.S. PATENT DOCUMENTS ABSTRACT Systems, methods, and co...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US20240346254A1 chunk=0 sim=0.577\n",
      "(19) United States (12) Patent Application Publication LIU et al. (54) NATURAL LANGUAGE TRAINING AND/OR AUGMENTATION WITH LARGE LANGUAGE MODELS (71) Applicant: MICROSOFT TECHNOLOGY LICENSING, LLC, Redmond, WA (US) (72) Inventors: Yang LIU, Bellevue, WA (US); Yichong XU, Bellevue, WA (US); Dan ITER, Austin, TX (US); Chenguang ZHU, Bellevue, WA (US); Nanshan ZENG, Bellevue, WA (US); Shuohang WANG. B...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US8332207 chunk=4 sim=0.551\n",
      "Patent Dec. 11, 2012 Sheet 4 of 4 US 8,332,207 B2 PROCESSOR(S) N. DATA PROCESSINGAPPARATUS TRANSLATION PROGRAM OPERATING SYSTEM HARDWARE/FIRMWARE PROCESSOR(S) ADDITIONAL DEVICE(S) COMPUTER READABLE MEDIUM 50 COMMUNICATIO N INTERFACE USER INTERFACE DEVICE(S) US 8,332,207 B2 1. LARGE LANGUAGE MODELS IN MACHINE TRANSLATON CROSS-REFERENCE TO RELATED APPLICATIONS This application claims the benefit und...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US8332207 chunk=0 sim=0.551\n",
      "US008332207 B2 (12) United States Patent Brants et al. (10) Patent No.: (45) Date of Patent: US 8,332,207 B2 Dec. 11, 2012 (54) LARGE LANGUAGE MODELS IN MACHINE TRANSLATION (75) Inventors: Thorsten Brants, Palo Alto, CA (US); Ashok C. Popat, Menlo Park, CA (US); Peng Xu, San Jose, CA (US); Franz J. Och, Mountain View, CA (US); Jeffrey Dean, Palo Alto, CA (US) (73) Assignee: Google Inc., Mountain V...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US12148421 chunk=0 sim=0.538\n",
      "(12) United States Patent Baeuml et al. US012148421B2 (10) Patent No.: US 12,148,421 B2 (45) Date of Patent: Nov. 19, 2024 (54) USING LARGE LANGUAGE MODEL(S) IN GENERATING AUTOMATED ASSISTANT RESPONSE(S (71) Applicant: GOOGLE LLC, Mountain View, CA (US) (72) Inventors: Martin Baeuml, Zurich (CH); Thushan Amarasiriwardena, Alameda, CA (US): Roberto Pieraccini, Zurich (CN); Vikram Sridar, Zurich (CH...\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "QUESTION 8: What system components or modules are mentioned as contributing to natural language generation quality?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "The system components or modules mentioned as contributing to natural language generation quality include:\n",
      "\n",
      "1. A Large Language Model (LLM) (e.g., BLOOM, BERT, ALEXA™, GPT-4, DEBERTA) — utilized for training and augmenting the natural language generation system. The LLM provides strong language understanding and generation capabilities across diverse tasks, performs imitation learning and reinforcement learning, retrieves relevant external information, and produces augmentation inputs such as selected information, queries/responses, and output templates to guide the smaller model.\n",
      "\n",
      "2. A Small Language Model — characterized by smaller size, improved adaptability, and resource efficiency. While it generally has inferior language understanding and generation quality compared to the large language model, it can be agilely fine-tuned and adapted with domain-specific data (e.g., healthcare datasets).\n",
      "\n",
      "3. Training Dataset — large unlabeled corpora (e.g., articles, images, audio) used by the large language model to train the natural language generation system.\n",
      "\n",
      "4. Domain-Specific Dataset — used to configure the small language model for specific contexts to improve adaptability.\n",
      "\n",
      "5. Augmentation Inputs — generated by the large language model during live deployment to guide the small language model in executing natural language generation tasks.\n",
      "\n",
      "Together, these components enable the natural language generation system to produce high-quality natural language outputs by leveraging the strengths of both large and small language models, balancing performance with adaptability and resource efficiency.\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "[0] patent=US20240346254A1 chunk=8 sim=0.642\n",
      "feedback inputs and the output score, the disclosed system can bypass the need to collect human feedback thereby further enhancing efficiency. Moreover, by taking on the behavior of the large language model, the natural language generation system can match and even exceed the performance of the large language model itself by retaining the reduced resource consumption and adaptability inherent to t...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US20240346254A1 chunk=12 sim=0.633\n",
      "In this way, the natural language generation system can produce high quality natural language outputs while retaining adaptability and resource efficiency of a small language model. [0026] Various examples, scenarios, and aspects that enable natural language training and/or augmentation with large language models are described below with respect to FIGS. 1-8. [0027] language model 102 can be confi...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US20240346254A1 chunk=11 sim=0.625\n",
      "[0020] routine for training a natural language generation system using a large language model. [0021] routine for augmenting a natural language generation system using a large language model. [0022] aspects of the techniques and technologies presented herein. [0023] techniques and technologies presented herein. DETAILED DESCRIPTION [0024] The techniques described herein enhance the operation of na...\n",
      "--------------------------------------------------------------------------------\n",
      "[3] patent=US20240346254A1 chunk=16 sim=0.623\n",
      "contrast to the system 100 discussed above which can be primarily utilized in a model training context, the system can be utilized in a live deployment environment. That is, external users can provide their own inputs 206 with new datasets 208 and natural language generation tasks 210 such as text summarization, conversation dialogue generation, query responses, and text synthesis. As such, the in...\n",
      "--------------------------------------------------------------------------------\n",
      "[4] patent=US20230252224A1 chunk=17 sim=0.611\n",
      "fiction movie storyboard. [0003] Generating natural language from machine representation systems is a common and increasingly important function. Existing natural language generation (NLG) systems, such as translators, summarizers, dialog generators, etc., while common, cannot produce variable output based on user-desired tunable specifications. Additionally, such existing systems cannot take inpu...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def run_eval_questions(\n",
    "    questions,\n",
    "    embed_model,\n",
    "    nn_index,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    client,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    top_k=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs RAG on each question and prints out:\n",
    "      - question\n",
    "      - answer\n",
    "      - top-k retrieved chunks\n",
    "    You will manually judge success from this output.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, q in enumerate(questions, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"QUESTION {i}: {q}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        answer, retrieved = rag_answer(\n",
    "            q,\n",
    "            embed_model,\n",
    "            nn_index,\n",
    "            embeddings,\n",
    "            chunks,\n",
    "            client,\n",
    "            model=model,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        \n",
    "        print(\"\\nANSWER:\\n\")\n",
    "        print(answer)\n",
    "        \n",
    "        print(\"\\nRETRIEVED CHUNKS:\\n\")\n",
    "        for r in retrieved:\n",
    "            print(f\"[{r['rank']}] patent={r['patent_id']} chunk={r['chunk_index']} \"\n",
    "                  f\"sim={r['score']:.3f}\")\n",
    "            print(r[\"text\"][:400] + (\"...\" if len(r[\"text\"]) > 400 else \"\"))\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # placeholder dict; you will fill these fields manually later\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved\": retrieved,\n",
    "            \"retrieval_success\": None,   # True/False after you inspect\n",
    "            \"relevant_rank\": None,       # e.g., 0,1,2 or None\n",
    "            \"support_level\": None,       # \"full\", \"partial\", \"none\"\n",
    "            \"hallucination\": None,       # True/False\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "eval_raw = run_eval_questions(\n",
    "    test_questions,\n",
    "    embed_model,\n",
    "    nn_index,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    client,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    top_k=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6b461",
   "metadata": {},
   "source": [
    "## Manual Evaluation Scoring\n",
    "\n",
    "Each question is scored on:\n",
    "\n",
    "- Retrieval success  \n",
    "- Relevant rank  \n",
    "- Support level  \n",
    "- Hallucination severity  \n",
    "\n",
    "These scores populate the evaluation DataFrame used in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ab95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scored = [\n",
    "    {\n",
    "        \"question\": 1,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"mostly_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 2,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"fully_supported\",\n",
    "        \"hallucination\": \"none\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 3,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"mostly_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 4,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"moderately_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 5,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"partially_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 6,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"weakly_supported\",\n",
    "        \"hallucination\": \"moderate\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 7,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"mostly_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 8,\n",
    "        \"retrieval_success\": True,\n",
    "        \"relevant_rank\": 0,\n",
    "        \"support_level\": \"mostly_supported\",\n",
    "        \"hallucination\": \"mild\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dceae2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>retrieval_success</th>\n",
       "      <th>relevant_rank</th>\n",
       "      <th>support_level</th>\n",
       "      <th>hallucination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>mostly_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>fully_supported</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>mostly_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>moderately_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>partially_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>weakly_supported</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>mostly_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>mostly_supported</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question  retrieval_success  relevant_rank         support_level  \\\n",
       "0         1               True              0      mostly_supported   \n",
       "1         2               True              0       fully_supported   \n",
       "2         3               True              0      mostly_supported   \n",
       "3         4               True              0  moderately_supported   \n",
       "4         5               True              0   partially_supported   \n",
       "5         6               True              0      weakly_supported   \n",
       "6         7               True              0      mostly_supported   \n",
       "7         8               True              0      mostly_supported   \n",
       "\n",
       "  hallucination  \n",
       "0          mild  \n",
       "1          none  \n",
       "2          mild  \n",
       "3          mild  \n",
       "4          mild  \n",
       "5      moderate  \n",
       "6          mild  \n",
       "7          mild  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_eval = pd.DataFrame(eval_scored)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77619be3",
   "metadata": {},
   "source": [
    "## Manual Evaluation\n",
    "\n",
    "Each question is scored on:\n",
    "\n",
    "- Retrieval success  \n",
    "- Relevant rank  \n",
    "- Support level  \n",
    "- Hallucination severity  \n",
    "\n",
    "We then see retrieval rate and success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b79776f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 8\n",
      "Retrieval success rate: 1.0\n",
      "\n",
      "Support level distribution:\n",
      "support_level\n",
      "mostly_supported        0.500\n",
      "fully_supported         0.125\n",
      "moderately_supported    0.125\n",
      "partially_supported     0.125\n",
      "weakly_supported        0.125\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Hallucination distribution:\n",
      "hallucination\n",
      "mild        0.750\n",
      "none        0.125\n",
      "moderate    0.125\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Average hallucination severity (0=none, 1=mild, 2=moderate, 3=severe):\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_eval already created from eval_scored\n",
    "\n",
    "# 1. Basic counts\n",
    "n = len(df_eval)\n",
    "\n",
    "# 2. Retrieval success rate (booleans → mean works)\n",
    "retrieval_rate = df_eval[\"retrieval_success\"].mean()\n",
    "\n",
    "# 3. Support level distribution (proportions)\n",
    "support_dist = df_eval[\"support_level\"].value_counts(normalize=True)\n",
    "\n",
    "# 4. Hallucination label distribution (proportions)\n",
    "hallucination_dist = df_eval[\"hallucination\"].value_counts(normalize=True)\n",
    "\n",
    "# 5. Optional: map hallucination severity to numbers for an average score\n",
    "hallucination_map = {\n",
    "    \"none\": 0,\n",
    "    \"mild\": 1,\n",
    "    \"moderate\": 2,\n",
    "    \"severe\": 3,   # not used here, but included for completeness\n",
    "}\n",
    "df_eval[\"hallucination_severity\"] = df_eval[\"hallucination\"].map(hallucination_map)\n",
    "avg_hallucination_severity = df_eval[\"hallucination_severity\"].mean()\n",
    "\n",
    "print(\"Number of questions:\", n)\n",
    "print(\"Retrieval success rate:\", retrieval_rate)\n",
    "\n",
    "print(\"\\nSupport level distribution:\")\n",
    "print(support_dist)\n",
    "\n",
    "print(\"\\nHallucination distribution:\")\n",
    "print(hallucination_dist)\n",
    "\n",
    "print(\"\\nAverage hallucination severity (0=none, 1=mild, 2=moderate, 3=severe):\")\n",
    "print(avg_hallucination_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93f01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
