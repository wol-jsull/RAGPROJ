{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67af026",
   "metadata": {},
   "source": [
    "# 03_embeddings.ipynb  \n",
    "### Embedding Patent Chunks and Building a Vector Index\n",
    "\n",
    "This notebook loads precomputed text chunks, encodes them into dense vector embeddings, saves the embeddings and metadata, and builds a nearest neighbors index for similarity search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c20c3f",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "Import libraries and define paths for chunk input, embedding output, and vector index construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19693555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sully\\RAGPROJ\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/sully/RAGPROJ/data/processed/chunks/patent_chunks.jsonl'),\n",
       " WindowsPath('C:/Users/sully/RAGPROJ/embeddings'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "CHUNK_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"chunks\"\n",
    "EMB_DIR = PROJECT_ROOT / \"embeddings\"\n",
    "\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_FILE = CHUNK_DIR / \"patent_chunks.jsonl\"\n",
    "CHUNK_FILE, EMB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99432c1",
   "metadata": {},
   "source": [
    "## Load Chunk Metadata\n",
    "Load all chunk records from the JSONL file.  \n",
    "Each record includes an ID, patent ID, chunk index, and text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52996cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2905 chunks from patent_chunks.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'US10452978_0',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 0,\n",
       "  'text': 'US010452978B2 ( 12 ) United States Patent Shazeer et al . ( 10 ) Patent No . : US 10 , 452 , 978 B2 ( 45 ) Date of Patent : Oct . 22 , 2019 ( 54 ) ATTENTION - BASED SEQUENCE TRANSDUCTION NEURAL NETWORKS ) U . S . Ci . ( 71 ) Applicant : Google LLC , Mountain View , CA ( US ) ( 58 ) Field of Classification Search CPC . . . . . . . . . . . . . . . . . GOON 3 / 08 ( 2013 . 01 ) ; G06N 3 / 04 ( 2013 . 01 ) ; G06N 3 / 0454 ( 2013 . 01 ) CPC USPC . . . . . . . . . . . . . . . . . . . . . . . . . GOOF 3 / 015 . . . . . . 706 / 15 , 45 See application file for complete search history . ( 72 ) Inventors : Noam M . Shazeer , Palo Alto , CA ( US ) ; Aidan Nicholas Gomez , Toronto ( CA ) ; Lukasz Mieczyslaw Kaiser , Mountain View , CA ( US ) ; Jakob D . Uszkoreit , Portola Valley , CA ( US ) ; Llion Owen Jones , San Francisco , CA ( US ) ; Niki J . Parmar , Sunnyvale , CA ( US ) ; Illia Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this'},\n",
       " {'id': 'US10452978_1',\n",
       "  'patent_id': 'US10452978',\n",
       "  'chunk_index': 1,\n",
       "  'text': 'Polosukhin , Mountain View , CA ( US ) ; Ashish Teku Vaswani , San Francisco , CA ( US ) ( 73 ) Assignee : Google LLC , Mountain View , CA ( US ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U . S . C . 154 ( b ) by 0 days . ( 21 ) Appl . No . : 16 / 021 , 971 ( 22 ) Filed : Jun . 28 , 2018 ( 65 ) Prior Publication Data US 2018 / 0341860 A1 Nov . 29 , 2018 Related U . S . Application Data ( 63 ) Continuation of application PCT / US2018 / 034224 , filed on May 23 , 2018 . No . ( 60 ) Provisional application No . 62 / 541 , 594 , filed on Aug . , 2017 , provisional application No . 62 / 510 , 256 , filed on May 23 , 2017 . ( 51 ) Int . Cl . G06F 15 / 18 GO6N 3 / 08 GOON 3 / 04 ( 2006 . 01 ) ( 2006 . 01 ) ( 2006 . 01 ) ( 56 ) References Cited PUBLICATIONS Luong et al . , Effective Approaches to Attention - based Neural Machine Translation , ( 2015 ) Conf . on Empirical Methods in Natural Lan guage Processing at p . 1412 - 1421 ( Year : 2015 ) . * ( Continued ) Primary Examiner - David R Vincent ( 74 ) Attorney , Agent , or Firm — Fish & Richardson P . C . ( 57 ) ABSTRACT Methods , systems , and apparatus , including computer pro grams encoded on a'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_chunks(jsonl_path: Path):\n",
    "    chunks = []\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            chunks.append(rec)\n",
    "    print(f\"Loaded {len(chunks)} chunks from {jsonl_path.name}\")\n",
    "    return chunks\n",
    "\n",
    "chunks = load_chunks(CHUNK_FILE)\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7cef9",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model\n",
    "Use a sentence-transformer model to embed each text chunk into a dense vector representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae9b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 384\n"
     ]
    }
   ],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(\"Embedding dim:\", embed_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553976b",
   "metadata": {},
   "source": [
    "## Build Embeddings for All Chunks\n",
    "Encode all chunk texts in batches and stack the results into a single NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595d8d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 64/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 128/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 192/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 256/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 320/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 384/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 448/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 512/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 576/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 640/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 704/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 768/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 832/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 896/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 960/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1024/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1088/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1152/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1216/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1280/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1344/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1408/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1472/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1536/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1600/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1664/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1728/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1792/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1856/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1920/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 1984/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2048/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2112/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2176/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2240/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2304/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2368/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2432/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2496/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2560/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2624/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2688/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2752/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2816/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2880/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2905/2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2905, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_embeddings(chunks, model, batch_size: int = 64):\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    ids = [c[\"id\"] for c in chunks]\n",
    "\n",
    "    all_embs = []\n",
    "    n = len(texts)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_texts = texts[start:end]\n",
    "        embs = model.encode(\n",
    "            batch_texts,\n",
    "            batch_size=len(batch_texts),\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "        all_embs.append(embs)\n",
    "        print(f\"Encoded {end}/{n}\")\n",
    "\n",
    "    embeddings = np.vstack(all_embs)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = build_embeddings(chunks, embed_model)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ea8e0",
   "metadata": {},
   "source": [
    "## Save Embeddings and Metadata\n",
    "Write the embeddings to a `.npy` file and save the corresponding chunk metadata to a JSONL file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80c4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to C:\\Users\\sully\\RAGPROJ\\embeddings\\embeddings.npy\n",
      "Saved metadata to C:\\Users\\sully\\RAGPROJ\\embeddings\\chunk_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "emb_path = EMB_DIR / \"embeddings.npy\"\n",
    "meta_path = EMB_DIR / \"chunk_metadata.jsonl\"\n",
    "\n",
    "# Save embeddings\n",
    "np.save(emb_path, embeddings)\n",
    "print(f\"Saved embeddings to {emb_path}\")\n",
    "\n",
    "# Save metadata (id, patent_id, chunk_index, text)\n",
    "with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in chunks:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "print(f\"Saved metadata to {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22655e74",
   "metadata": {},
   "source": [
    "## Build Nearest Neighbors Index\n",
    "Construct a k-nearest neighbors (kNN) index over the embeddings using cosine distance for similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29c21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_index(embeddings, n_neighbors: int = 5):\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=\"cosine\",\n",
    "    )\n",
    "    nn.fit(embeddings)\n",
    "    return nn\n",
    "\n",
    "nn_index = build_nn_index(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d362fa",
   "metadata": {},
   "source": [
    "## Test Similarity Search\n",
    "Run a sample query through the embedding model and nearest neighbors index.  \n",
    "Inspect the top retrieved chunks and their similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97bfb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] patent=US20240346254A1 chunk=12 sim=0.577\n",
      "In this way, the natural language generation system can produce high quality natural language outputs while retaining adaptability and resource efficiency of a small language model. [0026] Various examples, scenarios, and aspects that enable natural language training and/or augmentation with large language models are described below with respect to FIGS. 1-8. [0027] language model 102 can be confi...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] patent=US11562147 chunk=3 sim=0.576\n",
      "into a unified transformer encoder together with a corresponding image caption and multi-turn dialogue history input. The subject technology can initialize the unified transformer encoder with BERT for increased leveraging of the pre-trained language representation. To deeply fuse features from the two modalities, the subject technology make use of two visually-grounded pretraining objectives, suc...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] patent=US11562147 chunk=41 sim=0.576\n",
      "an utterance of the human user in the dialogue history or a language model response. 14. The system of claim 11, wherein a position level encoding layer from the plurality of text encoding layers generates the position level encoding, wherein the position level encoding identifies a token ordering in the text input. 15. A non-transitory, machine-readable medium having stored thereon machine-readab...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search(query: str, model, nn_index, embeddings, chunks, top_k: int = 5):\n",
    "    # 1. Embed query\n",
    "    q_emb = model.encode([query])\n",
    "    \n",
    "    # 2. Nearest neighbors\n",
    "    distances, indices = nn_index.kneighbors(q_emb, n_neighbors=top_k)\n",
    "    \n",
    "    results = []\n",
    "    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "        rec = chunks[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": 1 - dist,  # cosine similarity ≈ 1 - distance\n",
    "            \"id\": rec[\"id\"],\n",
    "            \"patent_id\": rec[\"patent_id\"],\n",
    "            \"chunk_index\": rec[\"chunk_index\"],\n",
    "            \"text\": rec[\"text\"][:400] + (\"...\" if len(rec[\"text\"]) > 400 else \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "query = \"transformer-based language model for dialogue generation\"\n",
    "results = search(query, embed_model, nn_index, embeddings, chunks, top_k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] patent={r['patent_id']} chunk={r['chunk_index']} sim={r['score']:.3f}\")\n",
    "    print(r[\"text\"])\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
