{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28fa8606",
   "metadata": {},
   "source": [
    "# 01_extract_text.ipynb  \n",
    "### PDF Text Extraction and Cleaning\n",
    "This notebook extracts text from raw patent PDFs, cleans it, and prepares standardized `.txt` files for later chunking and embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbdb6f",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "Load required libraries and define project paths for raw PDFs and cleaned text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96f6bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/sully/RAGPROJ/data/raw/patents'),\n",
       " WindowsPath('C:/Users/sully/RAGPROJ/data/processed/txt'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pdfminer.high_level\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"patents\"\n",
    "TXT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"txt\"\n",
    "\n",
    "TXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_DIR, TXT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062def2",
   "metadata": {},
   "source": [
    "## PDF to Text Extraction\n",
    "The `pdf_to_text` function extracts text from a PDF using pdfminer, with PyPDF as a fallback when pdfminer fails. Some patents may not yield extractable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a01377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Try to extract text using pdfminer first, then pypdf as fallback.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    # First try pdfminer\n",
    "    try:\n",
    "        text = pdfminer.high_level.extract_text(str(pdf_path)) or \"\"\n",
    "    except:\n",
    "        text = \"\"\n",
    "\n",
    "    # Fallback to PyPDF\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            reader = PdfReader(str(pdf_path))\n",
    "            pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "            text = \"\\n\".join(pages)\n",
    "        except:\n",
    "            text = \"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ed7e6",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "Removes page numbers, figure labels, line breaks, and extra spaces to produce clean text suitable for chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b016a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Remove standalone page numbers at the start of lines\n",
    "    text = re.sub(r'^\\s*\\d+\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    # Remove FIG/Fig lines\n",
    "    text = re.sub(r'FIG\\.?\\s*\\d+.*', ' ', text)\n",
    "    text = re.sub(r'Fig\\.?\\s*\\d+.*', ' ', text)\n",
    "    # Fix hyphenated line breaks (in case any remain)\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    # Replace all newlines with spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ddeb4",
   "metadata": {},
   "source": [
    "## Process All PDFs\n",
    "Extracts and cleans every PDF in the raw directory.  \n",
    "Skips files that already have corresponding `.txt` outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df6eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs(raw_dir: Path = RAW_DIR, out_dir: Path = TXT_DIR):\n",
    "    pdf_files = sorted([p for p in raw_dir.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    print(f\"Found {len(pdf_files)} PDFs\")\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        out_path = out_dir / (pdf.stem + \".txt\")\n",
    "\n",
    "        # üîí If you already created a txt by hand, don't touch it\n",
    "        if out_path.exists():\n",
    "            print(f\"Skipping {pdf.name} (txt already exists)\")\n",
    "            continue\n",
    "\n",
    "        raw = pdf_to_text(pdf)\n",
    "        if not raw.strip():\n",
    "            print(f\"WARNING: no text extracted from {pdf.name}\")\n",
    "            continue\n",
    "\n",
    "        clean = clean_text(raw)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(clean)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e4ade",
   "metadata": {},
   "source": [
    "## Test Extraction\n",
    "Run a quick test extraction to inspect the cleaned text and verify that extraction works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad240039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw length: 60769\n",
      "Clean length: 56671\n",
      "USOO9037464B1 United States Patent (12) Mikolov et al. (10) Patent No.: (45) Date of Patent: US 9,037.464 B1 May 19, 2015 (54) COMPUTING NUMERIC REPRESENTATIONS OF WORDS INA HIGH-DIMIENSIONAL SPACE 6,092,043 A * 7/2000 Squires et al. ................ TO4/251 8,566,102 B1 * 10/2013 Bangalore et al. . TO4/270.1 2013/0262467 A1* 10/2013 Zhang et al. .................. 707f737 ck (71) Applicant: Google Inc., Mountain View, CA (US) (72) Inventors: Tomas Mikolov, Jersey City, NJ (US); Kai Chen, San Bruno, CA (US); Gregory S. Corrado, San Francisco, CA Machine Learning Research, 3:1137-1155, 2003. (US); Jeffrey A. Dean, Palo Alto, CA (US) OTHER PUBLICATIONS Bengio and LeCun, ‚ÄúScaling learning algorithms towards AI. Large Scale Kernel Machines, MIT Press, 41 pages, 2007. Bengio et al., ‚ÄúA neural probabilistic language model.‚Äù Journal of Brants et al., \"Large language models in machine translation.\" Pro ceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 10 Collobert and Weston, ‚ÄúA Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.‚Äù Inter national Conference On Machine Learning, ICML, 8 pages, 2008. Collobert et al., ‚ÄúNatural Language Processing (Almost) from Scratch.‚Äù Journal of Machine Learning Research, 12:2493-2537, 2011. (Continued) Primary Examiner ‚Äî Daniel D Abebe 74). Att (74) Attorney, Agent, or Firm Agent, or Firm ‚Äî Fish & Richardson P.C. 1CaOSO 1S (73) Assi\n"
     ]
    }
   ],
   "source": [
    "test_pdf = RAW_DIR / \"US9037464.pdf\"  # change to any name from the list above\n",
    "\n",
    "raw = pdf_to_text(test_pdf)\n",
    "clean = clean_text(raw)\n",
    "\n",
    "print(\"Raw length:\", len(raw))\n",
    "print(\"Clean length:\", len(clean))\n",
    "print(clean[:1500])  # peek at the first 1500 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081ccc4",
   "metadata": {},
   "source": [
    "## Run Full Extraction\n",
    "Execute the PDF extraction and cleaning pipeline on all patents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f802cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs(raw_dir: Path = RAW_DIR, out_dir: Path = TXT_DIR):\n",
    "    pdf_files = sorted([p for p in raw_dir.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDFs in {raw_dir}\")\n",
    "    for pdf in pdf_files:\n",
    "        base_name = pdf.stem  # e.g., \"US9037464\"\n",
    "        out_path = out_dir / f\"{base_name}.txt\"\n",
    "        \n",
    "        if out_path.exists():\n",
    "            print(f\"Skipping {pdf.name} (already processed)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {pdf.name}...\")\n",
    "        raw = pdf_to_text(pdf)\n",
    "        if not raw.strip():\n",
    "            print(f\"  WARNING: no text extracted from {pdf.name}\")\n",
    "            continue\n",
    "        \n",
    "        clean = clean_text(raw)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(clean)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fa2c8",
   "metadata": {},
   "source": [
    "## Run Full Extraction\n",
    "Execute the PDF extraction and cleaning pipeline on all patents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ab7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 PDFs in C:\\Users\\sully\\RAGPROJ\\data\\raw\\patents\n",
      "Skipping US10452978.pdf (already processed)\n",
      "Skipping US10740433.pdf (already processed)\n",
      "Skipping US10902563.pdf (already processed)\n",
      "Skipping US11003865.pdf (already processed)\n",
      "Skipping US11023715.pdf (already processed)\n",
      "Skipping US11238332.pdf (already processed)\n",
      "Skipping US11295552.pdf (already processed)\n",
      "Skipping US11328398.pdf (already processed)\n",
      "Skipping US11562147.pdf (already processed)\n",
      "Skipping US11636570.pdf (already processed)\n",
      "Skipping US11749857.pdf (already processed)\n",
      "Skipping US11900261.pdf (already processed)\n",
      "Skipping US11921824.pdf (already processed)\n",
      "Skipping US11961514.pdf (already processed)\n",
      "Skipping US11989527.pdf (already processed)\n",
      "Skipping US11991338.pdf (already processed)\n",
      "Skipping US12148421.pdf (already processed)\n",
      "Skipping US12182506.pdf (already processed)\n",
      "Skipping US12217382.pdf (already processed)\n",
      "Skipping US12271791B2.pdf (already processed)\n",
      "Skipping US12282696B2.pdf (already processed)\n",
      "Skipping US20210183484A1.pdf (already processed)\n",
      "Skipping US20220101113A1.pdf (already processed)\n",
      "Skipping US20230252224A1.pdf (already processed)\n",
      "Skipping US20240185001A1.pdf (already processed)\n",
      "Skipping US20240256792A1.pdf (already processed)\n",
      "Skipping US20240346254A1.pdf (already processed)\n",
      "Skipping US8332207.pdf (already processed)\n",
      "Skipping US8812291.pdf (already processed)\n",
      "Skipping US9037464.pdf (already processed)\n",
      "Skipping US9740680.pdf (already processed)\n",
      "Skipping US_12380282_B2.pdf (already processed)\n",
      "Skipping US_12417081_B2.pdf (already processed)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "process_all_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d5a46",
   "metadata": {},
   "source": [
    "## Normalize All Text Files\n",
    "Re-cleans each `.txt` file to ensure consistent formatting, including manually pasted ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe342e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_all_text_files(txt_dir: Path = TXT_DIR):\n",
    "    txt_files = sorted(txt_dir.glob(\"*.txt\"))\n",
    "    print(f\"Normalizing {len(txt_files)} text files...\")\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        raw = txt_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        txt_file.write_text(cleaned, encoding=\"utf-8\")\n",
    "        print(f\"‚úî Normalized: {txt_file.name}\")\n",
    "\n",
    "    print(\"\\n‚ú® Done! All text files are now cleaned and uniform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c27456",
   "metadata": {},
   "source": [
    "## Run Normalization\n",
    "Apply normalization to all cleaned text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191163f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing 33 text files...\n",
      "‚úî Normalized: US10452978.txt\n",
      "‚úî Normalized: US10740433.txt\n",
      "‚úî Normalized: US10902563.txt\n",
      "‚úî Normalized: US11003865.txt\n",
      "‚úî Normalized: US11023715.txt\n",
      "‚úî Normalized: US11238332.txt\n",
      "‚úî Normalized: US11295552.txt\n",
      "‚úî Normalized: US11328398.txt\n",
      "‚úî Normalized: US11562147.txt\n",
      "‚úî Normalized: US11636570.txt\n",
      "‚úî Normalized: US11749857.txt\n",
      "‚úî Normalized: US11900261.txt\n",
      "‚úî Normalized: US11921824.txt\n",
      "‚úî Normalized: US11961514.txt\n",
      "‚úî Normalized: US11989527.txt\n",
      "‚úî Normalized: US11991338.txt\n",
      "‚úî Normalized: US12148421.txt\n",
      "‚úî Normalized: US12182506.txt\n",
      "‚úî Normalized: US12217382.txt\n",
      "‚úî Normalized: US12271791B2.txt\n",
      "‚úî Normalized: US12282696B2.txt\n",
      "‚úî Normalized: US20210183484A1.txt\n",
      "‚úî Normalized: US20220101113A1.txt\n",
      "‚úî Normalized: US20230252224A1.txt\n",
      "‚úî Normalized: US20240185001A1.txt\n",
      "‚úî Normalized: US20240256792A1.txt\n",
      "‚úî Normalized: US20240346254A1.txt\n",
      "‚úî Normalized: US8332207.txt\n",
      "‚úî Normalized: US8812291.txt\n",
      "‚úî Normalized: US9037464.txt\n",
      "‚úî Normalized: US9740680.txt\n",
      "‚úî Normalized: US_12380282_B2.txt\n",
      "‚úî Normalized: US_12417081_B2.txt\n",
      "\n",
      "‚ú® Done! All text files are now cleaned and uniform.\n"
     ]
    }
   ],
   "source": [
    "normalize_all_text_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
